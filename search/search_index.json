{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":" "},{"location":"developer_notes/","text":"\ud83d\udcda Developer Notes \ud83d\udc48 Location for storing documentation on tools specific to this project","title":"\ud83d\udcda Developer Notes"},{"location":"developer_notes/#developer-notes","text":"\ud83d\udc48 Location for storing documentation on tools specific to this project","title":"\ud83d\udcda Developer Notes"},{"location":"developer_notes/engineering/data_validation/tfdv/","text":"TensorFlow Data Validation (TFDV) Note The dataset used in this example below is the Census Income Dataset , a dataset that can be used to predict if an individual earns more than or less than 50k US Dollars annually Overview TFDV helps to understand, validate, and monitor production machine learning data at scale. It provides insight into some key questions in the data analysis process such as: What are the underlying statistics of my data? What does my training dataset look like? How does my evaluation and serving datasets compare to the training dataset? How can I find and fix data anomalies? As shown, you can use TFDV to compute descriptive statistics of the training data and generate a schema. You can then validate new datasets (e.g. the serving dataset from your customers) against this schema to detect and fix anomalies. This helps prevent the different types of skew. That way, you can be confident that your model is training on or predicting data that is consistent with the expected feature types and distribution. This tutorial will show you how to: Generate and visualize statistics from a dataset Detect and fix anomalies in an evaluation dataset Installation pip install tensorflow pip install tensorflow_data_validation Visualize Dataset Statistics You can compute your dataset statistics by using the generate_statistics_from_dataframe method. Under the hood, it distributes the analysis via Apache Beam which allows it to scale over large datasets. In this example we have two datasets, a train_df and eval_df . train_stats = tfdv.generate_statistics_from_dataframe(train_df) TFDV accepts three input formats: TensorFlow\u2019s TFRecord, Pandas Dataframe, and CSV file. This will generate stats on the following data types Numerical Data Categorical Data Count of data records Count of data records % of missing data records % of missing data records Mean, std, min, max unique records % of zero values Avg string length To visualize the results, you can use visualize_statistics . This will show tables for both numeric and categorical features. Within this table you can see the frequency, percentage of missing values and some basic statistics. Create a Data Schema Simply put, a schema describes standard characteristics of your data such as column data types and expected data value range. The schema is created on a dataset that you consider as reference, and can be reused to validate other incoming datasets. TFDV allows you to automatically generate an initial version of the schema using the infer_schema() method. This returns a Schema protocol buffer containing the result. As mentioned in the TFX paper (Section 3.3), the results of the schema inference can be summarized as follows: The expected type of each feature. The expected presence of each feature, in terms of a minimum count and fraction of examples that must contain the feature. The expected valency of the feature in each example, i.e., minimum and maximum number of values. The expected domain of a feature, i.e., the small universe of values for a string feature, or range for an integer feature. Note: TFDV will not automatically create a domain for numerical types. Comparing Train vs Evaluation Datasets So right now we have made a schema for the training dataset ( train_df ). Now we need to generate statistics on the evaluation set ( eval_df ). The purpose of this is to allow you to compare statistics between the two sets to see if the values of the features lie within the same distribution. If not, then there is some type of skew occuring. TFDV allows you to do this by passing a few additional parameters to visualize_statistics . lhs_statistics : Required parameter. Expects an instance of DatasetFeatureStatisticsList. rhs_statistics : Expects an instance of DatasetFeatureStatisticsList to compare with lhs_statistics. lhs_name : Name of the lhs_statistics dataset. rhs_name : Name of the rhs_statistics dataset. As seen above, we can see that age do not share the same characteristics across the two datasets as min and max values are 0 and 1000 respectively. You can also see that 0.02% of the data is missing from the workclass feature. We can remove any rows that do not fit our expected values Displaying Anomalies We can use the reference schema that was made eariler to check for anomalies in the evaluation data. This is done using the validate_statistics and display_anomalies methods. Detected anomalies can either be considered a real error that needs to be cleaned, or depending on your domain knowledge and the specific case, they can be accepted. You can see below that TFDV caught a value for categorical variables that weren't expected. For example: some values in the race feature in the evaluation data wasn't in the schema. Lets say you encode these categorical variables and feed them as input features to the model, the model will break as the number of input features would not be the same as they were in training. You can see how catching anomalies like this is crucial for productionizing ML models. Updating the Schema TFDV provides a set of utility methods and parameters that you can use for revising the inferred schema. This reference lists down the type of anomalies and the parameters that you can edit but we'll focus only on a couple here. Anomaly Detection Criteria If you want to allow some anomalies up to a certain point can relax the criteria for raising anomalies by using the distribution_constraints . As described here under ENUM_TYPE_UNEXPECTED_STRING_VALUES the detection criteria is calculated by (num of features not in domain / tot values) > (1 - feature.distribution_constraints.min_domain_mass In code this looks like: country_feature = tfdv . get_feature ( schema , 'native-country' ) country_feature . distribution_constraints . min_domain_mass = 0.9 Adding New Domain Value From our example earlier, imagine we want to actually have the value of Asian in our schema. This can be done with the following # Add new value to the domain of the feature `race` race_domain = tfdv . get_domain ( schema , 'race' ) race_domain . value . append ( 'Asian' ) Numerical Range Criteria As mentioned earlier, TFDV doesn't automatically make a domain for numeric values. If you recall from the display of statistics earlier that there was an age value of 1000. We can set a min and max value for this feature using the set_domain method. Now we can be sure we catch anomalies like that again in the future. If we run validate_statistics and display_anomalies again, after updating our schema and removing some anomalous rows, we can see that no anomalies are found. Data Slicing TDFV allows you to analyze specific slices of your dataset, which can be used to see if each feature is well-represented in the dataset. For this example we will compare male vs female in the sex column. For this we used the slicing_util provided by TFDV. None in this context is used if you want to use the entire domain. from tensorflow_data_validation.utils import slicing_util slice_fn = slicing_util . get_feature_value_slicer ( features = { 'sex' : None }) You need to tell TFDV that you need statistics for the features you set and you can do that through the slice_functions argument of tfdv.StatsOptions . # Declare stats options slice_stats_options = tfdv . StatsOptions ( schema = schema , slice_functions = [ slice_fn ], infer_type_from_schema = True ) Generating sliced statistics only works for CSVs so you will need to convert the Pandas dataframe to a CSV. Passing the slice_stats_options to generate_statistics_from_dataframe() will not produce the expected results. Therefore, you will need to use generate_statistics_from_csv() # Convert dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv` CSV_PATH = 'slice_sample.csv' train_df . to_csv ( CSV_PATH ) # Calculate statistics for the sliced dataset sliced_stats = tfdv . generate_statistics_from_csv ( CSV_PATH , stats_options = slice_stats_options ) With that, you now have the statistics for the set slice. These are packed into a DatasetFeatureStatisticsList protocol buffer. You can see the dataset names below. The first element in the list (i.e. index=0) is named All_Examples which just contains the statistics for the entire dataset. The next two elements (i.e. named sex_Male and sex_Female ) are the datasets that contain the stats for the slices. It is important to note that these datasets are of the type: DatasetFeatureStatistics . You will see why this is important after the cell below. You can then visualize the statistics as before to examine the slices. An important caveat is visualize_statistics() accepts a DatasetFeatureStatisticsList type instead of DatasetFeatureStatistics . Thus, at least for this version of TFDV, you will need to convert it to the correct type. from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList # Convert `Male` statistics (index=1) to the correct type and get the dataset name male_stats_list = DatasetFeatureStatisticsList () male_stats_list . datasets . extend ([ sliced_stats . datasets [ 1 ]]) male_stats_name = sliced_stats . datasets [ 1 ] . name # Convert `Female` statistics (index=2) to the correct type and get the dataset name female_stats_list = DatasetFeatureStatisticsList () female_stats_list . datasets . extend ([ sliced_stats . datasets [ 2 ]]) female_stats_name = sliced_stats . datasets [ 2 ] . name # Visualize the two slices side by side tfdv . visualize_statistics ( lhs_statistics = male_stats_list , rhs_statistics = female_stats_list , lhs_name = male_stats_name , rhs_name = female_stats_name ) As you can see below, we have a similar view of the data. However, this time the results are stratified by the different sex column values. You can see here that within this dataset Male is disproportionately more represented than Female . As mentioned in our MLOps documentation data slicing is a useful method for asses your model for bias. These steps can be refactored for assessing slices of data after the model has produced output. References TensorFlow Data Validation (TFDV) TFX: A TensorFlow-Based Production-Scale Machine Learning Platform Example Notebook Medium: Data validation pipeline for production ML: Tensor flow data validation(TFDV)","title":"TFDV"},{"location":"developer_notes/engineering/data_validation/tfdv/#tensorflow-data-validation-tfdv","text":"Note The dataset used in this example below is the Census Income Dataset , a dataset that can be used to predict if an individual earns more than or less than 50k US Dollars annually","title":"TensorFlow Data Validation (TFDV)"},{"location":"developer_notes/engineering/data_validation/tfdv/#overview","text":"TFDV helps to understand, validate, and monitor production machine learning data at scale. It provides insight into some key questions in the data analysis process such as: What are the underlying statistics of my data? What does my training dataset look like? How does my evaluation and serving datasets compare to the training dataset? How can I find and fix data anomalies? As shown, you can use TFDV to compute descriptive statistics of the training data and generate a schema. You can then validate new datasets (e.g. the serving dataset from your customers) against this schema to detect and fix anomalies. This helps prevent the different types of skew. That way, you can be confident that your model is training on or predicting data that is consistent with the expected feature types and distribution. This tutorial will show you how to: Generate and visualize statistics from a dataset Detect and fix anomalies in an evaluation dataset","title":"Overview"},{"location":"developer_notes/engineering/data_validation/tfdv/#installation","text":"pip install tensorflow pip install tensorflow_data_validation","title":"Installation"},{"location":"developer_notes/engineering/data_validation/tfdv/#visualize-dataset-statistics","text":"You can compute your dataset statistics by using the generate_statistics_from_dataframe method. Under the hood, it distributes the analysis via Apache Beam which allows it to scale over large datasets. In this example we have two datasets, a train_df and eval_df . train_stats = tfdv.generate_statistics_from_dataframe(train_df) TFDV accepts three input formats: TensorFlow\u2019s TFRecord, Pandas Dataframe, and CSV file. This will generate stats on the following data types Numerical Data Categorical Data Count of data records Count of data records % of missing data records % of missing data records Mean, std, min, max unique records % of zero values Avg string length To visualize the results, you can use visualize_statistics . This will show tables for both numeric and categorical features. Within this table you can see the frequency, percentage of missing values and some basic statistics.","title":"Visualize Dataset Statistics"},{"location":"developer_notes/engineering/data_validation/tfdv/#create-a-data-schema","text":"Simply put, a schema describes standard characteristics of your data such as column data types and expected data value range. The schema is created on a dataset that you consider as reference, and can be reused to validate other incoming datasets. TFDV allows you to automatically generate an initial version of the schema using the infer_schema() method. This returns a Schema protocol buffer containing the result. As mentioned in the TFX paper (Section 3.3), the results of the schema inference can be summarized as follows: The expected type of each feature. The expected presence of each feature, in terms of a minimum count and fraction of examples that must contain the feature. The expected valency of the feature in each example, i.e., minimum and maximum number of values. The expected domain of a feature, i.e., the small universe of values for a string feature, or range for an integer feature. Note: TFDV will not automatically create a domain for numerical types.","title":"Create a Data Schema"},{"location":"developer_notes/engineering/data_validation/tfdv/#comparing-train-vs-evaluation-datasets","text":"So right now we have made a schema for the training dataset ( train_df ). Now we need to generate statistics on the evaluation set ( eval_df ). The purpose of this is to allow you to compare statistics between the two sets to see if the values of the features lie within the same distribution. If not, then there is some type of skew occuring. TFDV allows you to do this by passing a few additional parameters to visualize_statistics . lhs_statistics : Required parameter. Expects an instance of DatasetFeatureStatisticsList. rhs_statistics : Expects an instance of DatasetFeatureStatisticsList to compare with lhs_statistics. lhs_name : Name of the lhs_statistics dataset. rhs_name : Name of the rhs_statistics dataset. As seen above, we can see that age do not share the same characteristics across the two datasets as min and max values are 0 and 1000 respectively. You can also see that 0.02% of the data is missing from the workclass feature. We can remove any rows that do not fit our expected values","title":"Comparing Train vs Evaluation Datasets"},{"location":"developer_notes/engineering/data_validation/tfdv/#displaying-anomalies","text":"We can use the reference schema that was made eariler to check for anomalies in the evaluation data. This is done using the validate_statistics and display_anomalies methods. Detected anomalies can either be considered a real error that needs to be cleaned, or depending on your domain knowledge and the specific case, they can be accepted. You can see below that TFDV caught a value for categorical variables that weren't expected. For example: some values in the race feature in the evaluation data wasn't in the schema. Lets say you encode these categorical variables and feed them as input features to the model, the model will break as the number of input features would not be the same as they were in training. You can see how catching anomalies like this is crucial for productionizing ML models.","title":"Displaying Anomalies"},{"location":"developer_notes/engineering/data_validation/tfdv/#updating-the-schema","text":"TFDV provides a set of utility methods and parameters that you can use for revising the inferred schema. This reference lists down the type of anomalies and the parameters that you can edit but we'll focus only on a couple here.","title":"Updating the Schema"},{"location":"developer_notes/engineering/data_validation/tfdv/#anomaly-detection-criteria","text":"If you want to allow some anomalies up to a certain point can relax the criteria for raising anomalies by using the distribution_constraints . As described here under ENUM_TYPE_UNEXPECTED_STRING_VALUES the detection criteria is calculated by (num of features not in domain / tot values) > (1 - feature.distribution_constraints.min_domain_mass In code this looks like: country_feature = tfdv . get_feature ( schema , 'native-country' ) country_feature . distribution_constraints . min_domain_mass = 0.9","title":"Anomaly Detection Criteria"},{"location":"developer_notes/engineering/data_validation/tfdv/#adding-new-domain-value","text":"From our example earlier, imagine we want to actually have the value of Asian in our schema. This can be done with the following # Add new value to the domain of the feature `race` race_domain = tfdv . get_domain ( schema , 'race' ) race_domain . value . append ( 'Asian' )","title":"Adding New Domain Value"},{"location":"developer_notes/engineering/data_validation/tfdv/#numerical-range-criteria","text":"As mentioned earlier, TFDV doesn't automatically make a domain for numeric values. If you recall from the display of statistics earlier that there was an age value of 1000. We can set a min and max value for this feature using the set_domain method. Now we can be sure we catch anomalies like that again in the future. If we run validate_statistics and display_anomalies again, after updating our schema and removing some anomalous rows, we can see that no anomalies are found.","title":"Numerical Range Criteria"},{"location":"developer_notes/engineering/data_validation/tfdv/#data-slicing","text":"TDFV allows you to analyze specific slices of your dataset, which can be used to see if each feature is well-represented in the dataset. For this example we will compare male vs female in the sex column. For this we used the slicing_util provided by TFDV. None in this context is used if you want to use the entire domain. from tensorflow_data_validation.utils import slicing_util slice_fn = slicing_util . get_feature_value_slicer ( features = { 'sex' : None }) You need to tell TFDV that you need statistics for the features you set and you can do that through the slice_functions argument of tfdv.StatsOptions . # Declare stats options slice_stats_options = tfdv . StatsOptions ( schema = schema , slice_functions = [ slice_fn ], infer_type_from_schema = True ) Generating sliced statistics only works for CSVs so you will need to convert the Pandas dataframe to a CSV. Passing the slice_stats_options to generate_statistics_from_dataframe() will not produce the expected results. Therefore, you will need to use generate_statistics_from_csv() # Convert dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv` CSV_PATH = 'slice_sample.csv' train_df . to_csv ( CSV_PATH ) # Calculate statistics for the sliced dataset sliced_stats = tfdv . generate_statistics_from_csv ( CSV_PATH , stats_options = slice_stats_options ) With that, you now have the statistics for the set slice. These are packed into a DatasetFeatureStatisticsList protocol buffer. You can see the dataset names below. The first element in the list (i.e. index=0) is named All_Examples which just contains the statistics for the entire dataset. The next two elements (i.e. named sex_Male and sex_Female ) are the datasets that contain the stats for the slices. It is important to note that these datasets are of the type: DatasetFeatureStatistics . You will see why this is important after the cell below. You can then visualize the statistics as before to examine the slices. An important caveat is visualize_statistics() accepts a DatasetFeatureStatisticsList type instead of DatasetFeatureStatistics . Thus, at least for this version of TFDV, you will need to convert it to the correct type. from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList # Convert `Male` statistics (index=1) to the correct type and get the dataset name male_stats_list = DatasetFeatureStatisticsList () male_stats_list . datasets . extend ([ sliced_stats . datasets [ 1 ]]) male_stats_name = sliced_stats . datasets [ 1 ] . name # Convert `Female` statistics (index=2) to the correct type and get the dataset name female_stats_list = DatasetFeatureStatisticsList () female_stats_list . datasets . extend ([ sliced_stats . datasets [ 2 ]]) female_stats_name = sliced_stats . datasets [ 2 ] . name # Visualize the two slices side by side tfdv . visualize_statistics ( lhs_statistics = male_stats_list , rhs_statistics = female_stats_list , lhs_name = male_stats_name , rhs_name = female_stats_name ) As you can see below, we have a similar view of the data. However, this time the results are stratified by the different sex column values. You can see here that within this dataset Male is disproportionately more represented than Female . As mentioned in our MLOps documentation data slicing is a useful method for asses your model for bias. These steps can be refactored for assessing slices of data after the model has produced output.","title":"Data Slicing"},{"location":"developer_notes/engineering/data_validation/tfdv/#references","text":"TensorFlow Data Validation (TFDV) TFX: A TensorFlow-Based Production-Scale Machine Learning Platform Example Notebook Medium: Data validation pipeline for production ML: Tensor flow data validation(TFDV)","title":"References"},{"location":"developer_notes/engineering/orchestration/airflow/","text":"Airflow Prerequisites To use AirFlow it is required that you have basic knowledge of Python and Docker. When downloading images from DockerHub you may need to disconnect from the company VPN. Overview Apache Airflow is an open-source tool made by AirBnB for developing, scheduling, and monitoring batch-oriented workflows using code. Airflow\u2019s extensible Python framework enables you to build workflows connecting with virtually any technology. A web interface helps manage the state of your workflows. Airflow is deployable in many ways, varying from a single process on your laptop to a distributed setup to support even the biggest workflows. Airflow follows a \"workflow-as-code\" principle. Traditionally, using Cronjobs was the default for scheduling events. However, some issues with this approach include poor failure handling, difficulty monitoring current and past processes and lack of scalability. Airflow aims to remedy these issues. Some nice features Can handle upstream/downstream dependencies gracefully. For example: if a step is dependent on another previous step to complete, it won't run until it passes successfully. If a prior task fails Airflow can automatically retry that step. Easy to see historical workflow runs Ease of deployment - easy integration with other technologies . You can also create your own provider packages. You can implement trigger event to spin off tasks Core Concepts DAGs A workflow in Airflow is represented a direct acyclic graph (DAG). A DAG is a collection of all the tasks you want to run, and in what order. Read more here Tasks So what is a task within a DAG? A task is defined as a unit of work. Represented as a node in the DAG graph. Tasks are written in Python. As you can see the graph above, task C is a downstream task of task A. Operators Operators call the thing you want to run. This could be a shell script, a bash command, a python script etc. There are many operators that can be used, and you can use multiple operators types within a DAG. Some terminology Dag Run: An instantiation of a DAG Task Instance: Specific point in time a task is executed Execution Date: Date and time when a dag run occurred Task Lifecycle There are 11 different states a task can be in, and these are represented with status colours within the dashboard UI. This tells us if there was any issues with the specific task, if it was successful and more. Read me here The scheduler within Airflow will take each task and determine which state it should be in. If there was no upstream issues, the task will be scheduled. From there it is picked up by the executor and queue it to be executed by the worker . Once complete, the task will be marked as success or failed. If the task is failed, or is prematurely cancelled, it will be marked as up for retry state. It will then become rescheduled for execution. Architecture WebServer : Used to serve the Airflow user interface. User Interface : Used to manage DAGs within a dashboard. Scheduler : Used to queue up new tasks for the Executor Workers : The component responsible for executing the task DAGs : Python files listing out workflows. These are visible by the UI, Scheduler, Workers and Executor Executor : The component that executes the task the has been queued. Metadata DB : In order for Airflow to be aware of the past and current state of the DAG, metadata is stored in a database. This can be an SQLite, MySQL or Postgres. airflow.cfg : The setup is configured by a data engineer within this file. This includes what DB to use, type of executor etc. Installation Official documentation here # Airflow needs a home. `~/airflow` is the default, but you can put it # somewhere else if you prefer (optional) export AIRFLOW_HOME = ~/airflow # Install Airflow using the constraints file AIRFLOW_VERSION = 2 .4.3 PYTHON_VERSION = \" $( python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1 -2 ) \" # For example: 3.7 CONSTRAINT_URL = \"https://raw.githubusercontent.com/apache/airflow/constraints- ${ AIRFLOW_VERSION } /constraints- ${ PYTHON_VERSION } .txt\" # For example: https://raw.githubusercontent.com/apache/airflow/constraints-2.4.3/constraints-3.7.txt pip install \"apache-airflow== ${ AIRFLOW_VERSION } \" --constraint \" ${ CONSTRAINT_URL } \" # The Standalone command will initialise the database, make a user, # and start all components for you. airflow standalone # Visit localhost:8080 in the browser and use the admin account details # shown on the terminal to login. # Enable the example_bash_operator dag in the home page Setup Docker You can run Airflow within a virtual environment and conda environment. However, Docker is the most suitable choice. There are two approaches you can take for spinning up Airflow within a docker environment: 1. Puckel/docker-airflow This provides an automated build of Docker for Airflow, and is suggested in the Microsoft documentation. However, this can be intimidating if you have never used Airflow before. It's worth mentioning that it appears to no longer be supported, therefore it is riddled with deprecation warnings. See github repo here . The main commands you will need: docker build --rm --build-arg AIRFLOW_DEPS = \"datadog,dask\" --build-arg PYTHON_DEPS = \"flask_oauthlib>=0.9\" -t puckel/docker-airflow:latest . docker run -d -p 8080 :8080 puckel/docker-airflow webserver 2. Official Docker Compose Airflow provide a docker run-through, but give the warning that it would be difficult to configure this if you are not experienced with docker. See Official documentation here . You can run through this setup guide. Unless you are using a Windows machine, you will need to run this command before spinning up anything: echo -e \"AIRFLOW_UID=$(id -u)\" > .env Here are the most useful commands you'll need: docker-compose up airflow-init need to run the first time your initializing the DB and base container. docker-compose up will spin up the containers in the background (add -d to run in background and free up the terminal) docker compose down will shutdown and clear all containers (add -v to remove all containers) How To: Schedule a DAG with DAG ( dag_id = \"example_dag\" , default_args = default_args , description = \"example dag\" , start_date = datetime ( 2022 , 11 , 2 , 12 , 51 ), #12:51 pm 2nd December 2022 schedule_interval = \"@daily\" ) as dag : You can schedule in what interval you would like to run a DAG, see here different cadences you can set the scheduler to run. This defined using the schedule_interval argument. It's worth noting that if you change the frequency of the DAG run, the metadata store will still persist the history of the previous runs, and highlight the future planned runs. You can see this in the calender tab of the dashboard. See the image below, where the cadence has been set to weekly. You can also use CRON expressions to set the cadence of when the DAG will run. See here for help on creating cron expressions. Again, you use the schedule_interval parameter for this: schedule_interval = \"0 0 * * *\" Make a DAG Lets say we want to make a DAG made up of tasks like the one shown above. Making a task can be easily done by instantiating the desired operator for your usecase. In this example below, we have 5 tasks using the BashOperator for executing bash commands, and the PythonOperator which has the ability to invoke python methods. task_one = BashOperator ( task_id = \"task_one\" , bash_command = \"echo hello world \" ) task_two = BashOperator ( task_id = \"task_two\" , bash_command = \"echo task 2\" ) task_three = BashOperator ( task_id = \"task_three\" , bash_command = \"echo task 3\" ) task_four = BashOperator ( task_id = \"task_four\" , bash_command = \"echo task 4\" ) task_five = PythonOperator ( task_id = \"task_five\" , python_callable = some_function , # protip: you can pass variables to the function you call # within the PythonOperator using the op_kwargs parameter op_kwargs = { 'string' : 'Hello' } ) So now that we have defined our individual tasks, we need to link them up into a directed graph. This can be done a number of ways. Traditionally, you would have used the .set_upstream() and .set_downstream() functions to indicate that the task object is dependent on another task object. task_one . set_downstream ( task_two ) task_one . set_downstream ( task_three ) task_two . set_downstream ( task_four ) task_three . set_downstream ( task_four ) task_four . set_downstream ( task_five ) Alternatively, since Airflow 1.8, this can be done with the Python >> and << operators. This is called BitShift Composition . task_one >> [ task_two , task_three ] task_two >> task_four task_three >> task_four task_four >> task_five There is a shorthand way of doing this using what's known as the Task Flow API. By using python decorators you can considerably decrease the code line-count. This approach isn't as verbose as the previous, so knowing where dependencies and flow of the tasks are not as intuitive. from airflow.decorators import dag , task @dag ( dag_id = 'taskflow_dag_v1' , default_args = default_args , start_date = datetime ( 2022 , 11 , 2 , 12 , 51 ), #12 pm 2nd December 2022 schedule_interval = \"@weekly\" ) def hello_world_etl (): @task () def get_name (): return \"Jonas\" @task () def greet ( name ): print ( f \"Hello { name } \" ) name = get_name () greet ( name = name ) example_dag = hello_world_etl () Share Task Metadata (XComs) Airflow gives you the ability to share information across different tasks using XComs (short for \u201ccross-communications\u201d). You can view the XComs table by navigating to Admin in the navigation bar. Airflow Operators implicitly pass the context variable ti , which is specific to airflow and stands for task instance . By default, every return value automatically gets added to XComs which can later be referenced in different task. You can also achieve this by using the ti.xcom_push() function. Using ti.xcom_pull() you can pull the metadata that was added to xcoms by passing the task_id . In the example below the log output from task_two will be \"Hello, i am Jerry Seinfield!\" . It's worth mentioning that XComs is not for large data sharing, as the maximum XComs variable size is 48KB. So it should only be used for passing around strings or paths to larger files, not the data itself. def greet ( ti ): # print(f\"Hello, i am {ti.xcom_pull(task_ids='task_one', key='name')}!\") # OR print ( f \"Hello, i am { ti . xcom_pull ( task_ids = 'task_one' ) } !\" ) def get_name ( ti ): # ti.xcom_push(key=\"name\", value=\"Jerry Seinfield\") # OR return 'Jerry Seinfield' task_one = PythonOperator ( task_id = \"task_one\" , python_callable = get_name , ) task_two = PythonOperator ( task_id = \"task_two\" , python_callable = greet ) Connect to a DB Lets say we want to expose the database to Airflow to store some data or log some important values. Firstly we need expose some ports in our database so we can query it using DBeaver. Within the docker-compose.yaml expose the postgres instance using the port 5432 docker-compose.yaml services : postgres : image : postgres:13 environment : POSTGRES_USER : airflow POSTGRES_PASSWORD : airflow POSTGRES_DB : airflow volumes : - postgres-db-volume:/var/lib/postgresql/data ports : - 5432:5432 Go to the Admin tab in the dashboard, and click Connections . Give it a connection name, this will be used to reference in the python code. You will see under Connection Type that we have a lot of different connections we can make. In this example we are using Postgres. When using docker you must use host.docker.internal , as localhost will not work in this instance. Lastly, provide the Airflow admin username and password and specify the 5432 port. You have use the PostgresOperator to interact with Postgres resources within a DAG. In this example we simply connect to posgres and create a table called dag_runs , then add the data and DAG ID to a row in the table. Note that the {{ }} symbols allow you to access environment variables, otherwise known as macros , within Airflow. For example {{ ds }} corresponds with the date. docker-compose.yaml task1 = PostgresOperator ( task_id = 'create_postgres_table' , postgres_conn_id = 'postgres_local' , #conn_id in dashboard: airflow/admin/connections sql = \"\"\" create table if not exists dag_runs ( dt date, dag_id character varying, primary key (dt, dag_id) ) \"\"\" ) task3 = PostgresOperator ( task_id = 'insert_into_table' , postgres_conn_id = 'postgres_local' , sql = \"\"\" insert into dag_runs (date, dag_id) values ('{{ ds }}', '{{ dag.dag_id }}') \"\"\" ) You can see here DBeaver is used to query the database table and values we just created with airflow. Install Python Packages It's most likely the case that you will just be using Airflow to orchestrate jobs in different environments. In the case that you need to install some dependencies in your Airflow container then you will need to extend the Airflow image. For this example we will be installing DVC. Firstly, you need to make the two files as seen below. By stating FROM apache/airflow:2.4.3 that means we are extending the base image, IE: adding a little something extra. We copy the requirements.txt and run a pip install which will make sure DVC is present in the docker container at runtime. requirements.txt dvc>=2.1.0 Dockerfile FROM apache/airflow:2.4.3 COPY requirements.txt /requirements.txt RUN pip install --user --upgrade pip RUN pip install --no-cache-dir --user -r /requirements.txt Now that you have defined the dependencies, you need to build the image. This can be done using: docker build . --tag <new_image_name>:latest replacing new_image_name with whatever name you like. You will now need to edit the docker-compose.yaml , replacing apache/airflow:2.4.3 with the name you just created. docker-compose.yaml version : '3' x-airflow-common : &airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image : ${AIRFLOW_IMAGE_NAME:-extended_airflow_image:latest} Then run docker compose up to spin up the docker containers. Lets make a simple task that proves DVC has been installed successfully. extended_dag.py def print_version (): import dvc print ( f \" { dvc . __version__ } \" ) task1 = PythonOperator ( task_id = 'print_version' , python_callable = print_version ) You will see in the Airflow task logs that the version was printed out as expected. We have successfully extended our docker image. Sensors A Sensor is a special type of operator that waits for event to occur. This is useful in cases where some task is dependent on a file, but you don't have certainty on when it will be available. You can view a list of the available Azure Sensor Operators here . In this example we will connect our local Airflow containers to the our teams Azure cloud subscription, and set up a listener to print \"hello new file\" when a new file appears within blob storage. Firstly, go to the Azure portal and find the blob container you'd like to apply the sensor to. Go to Access Keys and copy the connection string . Go to the Airflow Dashboard and make a new connection. Make note of the connection ID because you will need to reference this in the python code later. Fill out the rest of form similar to the screenshot below and paste in the Azure connection string in Blob Storage Connection String (optional) Within your python script, make a WasbBlobSensor object using the name of the container, the file its looking for. Make sure to give wasb_conn_id the connection ID you made earlier. with DAG ( dag_id = 'dag_with_sensor' , start_date = datetime ( 2022 , 12 , 1 ), ) as dag : wait_for_blob = WasbBlobSensor ( task_id = \"wait_for_blob\" , container_name = 'blob' , blob_name = 'test_sensor_file.csv' , wasb_conn_id = 'azure_blob_connect' ) This task will run continously until the criteria has been met (that the \"test_sensor_file.csv\" blob/file is within the container). I manually uploaded a file to show what this would look like in the real world. You should see something like this in the logs for the file that was uploaded: Caveats Airflow is not meant to be used for streaming workloads, it is only used for batch processing. Look into Apache Kafka as an alternative. You need some coding experience to use Airflow, as its discourages to use the UI. References Airflow: How-to Guides Configuration Reference Connecting to Azure Microsoft: Deploying Apache Airflow in Azure to build and run data pipelines Youtube: Airflow Tutorial for Beginners - Full Course in 2 Hours 2022 Cheatsheet airflow db init to initialize the sqllite database, airflow users create to make a new user airflow scheduler to turn on the scheduler docker compose up will start the scheduler, webserver, triggerer Known issues sudo chmod 666 /var/run/docker.sock","title":"Apache Airflow"},{"location":"developer_notes/engineering/orchestration/airflow/#airflow","text":"Prerequisites To use AirFlow it is required that you have basic knowledge of Python and Docker. When downloading images from DockerHub you may need to disconnect from the company VPN.","title":"Airflow"},{"location":"developer_notes/engineering/orchestration/airflow/#overview","text":"Apache Airflow is an open-source tool made by AirBnB for developing, scheduling, and monitoring batch-oriented workflows using code. Airflow\u2019s extensible Python framework enables you to build workflows connecting with virtually any technology. A web interface helps manage the state of your workflows. Airflow is deployable in many ways, varying from a single process on your laptop to a distributed setup to support even the biggest workflows. Airflow follows a \"workflow-as-code\" principle. Traditionally, using Cronjobs was the default for scheduling events. However, some issues with this approach include poor failure handling, difficulty monitoring current and past processes and lack of scalability. Airflow aims to remedy these issues. Some nice features Can handle upstream/downstream dependencies gracefully. For example: if a step is dependent on another previous step to complete, it won't run until it passes successfully. If a prior task fails Airflow can automatically retry that step. Easy to see historical workflow runs Ease of deployment - easy integration with other technologies . You can also create your own provider packages. You can implement trigger event to spin off tasks","title":"Overview"},{"location":"developer_notes/engineering/orchestration/airflow/#core-concepts","text":"","title":"Core Concepts"},{"location":"developer_notes/engineering/orchestration/airflow/#dags","text":"A workflow in Airflow is represented a direct acyclic graph (DAG). A DAG is a collection of all the tasks you want to run, and in what order. Read more here","title":"DAGs"},{"location":"developer_notes/engineering/orchestration/airflow/#tasks","text":"So what is a task within a DAG? A task is defined as a unit of work. Represented as a node in the DAG graph. Tasks are written in Python. As you can see the graph above, task C is a downstream task of task A.","title":"Tasks"},{"location":"developer_notes/engineering/orchestration/airflow/#operators","text":"Operators call the thing you want to run. This could be a shell script, a bash command, a python script etc. There are many operators that can be used, and you can use multiple operators types within a DAG. Some terminology Dag Run: An instantiation of a DAG Task Instance: Specific point in time a task is executed Execution Date: Date and time when a dag run occurred","title":"Operators"},{"location":"developer_notes/engineering/orchestration/airflow/#task-lifecycle","text":"There are 11 different states a task can be in, and these are represented with status colours within the dashboard UI. This tells us if there was any issues with the specific task, if it was successful and more. Read me here The scheduler within Airflow will take each task and determine which state it should be in. If there was no upstream issues, the task will be scheduled. From there it is picked up by the executor and queue it to be executed by the worker . Once complete, the task will be marked as success or failed. If the task is failed, or is prematurely cancelled, it will be marked as up for retry state. It will then become rescheduled for execution.","title":"Task Lifecycle"},{"location":"developer_notes/engineering/orchestration/airflow/#architecture","text":"WebServer : Used to serve the Airflow user interface. User Interface : Used to manage DAGs within a dashboard. Scheduler : Used to queue up new tasks for the Executor Workers : The component responsible for executing the task DAGs : Python files listing out workflows. These are visible by the UI, Scheduler, Workers and Executor Executor : The component that executes the task the has been queued. Metadata DB : In order for Airflow to be aware of the past and current state of the DAG, metadata is stored in a database. This can be an SQLite, MySQL or Postgres. airflow.cfg : The setup is configured by a data engineer within this file. This includes what DB to use, type of executor etc.","title":"Architecture"},{"location":"developer_notes/engineering/orchestration/airflow/#installation","text":"Official documentation here # Airflow needs a home. `~/airflow` is the default, but you can put it # somewhere else if you prefer (optional) export AIRFLOW_HOME = ~/airflow # Install Airflow using the constraints file AIRFLOW_VERSION = 2 .4.3 PYTHON_VERSION = \" $( python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1 -2 ) \" # For example: 3.7 CONSTRAINT_URL = \"https://raw.githubusercontent.com/apache/airflow/constraints- ${ AIRFLOW_VERSION } /constraints- ${ PYTHON_VERSION } .txt\" # For example: https://raw.githubusercontent.com/apache/airflow/constraints-2.4.3/constraints-3.7.txt pip install \"apache-airflow== ${ AIRFLOW_VERSION } \" --constraint \" ${ CONSTRAINT_URL } \" # The Standalone command will initialise the database, make a user, # and start all components for you. airflow standalone # Visit localhost:8080 in the browser and use the admin account details # shown on the terminal to login. # Enable the example_bash_operator dag in the home page","title":"Installation"},{"location":"developer_notes/engineering/orchestration/airflow/#setup-docker","text":"You can run Airflow within a virtual environment and conda environment. However, Docker is the most suitable choice. There are two approaches you can take for spinning up Airflow within a docker environment:","title":"Setup Docker"},{"location":"developer_notes/engineering/orchestration/airflow/#1-puckeldocker-airflow","text":"This provides an automated build of Docker for Airflow, and is suggested in the Microsoft documentation. However, this can be intimidating if you have never used Airflow before. It's worth mentioning that it appears to no longer be supported, therefore it is riddled with deprecation warnings. See github repo here . The main commands you will need: docker build --rm --build-arg AIRFLOW_DEPS = \"datadog,dask\" --build-arg PYTHON_DEPS = \"flask_oauthlib>=0.9\" -t puckel/docker-airflow:latest . docker run -d -p 8080 :8080 puckel/docker-airflow webserver","title":"1. Puckel/docker-airflow"},{"location":"developer_notes/engineering/orchestration/airflow/#2-official-docker-compose","text":"Airflow provide a docker run-through, but give the warning that it would be difficult to configure this if you are not experienced with docker. See Official documentation here . You can run through this setup guide. Unless you are using a Windows machine, you will need to run this command before spinning up anything: echo -e \"AIRFLOW_UID=$(id -u)\" > .env Here are the most useful commands you'll need: docker-compose up airflow-init need to run the first time your initializing the DB and base container. docker-compose up will spin up the containers in the background (add -d to run in background and free up the terminal) docker compose down will shutdown and clear all containers (add -v to remove all containers)","title":"2. Official Docker Compose"},{"location":"developer_notes/engineering/orchestration/airflow/#how-to","text":"","title":"How To:"},{"location":"developer_notes/engineering/orchestration/airflow/#schedule-a-dag","text":"with DAG ( dag_id = \"example_dag\" , default_args = default_args , description = \"example dag\" , start_date = datetime ( 2022 , 11 , 2 , 12 , 51 ), #12:51 pm 2nd December 2022 schedule_interval = \"@daily\" ) as dag : You can schedule in what interval you would like to run a DAG, see here different cadences you can set the scheduler to run. This defined using the schedule_interval argument. It's worth noting that if you change the frequency of the DAG run, the metadata store will still persist the history of the previous runs, and highlight the future planned runs. You can see this in the calender tab of the dashboard. See the image below, where the cadence has been set to weekly. You can also use CRON expressions to set the cadence of when the DAG will run. See here for help on creating cron expressions. Again, you use the schedule_interval parameter for this: schedule_interval = \"0 0 * * *\"","title":"Schedule a DAG"},{"location":"developer_notes/engineering/orchestration/airflow/#make-a-dag","text":"Lets say we want to make a DAG made up of tasks like the one shown above. Making a task can be easily done by instantiating the desired operator for your usecase. In this example below, we have 5 tasks using the BashOperator for executing bash commands, and the PythonOperator which has the ability to invoke python methods. task_one = BashOperator ( task_id = \"task_one\" , bash_command = \"echo hello world \" ) task_two = BashOperator ( task_id = \"task_two\" , bash_command = \"echo task 2\" ) task_three = BashOperator ( task_id = \"task_three\" , bash_command = \"echo task 3\" ) task_four = BashOperator ( task_id = \"task_four\" , bash_command = \"echo task 4\" ) task_five = PythonOperator ( task_id = \"task_five\" , python_callable = some_function , # protip: you can pass variables to the function you call # within the PythonOperator using the op_kwargs parameter op_kwargs = { 'string' : 'Hello' } ) So now that we have defined our individual tasks, we need to link them up into a directed graph. This can be done a number of ways. Traditionally, you would have used the .set_upstream() and .set_downstream() functions to indicate that the task object is dependent on another task object. task_one . set_downstream ( task_two ) task_one . set_downstream ( task_three ) task_two . set_downstream ( task_four ) task_three . set_downstream ( task_four ) task_four . set_downstream ( task_five ) Alternatively, since Airflow 1.8, this can be done with the Python >> and << operators. This is called BitShift Composition . task_one >> [ task_two , task_three ] task_two >> task_four task_three >> task_four task_four >> task_five There is a shorthand way of doing this using what's known as the Task Flow API. By using python decorators you can considerably decrease the code line-count. This approach isn't as verbose as the previous, so knowing where dependencies and flow of the tasks are not as intuitive. from airflow.decorators import dag , task @dag ( dag_id = 'taskflow_dag_v1' , default_args = default_args , start_date = datetime ( 2022 , 11 , 2 , 12 , 51 ), #12 pm 2nd December 2022 schedule_interval = \"@weekly\" ) def hello_world_etl (): @task () def get_name (): return \"Jonas\" @task () def greet ( name ): print ( f \"Hello { name } \" ) name = get_name () greet ( name = name ) example_dag = hello_world_etl ()","title":"Make a DAG"},{"location":"developer_notes/engineering/orchestration/airflow/#share-task-metadata-xcoms","text":"Airflow gives you the ability to share information across different tasks using XComs (short for \u201ccross-communications\u201d). You can view the XComs table by navigating to Admin in the navigation bar. Airflow Operators implicitly pass the context variable ti , which is specific to airflow and stands for task instance . By default, every return value automatically gets added to XComs which can later be referenced in different task. You can also achieve this by using the ti.xcom_push() function. Using ti.xcom_pull() you can pull the metadata that was added to xcoms by passing the task_id . In the example below the log output from task_two will be \"Hello, i am Jerry Seinfield!\" . It's worth mentioning that XComs is not for large data sharing, as the maximum XComs variable size is 48KB. So it should only be used for passing around strings or paths to larger files, not the data itself. def greet ( ti ): # print(f\"Hello, i am {ti.xcom_pull(task_ids='task_one', key='name')}!\") # OR print ( f \"Hello, i am { ti . xcom_pull ( task_ids = 'task_one' ) } !\" ) def get_name ( ti ): # ti.xcom_push(key=\"name\", value=\"Jerry Seinfield\") # OR return 'Jerry Seinfield' task_one = PythonOperator ( task_id = \"task_one\" , python_callable = get_name , ) task_two = PythonOperator ( task_id = \"task_two\" , python_callable = greet )","title":"Share Task Metadata (XComs)"},{"location":"developer_notes/engineering/orchestration/airflow/#connect-to-a-db","text":"Lets say we want to expose the database to Airflow to store some data or log some important values. Firstly we need expose some ports in our database so we can query it using DBeaver. Within the docker-compose.yaml expose the postgres instance using the port 5432 docker-compose.yaml services : postgres : image : postgres:13 environment : POSTGRES_USER : airflow POSTGRES_PASSWORD : airflow POSTGRES_DB : airflow volumes : - postgres-db-volume:/var/lib/postgresql/data ports : - 5432:5432 Go to the Admin tab in the dashboard, and click Connections . Give it a connection name, this will be used to reference in the python code. You will see under Connection Type that we have a lot of different connections we can make. In this example we are using Postgres. When using docker you must use host.docker.internal , as localhost will not work in this instance. Lastly, provide the Airflow admin username and password and specify the 5432 port. You have use the PostgresOperator to interact with Postgres resources within a DAG. In this example we simply connect to posgres and create a table called dag_runs , then add the data and DAG ID to a row in the table. Note that the {{ }} symbols allow you to access environment variables, otherwise known as macros , within Airflow. For example {{ ds }} corresponds with the date. docker-compose.yaml task1 = PostgresOperator ( task_id = 'create_postgres_table' , postgres_conn_id = 'postgres_local' , #conn_id in dashboard: airflow/admin/connections sql = \"\"\" create table if not exists dag_runs ( dt date, dag_id character varying, primary key (dt, dag_id) ) \"\"\" ) task3 = PostgresOperator ( task_id = 'insert_into_table' , postgres_conn_id = 'postgres_local' , sql = \"\"\" insert into dag_runs (date, dag_id) values ('{{ ds }}', '{{ dag.dag_id }}') \"\"\" ) You can see here DBeaver is used to query the database table and values we just created with airflow.","title":"Connect to a DB"},{"location":"developer_notes/engineering/orchestration/airflow/#install-python-packages","text":"It's most likely the case that you will just be using Airflow to orchestrate jobs in different environments. In the case that you need to install some dependencies in your Airflow container then you will need to extend the Airflow image. For this example we will be installing DVC. Firstly, you need to make the two files as seen below. By stating FROM apache/airflow:2.4.3 that means we are extending the base image, IE: adding a little something extra. We copy the requirements.txt and run a pip install which will make sure DVC is present in the docker container at runtime. requirements.txt dvc>=2.1.0 Dockerfile FROM apache/airflow:2.4.3 COPY requirements.txt /requirements.txt RUN pip install --user --upgrade pip RUN pip install --no-cache-dir --user -r /requirements.txt Now that you have defined the dependencies, you need to build the image. This can be done using: docker build . --tag <new_image_name>:latest replacing new_image_name with whatever name you like. You will now need to edit the docker-compose.yaml , replacing apache/airflow:2.4.3 with the name you just created. docker-compose.yaml version : '3' x-airflow-common : &airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image : ${AIRFLOW_IMAGE_NAME:-extended_airflow_image:latest} Then run docker compose up to spin up the docker containers. Lets make a simple task that proves DVC has been installed successfully. extended_dag.py def print_version (): import dvc print ( f \" { dvc . __version__ } \" ) task1 = PythonOperator ( task_id = 'print_version' , python_callable = print_version ) You will see in the Airflow task logs that the version was printed out as expected. We have successfully extended our docker image.","title":"Install Python Packages"},{"location":"developer_notes/engineering/orchestration/airflow/#sensors","text":"A Sensor is a special type of operator that waits for event to occur. This is useful in cases where some task is dependent on a file, but you don't have certainty on when it will be available. You can view a list of the available Azure Sensor Operators here . In this example we will connect our local Airflow containers to the our teams Azure cloud subscription, and set up a listener to print \"hello new file\" when a new file appears within blob storage. Firstly, go to the Azure portal and find the blob container you'd like to apply the sensor to. Go to Access Keys and copy the connection string . Go to the Airflow Dashboard and make a new connection. Make note of the connection ID because you will need to reference this in the python code later. Fill out the rest of form similar to the screenshot below and paste in the Azure connection string in Blob Storage Connection String (optional) Within your python script, make a WasbBlobSensor object using the name of the container, the file its looking for. Make sure to give wasb_conn_id the connection ID you made earlier. with DAG ( dag_id = 'dag_with_sensor' , start_date = datetime ( 2022 , 12 , 1 ), ) as dag : wait_for_blob = WasbBlobSensor ( task_id = \"wait_for_blob\" , container_name = 'blob' , blob_name = 'test_sensor_file.csv' , wasb_conn_id = 'azure_blob_connect' ) This task will run continously until the criteria has been met (that the \"test_sensor_file.csv\" blob/file is within the container). I manually uploaded a file to show what this would look like in the real world. You should see something like this in the logs for the file that was uploaded:","title":"Sensors"},{"location":"developer_notes/engineering/orchestration/airflow/#caveats","text":"Airflow is not meant to be used for streaming workloads, it is only used for batch processing. Look into Apache Kafka as an alternative. You need some coding experience to use Airflow, as its discourages to use the UI.","title":"Caveats"},{"location":"developer_notes/engineering/orchestration/airflow/#references","text":"Airflow: How-to Guides Configuration Reference Connecting to Azure Microsoft: Deploying Apache Airflow in Azure to build and run data pipelines Youtube: Airflow Tutorial for Beginners - Full Course in 2 Hours 2022","title":"References"},{"location":"developer_notes/engineering/orchestration/airflow/#cheatsheet","text":"airflow db init to initialize the sqllite database, airflow users create to make a new user airflow scheduler to turn on the scheduler docker compose up will start the scheduler, webserver, triggerer","title":"Cheatsheet"},{"location":"developer_notes/engineering/orchestration/airflow/#known-issues","text":"sudo chmod 666 /var/run/docker.sock","title":"Known issues"},{"location":"developer_notes/engineering/tools/idp_geo_map/","text":"Overview \ud83d\udc4b The Optum-geo library is a tool developed for python to convert datasets from one geo-location to another. Lets say for example you have a time series dataset that is on a county (FIPS) level, but you want to convert it to a state level. This will aggregate all the relevant data respective to each county within each state. This isn't just restricted to county to state mappings. Check the available mappings below: Zip3 \u2192 FIPS FIPS \u2192 CBSA FIPS \u2192 CSA FIPS \u2192 State FIPS \u2192 Micropolitan Statistical Area FIPS \u2192 Metropolitan Statistical Area Understanding USE Geo Resolutions\ud83d\uddfa County Level FIPS FIPS the lowest resolution in this library. A Federal Information Processing Series is 5 digit numeric code. The first two digits relate to US states and the three digits relate to the county. (3142 counties) Example: FIPS 06071 relates to, 06-California and 071-San Bernardino County Core Based Statistical Areas (CBSA) CBSAs represent geographic entities based on the concept of a core area with a large population nucleus, plus adjacent communities having a high degree of social and economic integration with that core. A CBSA consists of a U.S. county or counties associated with at least one urban core ( with a population of at least 10,000) along with any adjacent counties having a high degree of social and economic integration with the core as measured through commuting ties with the counties containing the core. The offical list of CBSA can be obtained from US Census Delineation Files CBSA are further categorized as Metropolitian statisitical area MSA where the urbanized core of the CBSA is 50,000 or more population (392 such areas, 1180 counites, representing 86.1% of the US population) Micropolitan statistical areas where the urban cluster is of at least 10,000 and less than 50,000 population (546 such areas, 661 counites, representing 8.3% of the US population) Codes for Metropolitan and Micropolitan Statistical Areas and Metropolitan Divisions fall within the 10000 to 49999 range and are assigned in alphabetical order by area title. Metropolitan Divisions are distinguished by a5-digit code ending in 4. Combined Statiscal Areas (CSA) are adjacent Metropolitan and Micropolitan Statistical Areas, in various combinations, that have social and economic ties as measured by commuting, but at lower levels than are found among counties within CBSAs. This can be characterized as representing larger regions that reflect broader social and economic interactions, such as wholesaling, commodity distribution, and weekend recreation activities. (175 such areas, representing 551 metro & micro CBSA) - Combined Statistical Area and Combined NECTA(New England City & Town Areas) codes are 3 digits in length. Combined Statistical Area codes fall within the 100 to 599 range. Combined NECTA codes fall within the 700 to 799 range. A detailed explanation of the above can be found at Office of Management Budget . Zone Improvement Plan ZIP Codes are probably the most recognisable of all geo codes. Zip codes are not drawn according to to county boundaries. They are created by the the United States Postal Service. 20% of ZIP codes lie in more than 1 county. ZIP is broken down along national area (digit 1), region or city (digit 2-3), delivery/postal office area(digts 4-5) Example Here is an example of how the library will convert FIPS level data to CBSA level. The CBSA 26700 is made up of two counties (46073 and 46005). Alternatively, see the examples notebook for more examples on how to use it. Input Example of a datafame in FIPS level with 46073 and 46005 FIPS This data is used as input fips county province_state country date confirmed 46073 Jerauld South Dakota US 2020-07-05 39 46073 Jerauld South Dakota US 2020-07-06 39 46073 Jerauld South Dakota US 2020-07-07 39 46073 Jerauld South Dakota US 2020-07-08 39 46073 Jerauld South Dakota US 2020-07-09 39 46005 Beadle South Dakota US 2020-07-05 540 46005 Beadle South Dakota US 2020-07-06 541 46005 Beadle South Dakota US 2020-07-07 545 46005 Beadle South Dakota US 2020-07-08 549 46005 Beadle South Dakota US 2020-07-09 551 Output The package will output an aggregated version on a CBSA level confirmed cbsa_name state date cbsa 579 Huron, SD South Dakota 2020-07-05 26700 580 Huron, SD South Dakota 2020-07-06 26700 584 Huron, SD South Dakota 2020-07-07 26700 588 Huron, SD South Dakota 2020-07-08 26700 590 Huron, SD South Dakota 2020-07-09 26700 Datasources Source files that are used to make mapping files in /src/optum_geo/data are named by the source_target resolution_year update and are saved in their respective folders. Warning: As the census bureau updates these resolutions roughly once a year, ensure that you are using the most up to data mappings. Resolution Source Link Census Bureau FIPS to CBSA source Y FIPS to CSA source Y FIPS to State source (second file) Y FIPS to MSA source Y ZIP3 to FIPS Optum Maps N Installation The optum-geo libary is very easy to get up and running. To do this you need to: Open your terminal clone the repo . cd ./optum-geo-standard Now you will need to install like any other pip installable package. (make sure you have activated your conda environment) pip install . Boom! now you have the idp geo mapper library installed \ud83d\udc4d How to use To use the package you import it like any other python package. from optum_geo.aggregation import aggregator Now you are ready to start converting your data. Here is an example below of how to convert a fips time series dataset into a CBSA dataset df , _ = aggregator . aggregate_data ( data , current_res = 'fips' , target_res = 'CBSA' , current_res_col = 'fips' , group_by = [ 'date' ], convert_columns = [ 'confirmed' ], collect_unmappable = True , mapping_file = None , map_type = None , mapping_src = 'census' ) aggregator.aggregate_data Function takes in a data frame and converts the current geographical resolution to a higher one. Optum-geo uses data provided by the US census bureau for it's mapping files, with the exception of mapping Zip3 to FIPS which is provided by the Optum Maps team. Args: data (pd.DataFrame) : the dataframe a user wishes to convert; be careful the given dataframe will be modified current_res (str) : the current geographical resolution column the data is in target_res (str) : the resolution the user wants to convert to [fips, zip3, CBSA, CSA] group_by (str) : these columns will be used - in addition to target_res - to group the dataframe rows and aggregate the convert_columns convert_columns (list) : the columns they wish to aggregate [ex: confirmed_cases , deaths etc.] mapping_file : if you have a file that is not included, it can be imported (target resolution must be lowercas and acronym only) map_type(str or None) : only need to be specified if a custom mapping_file is used; must be MAP_TYPE_FUNCTION (a current res maps to at most one target) or MAP_TYPE_ONE_TO_MANY (a current res can map to multiple targets) collect_unmappable(bool) : if set to True , maps everything that cannot be mapped to a target_res to None instead mapping_src (string) : Choose the source from where the mapping files are generated [ex: census or optum_maps] Returns: pd.DataFrame : result of aggregation in a dataframe and a dictionary containing meta information","title":"Idp geo map"},{"location":"developer_notes/engineering/tools/idp_geo_map/#overview","text":"The Optum-geo library is a tool developed for python to convert datasets from one geo-location to another. Lets say for example you have a time series dataset that is on a county (FIPS) level, but you want to convert it to a state level. This will aggregate all the relevant data respective to each county within each state. This isn't just restricted to county to state mappings. Check the available mappings below: Zip3 \u2192 FIPS FIPS \u2192 CBSA FIPS \u2192 CSA FIPS \u2192 State FIPS \u2192 Micropolitan Statistical Area FIPS \u2192 Metropolitan Statistical Area Understanding USE Geo Resolutions\ud83d\uddfa","title":"Overview \ud83d\udc4b"},{"location":"developer_notes/engineering/tools/idp_geo_map/#example","text":"Here is an example of how the library will convert FIPS level data to CBSA level. The CBSA 26700 is made up of two counties (46073 and 46005). Alternatively, see the examples notebook for more examples on how to use it. Input Example of a datafame in FIPS level with 46073 and 46005 FIPS This data is used as input fips county province_state country date confirmed 46073 Jerauld South Dakota US 2020-07-05 39 46073 Jerauld South Dakota US 2020-07-06 39 46073 Jerauld South Dakota US 2020-07-07 39 46073 Jerauld South Dakota US 2020-07-08 39 46073 Jerauld South Dakota US 2020-07-09 39 46005 Beadle South Dakota US 2020-07-05 540 46005 Beadle South Dakota US 2020-07-06 541 46005 Beadle South Dakota US 2020-07-07 545 46005 Beadle South Dakota US 2020-07-08 549 46005 Beadle South Dakota US 2020-07-09 551 Output The package will output an aggregated version on a CBSA level confirmed cbsa_name state date cbsa 579 Huron, SD South Dakota 2020-07-05 26700 580 Huron, SD South Dakota 2020-07-06 26700 584 Huron, SD South Dakota 2020-07-07 26700 588 Huron, SD South Dakota 2020-07-08 26700 590 Huron, SD South Dakota 2020-07-09 26700","title":"Example"},{"location":"developer_notes/engineering/tools/idp_geo_map/#datasources","text":"Source files that are used to make mapping files in /src/optum_geo/data are named by the source_target resolution_year update and are saved in their respective folders. Warning: As the census bureau updates these resolutions roughly once a year, ensure that you are using the most up to data mappings. Resolution Source Link Census Bureau FIPS to CBSA source Y FIPS to CSA source Y FIPS to State source (second file) Y FIPS to MSA source Y ZIP3 to FIPS Optum Maps N","title":"Datasources"},{"location":"developer_notes/engineering/tools/idp_geo_map/#installation","text":"The optum-geo libary is very easy to get up and running. To do this you need to: Open your terminal clone the repo . cd ./optum-geo-standard Now you will need to install like any other pip installable package. (make sure you have activated your conda environment) pip install . Boom! now you have the idp geo mapper library installed \ud83d\udc4d","title":"Installation"},{"location":"developer_notes/engineering/tools/idp_geo_map/#how-to-use","text":"To use the package you import it like any other python package. from optum_geo.aggregation import aggregator Now you are ready to start converting your data. Here is an example below of how to convert a fips time series dataset into a CBSA dataset df , _ = aggregator . aggregate_data ( data , current_res = 'fips' , target_res = 'CBSA' , current_res_col = 'fips' , group_by = [ 'date' ], convert_columns = [ 'confirmed' ], collect_unmappable = True , mapping_file = None , map_type = None , mapping_src = 'census' ) aggregator.aggregate_data Function takes in a data frame and converts the current geographical resolution to a higher one. Optum-geo uses data provided by the US census bureau for it's mapping files, with the exception of mapping Zip3 to FIPS which is provided by the Optum Maps team. Args: data (pd.DataFrame) : the dataframe a user wishes to convert; be careful the given dataframe will be modified current_res (str) : the current geographical resolution column the data is in target_res (str) : the resolution the user wants to convert to [fips, zip3, CBSA, CSA] group_by (str) : these columns will be used - in addition to target_res - to group the dataframe rows and aggregate the convert_columns convert_columns (list) : the columns they wish to aggregate [ex: confirmed_cases , deaths etc.] mapping_file : if you have a file that is not included, it can be imported (target resolution must be lowercas and acronym only) map_type(str or None) : only need to be specified if a custom mapping_file is used; must be MAP_TYPE_FUNCTION (a current res maps to at most one target) or MAP_TYPE_ONE_TO_MANY (a current res can map to multiple targets) collect_unmappable(bool) : if set to True , maps everything that cannot be mapped to a target_res to None instead mapping_src (string) : Choose the source from where the mapping files are generated [ex: census or optum_maps] Returns: pd.DataFrame : result of aggregation in a dataframe and a dictionary containing meta information","title":"How to use"},{"location":"developer_notes/engineering/tools/idp_retain/","text":"IDP Retain CLI Tool A standardized PyPi installable package for RETAIN preprocessing and modelling, that can be used for any project. Setting up the data preprocessing notebooks, modelling notebooks and peech library across all projects using RETAIN will introduce time wasting and overhead. See github repository here","title":"IDP Retain CLI Tool"},{"location":"developer_notes/engineering/tools/idp_retain/#idp-retain-cli-tool","text":"A standardized PyPi installable package for RETAIN preprocessing and modelling, that can be used for any project. Setting up the data preprocessing notebooks, modelling notebooks and peech library across all projects using RETAIN will introduce time wasting and overhead. See github repository here","title":"IDP Retain CLI Tool"},{"location":"developer_notes/engineering/tools/idp_tools/","text":"Useful IDP in-house built tools Here is a list of resuable assets that can be shared and utilized across projects and teams. These tools can be machine learning libraries, automation tools, preprocessing utilities etc. Name Link Domain Description Documentation idp-retain Link Machine Learning A standardized PyPi installable package for RETAIN preprocessing and modelling, that can be used for any project. Setting up the data preprocessing notebooks, modelling notebooks and peech library across all projects using RETAIN will introduce time wasting and overhead.vz Documentation available on the repository README idp-geo-mapper Link Data Preprocess, time series The IDP Geo Mapper library is a tool developed for python to convert datasets from one geo-location to another. Lets say for example you have a time series dataset that is on a county (FIPS) level, but you want to convert it to a state level. This will aggregate all the relevant data respective to each county within each state. This isn't just restricted to county to state mappings. Read the docs here","title":"Useful IDP in-house built tools"},{"location":"developer_notes/engineering/tools/idp_tools/#useful-idp-in-house-built-tools","text":"Here is a list of resuable assets that can be shared and utilized across projects and teams. These tools can be machine learning libraries, automation tools, preprocessing utilities etc. Name Link Domain Description Documentation idp-retain Link Machine Learning A standardized PyPi installable package for RETAIN preprocessing and modelling, that can be used for any project. Setting up the data preprocessing notebooks, modelling notebooks and peech library across all projects using RETAIN will introduce time wasting and overhead.vz Documentation available on the repository README idp-geo-mapper Link Data Preprocess, time series The IDP Geo Mapper library is a tool developed for python to convert datasets from one geo-location to another. Lets say for example you have a time series dataset that is on a county (FIPS) level, but you want to convert it to a state level. This will aggregate all the relevant data respective to each county within each state. This isn't just restricted to county to state mappings. Read the docs here","title":"Useful IDP in-house built tools"},{"location":"developer_notes/operations/git/git_actions/","text":"GitHub Actions GitHub Actions are built in plugins that allow you to automate sepcific \"actions\" within your workflow. These are used to simplify development and reduce the amount of friction when making updates to code on a repo, IE: deploying an application. See official documentation here Terminology: Workflow : A yaml file listing out all of the automated steps Events : A trigger for jobs to run (ex: on: push ) Jobs : A collection of steps that do something (ex jobs: ) Runners : A container environment used to run our code and perform checks (ex: runs-on: ubuntu latest ) Steps : A collection of sequential steps of actions (ex: steps: ) Actions : An API that can be used to perform a task (ex: uses: actions/super-linter@v3 ) Use premade Actions Rather than you trying to set up your own github actions workflow file, you can use prebuilt github actions by navigativing to the Actions marketplace Click Actions tab Use the Search Workflows search bar Find the action you want to use. In this example i'll use Super-linter which is a automated linter that checks if python is formatted correctly. Click Configure , and review the boilerplate yaml code placed into the file. Name the file as needed, but can be left to default. Select Start Commit , add a description and select Commit . Once this code reached the master branch, you will see in the Actions tab that there is a job building You will be able to view job logs if you click into the workflow The showcase how Actions are actually useful, we will show how a linter would catch poorly formatted code, and suggest changes to make sure the code being submitted follows conventions and standards. For example, in the code snippet below you can see that one of the functions are using camel casing (which isn't used in Python function naming) and there is no spacing between the functions. bad_code.py def ExampleFunction (): \"\"\"Example function\"\"\" return None def example_function_2 (): return True print ( example_function_2 ()) You will see that the job is running in the background. The configured workflow is calling the Action to check the code Go view the logs to see what code failed against the linter. You can see below it will suggest what changes to make to the code. Go back, check the code, make the suggested updates and push to the main branch again. You should see that the job will build successfully. The validated code is now present on the main branch. Set up your own Actions Go to your repo Select create new file. We need to be very specific with the naming, when typing, first enter .github + / + workflows + / . Then you can name the workflow file whatever you like. Write your workflow (may take a few attempts to get it working correctly) Commit Request Action Approval If you aren't able to find what you need in the UGH-approved actions , you can request to get one approved here . The Action, if open source, must be registered with, and pass the license and vulnerability scan in Barista before it will be considered. When you have passed the scan, you can submit an Issue to request this Action be approved. Note: approvals may take some time. Recommended GitHub Actions templates There is a list of internal Actions you can use that have been approved by UHG Go to actions template repo to see find useful github workflow files to incorporate into your project repository. Useful Resources Youtube: 5 Ways to DevOps-ify your App - Github Actions Tutorial Blog: Github Actions\u2014 Everything You Need to Know to Get Started Blog: Using GitHub Actions for MLOps & Data Science Git Repo: Awesome Actions Git Repo: starter workflows","title":"Actions"},{"location":"developer_notes/operations/git/git_actions/#github-actions","text":"GitHub Actions are built in plugins that allow you to automate sepcific \"actions\" within your workflow. These are used to simplify development and reduce the amount of friction when making updates to code on a repo, IE: deploying an application. See official documentation here Terminology: Workflow : A yaml file listing out all of the automated steps Events : A trigger for jobs to run (ex: on: push ) Jobs : A collection of steps that do something (ex jobs: ) Runners : A container environment used to run our code and perform checks (ex: runs-on: ubuntu latest ) Steps : A collection of sequential steps of actions (ex: steps: ) Actions : An API that can be used to perform a task (ex: uses: actions/super-linter@v3 )","title":"GitHub Actions"},{"location":"developer_notes/operations/git/git_actions/#use-premade-actions","text":"Rather than you trying to set up your own github actions workflow file, you can use prebuilt github actions by navigativing to the Actions marketplace Click Actions tab Use the Search Workflows search bar Find the action you want to use. In this example i'll use Super-linter which is a automated linter that checks if python is formatted correctly. Click Configure , and review the boilerplate yaml code placed into the file. Name the file as needed, but can be left to default. Select Start Commit , add a description and select Commit . Once this code reached the master branch, you will see in the Actions tab that there is a job building You will be able to view job logs if you click into the workflow The showcase how Actions are actually useful, we will show how a linter would catch poorly formatted code, and suggest changes to make sure the code being submitted follows conventions and standards. For example, in the code snippet below you can see that one of the functions are using camel casing (which isn't used in Python function naming) and there is no spacing between the functions. bad_code.py def ExampleFunction (): \"\"\"Example function\"\"\" return None def example_function_2 (): return True print ( example_function_2 ()) You will see that the job is running in the background. The configured workflow is calling the Action to check the code Go view the logs to see what code failed against the linter. You can see below it will suggest what changes to make to the code. Go back, check the code, make the suggested updates and push to the main branch again. You should see that the job will build successfully. The validated code is now present on the main branch.","title":"Use premade Actions"},{"location":"developer_notes/operations/git/git_actions/#set-up-your-own-actions","text":"Go to your repo Select create new file. We need to be very specific with the naming, when typing, first enter .github + / + workflows + / . Then you can name the workflow file whatever you like. Write your workflow (may take a few attempts to get it working correctly) Commit","title":"Set up your own Actions"},{"location":"developer_notes/operations/git/git_actions/#request-action-approval","text":"If you aren't able to find what you need in the UGH-approved actions , you can request to get one approved here . The Action, if open source, must be registered with, and pass the license and vulnerability scan in Barista before it will be considered. When you have passed the scan, you can submit an Issue to request this Action be approved. Note: approvals may take some time.","title":"Request Action Approval"},{"location":"developer_notes/operations/git/git_actions/#recommended-github-actions-templates","text":"There is a list of internal Actions you can use that have been approved by UHG Go to actions template repo to see find useful github workflow files to incorporate into your project repository.","title":"Recommended GitHub Actions templates"},{"location":"developer_notes/operations/git/git_actions/#useful-resources","text":"Youtube: 5 Ways to DevOps-ify your App - Github Actions Tutorial Blog: Github Actions\u2014 Everything You Need to Know to Get Started Blog: Using GitHub Actions for MLOps & Data Science Git Repo: Awesome Actions Git Repo: starter workflows","title":"Useful Resources"},{"location":"developer_notes/operations/git/git_intro/","text":"Intro to Version Control with Git Overview What is Git? Git is a version control system used for tracking changes in repositories of code\u200b Why? Because humans are dumb and we make mistakes \ud83d\ude42\u200b Also we may want to revert back to older versions of code What is GitHub?\u200b GitHub is the web interface for Git, and a place for us to view repositories of code. \u200b It also allows you to track progress, fork other peoples code, create organizations, do code reviews and manage projects. Branching Branches in github are an extremely useful resource for versioning code and scheduling reviews of code. A branch is a term for another version of the code base that can be worked on in isolation, until its ready to be merged with the main codebase, otherwise known as master (or main). A branch is usually created when you want to test out a new feature in your software\u200b. Branches are a core concept with Git \u2013 you can test out new ideas and break whatever you want with affecting the \u201cmaster\u201d branch\u200b. Branches follow a graph structure. RULE OF THUMB:\u200b\u200b Whatever is in your master branch should always be fully functioning and deployable - never push buggy or broken features to master\u200b! CICD Pipelines can be used to enforce code quality and functionality. Basic Commands Cloning a Repo git clone https://github.optum.com/<ORG>/<REPO>.git Checking and Creating your Branch Inside the repo, check your current working branch by using \u2018git branch\u2019. Your current branch will have a star to the left of it. git branch * master If you want to create a new branch, then use the checkout command. Any exisiting, unstaged modifications will be carried over to the new branch. Ideally, you want to create and switch to a new branch before any modifications are made. ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git checkout -b cpaulisi-data-1 master M package-lock.json M src/pages/main-page.js Switched to a new branch 'cpaulisi-data-1' The current branch can then be verified. ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git branch * cpaulisi-data-1 master Switching your origin The 'origin' in a git repo is the remote url where your repo is housed. This can be verified via the remote command: ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git remote -v origin https://github.optum.com/cpaulisi/DemoHarness.git ( fetch ) origin https://github.optum.com/cpaulisi/DemoHarness.git ( push ) If you wish to keep your code, but changes the remote url for where it's being pushed to (without having to re-clone the repo), then use the set-url command within remote: git remote set-url origin https://github.optum.com/<ORG>/<REPO>.git Adding and Committing Changes Staging Once changes have been made, file uploads happen via the add-commit-push pipeline. Adding a file is also known as staging a file. Specific files can be added (preferred), but directories can also be added as well. Always be a specific as possible with the files added to a commit. Adding directories blindly can get messy quick. Everyone's life is easier when commits are organized/consistent. ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git add example_file.py Commiting Committing is synonymous with making a \"save\" of the current state of the code using the files in the staging area. Once selected files are staged they're committed using git commit -m . ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git commit -m \"ADD: Added data intro\" [ cpaulisi-data-1 6da464b ] ADD: Added data intro 2 files changed, 25852 insertions ( + ) , 38 deletions ( - ) Pushing After a commit is finalized, you must push this local commit to the remote machine. $ git push If no remote branch is set yet for the branch you're currently working on, you may have to create a remote branch before you can commit. You may see an error such as this after a git push : fatal: The current branch cpaulisi-data-1 has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin cpaulisi-data-1 As the error message suggests, this can easily solved with the alluded-to command: ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git push --set-upstream origin cpaulisi-data-1 Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 6 threads Compressing objects: 100 % ( 5 /5 ) , done . Writing objects: 100 % ( 6 /6 ) , 254 .15 KiB | 6 .69 MiB/s, done . Total 6 ( delta 3 ) , reused 0 ( delta 0 ) , pack-reused 0 remote: Resolving deltas: 100 % ( 3 /3 ) , completed with 3 local objects. remote: remote: Create a pull request for 'cpaulisi-data-1' on GitHub by visiting: remote: https://github.optum.com/cpaulisi/DemoHarness/pull/new/cpaulisi-data-1 remote: To https://github.optum.com/cpaulisi/DemoHarness.git * [ new branch ] cpaulisi-data-1 -> cpaulisi-data-1 Branch 'cpaulisi-data-1' set up to track remote branch 'cpaulisi-data-1' from 'origin' . Checking the Status of your Modifications Your exisiting modifications on your current branch can be checked using git status . Files respectively can appear as unstaged, staged, and committed. Unstaged: LAMU02F93E6MD6R:DemoHarness cpaulisi$ git status On branch cpaulisi-data-1 Changes not staged for commit: ( use \"git add <file>...\" to update what will be committed ) ( use \"git restore <file>...\" to discard changes in working directory ) modified: package-lock.json modified: src/pages/main-page.js Staged ( after git add ): ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git status On branch cpaulisi-data-1 Changes to be committed: ( use \"git restore --staged <file>...\" to unstage ) modified: package-lock.json modified: src/pages/main-page.js Committed ( after git commit ): ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git status On branch cpaulisi-data-1 nothing to commit, working tree clean Creating a Pull Request After a commit on a branch is pushed to the remote, you can create a pull request that allows everyone on the project team to review changes. On most repos, this is extremely self-explanatory, just nagivate to the github repo url. There, you'll see a notification that looks like this: From there, you can add reviewers, and specify the changes made in the pull request. In your pull request, you can view comments on code, mentions, and other information relating to the source material. Don't merge your pull request back into main/master or any other branch until all reviewers have approved the changes and you have discussed it with your team. After that, you can make the necessary changes to your pull request, including assignees, reviewers, checklists, and descriptions. When you're done, create the pull request. Creating a Pull Request (From a Fork) If you are working on a forked repo you must specify that the pull request is to be attached specifically to the forked version of the repo. This is simple, just click on the \" Compare and pull request \" button on the notification pictured above. Then just changed the \"base repository\" to the forked version of the repo (your version, not the original). In our case, this looks like changing this to this Flow of Development Now that you know the basics of how to interact with git, its good to mention how this can all be used to contribute to projects in an industry-standard fashon. Here are the steps: Make changes locally stage, commit and push to your branch Submit a pull request Have code reviewed and approved, address changes as needed. If approved, merge the PR. This will spin a Jenkins or Github Actions CICD pipeline. This can include unit tests, format linting, test builds etc. to ensure code is at a production standard. This is the CI portion of \"CICD\". If tests fail then the merge is cancelled. If the tests pass, the code is merged to the master branch. A build should take place to promote the new code to the production code base. This is the CD portion of \"CICD\" Protips .gitignore files These tell git to ignore certain files that you don\u2019t want to be pushed to the remote repository.\u200b These can be a saving grace\u200b use the ** wildcard to ignore file types in multiple directories. Make sure to use them in your repository\u200b Protip: you can autogenerate a .gitignore for the language you are using on GitHub.\u200b Version control your code, not your data\u200b Version control your code, not your data\u200b Large data or binary files should never live on a repository. Especially if it\u2019s PHI/PII Data\u200b Testing data / files needed for some task is the exception.\u200b Use informative commit comments\u200b\u200b Don't use the default commit message given by Git, this make it hard to find specific changes and is generally bad practice. Provide a prefix along with a meaningful description of the changes. Git commit \u2013m \u201cADD \u2013 added new function to perform regexes on label column\u200b Git commit \u2013m \u201cUPDATE \u2013 updated code to get 2 different regular expressions\u201d\u200b Git commit \u2013m \u201cFIX \u2013 fix function that accidentally printed credit card details \ud83d\ude15\u201c\u200b Git commit \u2013m \u201c<name of the user story you are working on>\u201d Always refresh your branches\u200b\u200b Always refresh your branches\u200b. This means you should git merge master into your branch regularly\u200b to recieve up-to-date code. You don\u2019t want your branch going stale and falling too far behind develop and master. References Refer to this cheat sheet to quickly find the commands you need.","title":"Git Introduction"},{"location":"developer_notes/operations/git/git_intro/#intro-to-version-control-with-git","text":"","title":"Intro to Version Control with Git"},{"location":"developer_notes/operations/git/git_intro/#overview","text":"","title":"Overview"},{"location":"developer_notes/operations/git/git_intro/#what-is-git","text":"Git is a version control system used for tracking changes in repositories of code\u200b Why? Because humans are dumb and we make mistakes \ud83d\ude42\u200b Also we may want to revert back to older versions of code","title":"What is Git?"},{"location":"developer_notes/operations/git/git_intro/#what-is-github","text":"GitHub is the web interface for Git, and a place for us to view repositories of code. \u200b It also allows you to track progress, fork other peoples code, create organizations, do code reviews and manage projects.","title":"What is GitHub?\u200b"},{"location":"developer_notes/operations/git/git_intro/#branching","text":"Branches in github are an extremely useful resource for versioning code and scheduling reviews of code. A branch is a term for another version of the code base that can be worked on in isolation, until its ready to be merged with the main codebase, otherwise known as master (or main). A branch is usually created when you want to test out a new feature in your software\u200b. Branches are a core concept with Git \u2013 you can test out new ideas and break whatever you want with affecting the \u201cmaster\u201d branch\u200b. Branches follow a graph structure. RULE OF THUMB:\u200b\u200b Whatever is in your master branch should always be fully functioning and deployable - never push buggy or broken features to master\u200b! CICD Pipelines can be used to enforce code quality and functionality.","title":"Branching"},{"location":"developer_notes/operations/git/git_intro/#basic-commands","text":"","title":"Basic Commands"},{"location":"developer_notes/operations/git/git_intro/#cloning-a-repo","text":"git clone https://github.optum.com/<ORG>/<REPO>.git","title":"Cloning a Repo"},{"location":"developer_notes/operations/git/git_intro/#checking-and-creating-your-branch","text":"Inside the repo, check your current working branch by using \u2018git branch\u2019. Your current branch will have a star to the left of it. git branch * master If you want to create a new branch, then use the checkout command. Any exisiting, unstaged modifications will be carried over to the new branch. Ideally, you want to create and switch to a new branch before any modifications are made. ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git checkout -b cpaulisi-data-1 master M package-lock.json M src/pages/main-page.js Switched to a new branch 'cpaulisi-data-1' The current branch can then be verified. ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git branch * cpaulisi-data-1 master","title":"Checking and Creating your Branch"},{"location":"developer_notes/operations/git/git_intro/#switching-your-origin","text":"The 'origin' in a git repo is the remote url where your repo is housed. This can be verified via the remote command: ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git remote -v origin https://github.optum.com/cpaulisi/DemoHarness.git ( fetch ) origin https://github.optum.com/cpaulisi/DemoHarness.git ( push ) If you wish to keep your code, but changes the remote url for where it's being pushed to (without having to re-clone the repo), then use the set-url command within remote: git remote set-url origin https://github.optum.com/<ORG>/<REPO>.git","title":"Switching your origin"},{"location":"developer_notes/operations/git/git_intro/#adding-and-committing-changes","text":"Staging Once changes have been made, file uploads happen via the add-commit-push pipeline. Adding a file is also known as staging a file. Specific files can be added (preferred), but directories can also be added as well. Always be a specific as possible with the files added to a commit. Adding directories blindly can get messy quick. Everyone's life is easier when commits are organized/consistent. ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git add example_file.py Commiting Committing is synonymous with making a \"save\" of the current state of the code using the files in the staging area. Once selected files are staged they're committed using git commit -m . ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git commit -m \"ADD: Added data intro\" [ cpaulisi-data-1 6da464b ] ADD: Added data intro 2 files changed, 25852 insertions ( + ) , 38 deletions ( - ) Pushing After a commit is finalized, you must push this local commit to the remote machine. $ git push If no remote branch is set yet for the branch you're currently working on, you may have to create a remote branch before you can commit. You may see an error such as this after a git push : fatal: The current branch cpaulisi-data-1 has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin cpaulisi-data-1 As the error message suggests, this can easily solved with the alluded-to command: ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git push --set-upstream origin cpaulisi-data-1 Enumerating objects: 11 , done . Counting objects: 100 % ( 11 /11 ) , done . Delta compression using up to 6 threads Compressing objects: 100 % ( 5 /5 ) , done . Writing objects: 100 % ( 6 /6 ) , 254 .15 KiB | 6 .69 MiB/s, done . Total 6 ( delta 3 ) , reused 0 ( delta 0 ) , pack-reused 0 remote: Resolving deltas: 100 % ( 3 /3 ) , completed with 3 local objects. remote: remote: Create a pull request for 'cpaulisi-data-1' on GitHub by visiting: remote: https://github.optum.com/cpaulisi/DemoHarness/pull/new/cpaulisi-data-1 remote: To https://github.optum.com/cpaulisi/DemoHarness.git * [ new branch ] cpaulisi-data-1 -> cpaulisi-data-1 Branch 'cpaulisi-data-1' set up to track remote branch 'cpaulisi-data-1' from 'origin' .","title":"Adding and Committing Changes"},{"location":"developer_notes/operations/git/git_intro/#checking-the-status-of-your-modifications","text":"Your exisiting modifications on your current branch can be checked using git status . Files respectively can appear as unstaged, staged, and committed. Unstaged: LAMU02F93E6MD6R:DemoHarness cpaulisi$ git status On branch cpaulisi-data-1 Changes not staged for commit: ( use \"git add <file>...\" to update what will be committed ) ( use \"git restore <file>...\" to discard changes in working directory ) modified: package-lock.json modified: src/pages/main-page.js Staged ( after git add ): ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git status On branch cpaulisi-data-1 Changes to be committed: ( use \"git restore --staged <file>...\" to unstage ) modified: package-lock.json modified: src/pages/main-page.js Committed ( after git commit ): ( base ) LAMU02F93E6MD6R:DemoHarness cpaulisi$ git status On branch cpaulisi-data-1 nothing to commit, working tree clean","title":"Checking the Status of your Modifications"},{"location":"developer_notes/operations/git/git_intro/#creating-a-pull-request","text":"After a commit on a branch is pushed to the remote, you can create a pull request that allows everyone on the project team to review changes. On most repos, this is extremely self-explanatory, just nagivate to the github repo url. There, you'll see a notification that looks like this: From there, you can add reviewers, and specify the changes made in the pull request. In your pull request, you can view comments on code, mentions, and other information relating to the source material. Don't merge your pull request back into main/master or any other branch until all reviewers have approved the changes and you have discussed it with your team. After that, you can make the necessary changes to your pull request, including assignees, reviewers, checklists, and descriptions. When you're done, create the pull request.","title":"Creating a Pull Request"},{"location":"developer_notes/operations/git/git_intro/#creating-a-pull-request-from-a-fork","text":"If you are working on a forked repo you must specify that the pull request is to be attached specifically to the forked version of the repo. This is simple, just click on the \" Compare and pull request \" button on the notification pictured above. Then just changed the \"base repository\" to the forked version of the repo (your version, not the original). In our case, this looks like changing this to this","title":"Creating a Pull Request (From a Fork)"},{"location":"developer_notes/operations/git/git_intro/#flow-of-development","text":"Now that you know the basics of how to interact with git, its good to mention how this can all be used to contribute to projects in an industry-standard fashon. Here are the steps: Make changes locally stage, commit and push to your branch Submit a pull request Have code reviewed and approved, address changes as needed. If approved, merge the PR. This will spin a Jenkins or Github Actions CICD pipeline. This can include unit tests, format linting, test builds etc. to ensure code is at a production standard. This is the CI portion of \"CICD\". If tests fail then the merge is cancelled. If the tests pass, the code is merged to the master branch. A build should take place to promote the new code to the production code base. This is the CD portion of \"CICD\"","title":"Flow of Development"},{"location":"developer_notes/operations/git/git_intro/#protips","text":".gitignore files These tell git to ignore certain files that you don\u2019t want to be pushed to the remote repository.\u200b These can be a saving grace\u200b use the ** wildcard to ignore file types in multiple directories. Make sure to use them in your repository\u200b Protip: you can autogenerate a .gitignore for the language you are using on GitHub.\u200b Version control your code, not your data\u200b Version control your code, not your data\u200b Large data or binary files should never live on a repository. Especially if it\u2019s PHI/PII Data\u200b Testing data / files needed for some task is the exception.\u200b Use informative commit comments\u200b\u200b Don't use the default commit message given by Git, this make it hard to find specific changes and is generally bad practice. Provide a prefix along with a meaningful description of the changes. Git commit \u2013m \u201cADD \u2013 added new function to perform regexes on label column\u200b Git commit \u2013m \u201cUPDATE \u2013 updated code to get 2 different regular expressions\u201d\u200b Git commit \u2013m \u201cFIX \u2013 fix function that accidentally printed credit card details \ud83d\ude15\u201c\u200b Git commit \u2013m \u201c<name of the user story you are working on>\u201d Always refresh your branches\u200b\u200b Always refresh your branches\u200b. This means you should git merge master into your branch regularly\u200b to recieve up-to-date code. You don\u2019t want your branch going stale and falling too far behind develop and master.","title":"Protips"},{"location":"developer_notes/operations/git/git_intro/#references","text":"Refer to this cheat sheet to quickly find the commands you need.","title":"References"},{"location":"developer_notes/operations/mlops/overview/","text":"Machine Learning Operations (MLOps) Note The topic of MLOps is an entire discipline on top of Machine Learning. This article aims to condense all of the core concepts into a single story. The ML Lifecycle Inspired by the Software Development Life Cycle (SDLC), the ML lifecycle aims to provide a framework around planning and development machine learning projects. ML practitioners who don't have any experience with MLOps may be familiar with a few of these steps such as EDA, data preprocessing, training and evaluation. However, this is only a portion of what it takes to make a production grade machine learning project. Steps Data collection EDA Data cleaning (not model specific) Preprocessing (model specific, data segregation, feature engineering etc.) Model training Evaluation Deployment Batch: setup environment for periodic runs for throughput Service: a REST API environment can be set up to produce ML value on demand Real-time (streaming): used for continous inference on a low latency data stream Monitoring Performance drop? Repeat step 1 What is MLOps? Software engineers have done a great job of establishing best practices for software engineering projects through the use of DevOps. However, as machine learning is a more iterative process, this needs to be altered to suit our needs. This is where ML Operations comes in. ML Operations, or commonly referred to as MLOps, is a set off best practices for efficientend-to-end development and productionalization of machine learning projects. When dealing with MLOps, you will hear the word \" pipeline \" a lot. In essence, an ML Pipeline is just a sequences of independent, modular and reusable steps to achieve each part in the machine learning life cycle. This is essential for making a machine learning model usable. It's often the case the ML models, no matter how good the performance, never get used in the real world. \u201c70% accuracy on a model is infinitely better than 95% accuracy on a model that can\u2019t be used in production\u201d But by using MLOps principles we can avoid \"racing to a redlight\". The attributes of a good ML pipeline are: high performance scalability reliability automatation reproducibility There are three \"pillars\" to a project that makes use of MLOps principles: The Data Pipeline (data ingestion, cleaning, sorting) The ML Pipeline (processing, experimentation, testing) The Deployment Pipeline (Inference, monitoring) ML Pipeline Stack With respect to ML pipelines, there are a number of crucial concepts that need to be understood before attempting to build an MLOps Stack. The code used for modelling is only a small part in the grand scheme of things. Below are just brief descriptions of the concepts. We'll dig a little deeper into how these are implemented within this article. Obviously we can't test out every tool listed, but the following sections will show what we used. The concepts will remain the same so don't need to use the exact tools we did. These are the best practices that should be put in place so you can ensure you have a durable and robust MLOps pipeline. Attribute Purpose Description Data Validator Automated Validation This involved checking for data integrity and data drift , Which is defined as a variation in the production data from the data that was used to test and validate the model before deploying it in production. Data Versioning Reproducibility Needed for tracking different snapshots of your data for experiments. Lets say you train a model 10 times, using different feature engineering techniques each time - if your data is version controlled then you can easily roll back to previous version of your data Experiment Tracker Reproducibility The concept of keeping track of all parameters related to model training such as hyperparameters, metrics, configurations, dataset paths etc. This is crucial for keeping a record of past experiments, and should never be deleted unless 100% centrain you won't need it again. Artifact Store Reproducibility Artifact tracking involves storing a record of the outputs from each component (step) in the pipeline such as models, images, datasets etc. Model Registry Deployment This is a centralized repository for storing your trained models. In production the inference pipeline will reference the lastest model that have been promoted to this registry. Model Monitor Automated Validation Model monitoring involved checking for model performance drift due to changes in the data. Pipeline Orchestrator Orchestration Now that you have defined your stack, you need something that can actually define which componenets will be used, and when. Using an orchestrator tool can enable you to schedule automated runs of your data or ML pipeline There are also some optional attributes that can be integrated into your ML Stack. These are more of a \"nice-to-have's\" or are only required in certain circumstances. We wont cover these in the rest of the article, but they're good to be aware of. Attribute Purpose Description Alerter Automated Validation Alerters allow you to send messages to chat services (like Teams, Slack, etc.) from within your pipelines. This is useful to immediately get notified when failures happen, for general monitoring/reporting, and also for building human-in-the-loop ML. Feature store Reproducibility Feature stores allow data teams to serve data via an offline store and an online low-latency store where data is kept in sync between the two. It also offers a centralized registry where features (and feature schemas) are stored for use within a team or wider organization. Its important to note that there are many tools used to achieve a good MLOps pipeline. Sometimes it may be difficult to find out what you should use. Also, a lot of these tools are not mutual exclusive, meaning that some tools can do multiple stack operations. For example, MLFlow can act as an experiment tracker and a model registry. A good website for reading up on the different options is https://mymlops.com . The list isn't extensive but its a good place to start and to see what integrates nicely together. Levels of MLOps Readiness To understand where you are within the stages of bringing a ML project to production, its useful categorize the readiness of your ML Pipeline. For this we have 3 levels: Level 0: Proof-of-concept In this stage the project is in proof-of-concept or expoloratory analysis phase Code is standalone, normally consisting of just notebooks Think \u201cKaggle competition\u201d Infrastructure is minimal or nonexistent at this stage There is requirement for model monitoring, productionatization or inference, therefore MLOps not neccessary This is the lowest form of readiness, suited for smaller teams in the early stages Level 1: At this level, there is some sort of pipeline emerging. More time is spent thinking about reusability and automation. Scripts are organized reusable components (utilities) Process of training models are standardised, making it easier for adding new models. Artifacts are tracked, meaning there is some type of versioning on datasets or models Ability to retrain models easily can prevent model drift. Time to go to production is reduced. This level of readiness is required for small to medium sized teams and ML projects that are in development Level 2: Continuous integration and continuous deployment (CICD) Every change in the components that passes the tests is merged automatically Every change that passes the tests is released in the production pipeline Continuous training New pipeline is automatically triggered when model/data drift is found within the model monitoring tests Rapid iterations on prod pipelines. Customers see continuous improvements This level of pipeline readiness is essential for large teams or projects that are supposed to go into production. 1. Data Validation When you're at level 0 of pipeline readiness, you would have likely extracted data manually from some datasource and done some exploratory data analysis to ensure quality and integrity of the data. This may include finding columns that are redundant, have erroneous or missing data. You would have gotten a sense of what the data should or shouldn't look like for your usecase. The same way that models training is iterative, so too should your data validation. Each time an extract occurs you should make sure you setup a process to preventatively catch problems in your data that may cause issues further down in the pipeline. This can be done in a number of ways: Types of Drift Data drift: Data drift is when the input data has changed and distribution of the variables is significantly different. Generally, a consequence of seasonal changes or changes in consumer preferences over time. For instance, educational data collected before Covid shows a lesser preference for online learning than post-covid. Data drift can also occur at the expense of being trained on specific data but being exposed to a wider scope of data upon production. Spam detection is a good case in point. Concept drift: Contrary to data drift, where the data changes, concept drift occurs when the model\u2019s predicted target or its statistical properties change over time. The distributions might remain the same. Instead, the relationships between the model inputs and outputs change. Concept drift can happen suddenly or gradually. For example, spammer's trying to change how they make spam emails over time. Deterministic Tests Deterministic when we are measuring something that can be measured without uncertainty. Some examples of this could be: The number of columns, its a definite number The amount of unique values for a categorical variable Numeric values are within expected ranges (like age being greater than 0 or less than 120) It will depend on what language you want to use, but this can be done in python using PyTest. Here is an example of a deterministic test using fixtures to load in data. Here is another example of data validation using PyTest. conftest.py ''' \u2022 conftest is used to share fixtures across all tests \u2022 Also used to host code needed to handle command line params ''' import pytest import pandas as pd @pytest . fixture ( scope = \"session\" ) def data ( request ): \"\"\" Import data as a fixture to be importing within testing \"\"\" reference_data = pd . read_csv ( ref_path ) # previous dataset sample_data = pd . read_csv ( sample_path ) # new data return reference_data , sample_data test_data.py def test_column_presence_and_type ( data ): \"\"\" Assert the types of columns \"\"\" # ignore reference data _ , data = data required_columns = { \"patient_id\" : pd . api . types . is_integer_dtype , \"body_weight\" : pd . api . types . is_float_dtype , \"name\" : pd . api . types . is_string_dtype , \"age\" : pd . api . types . is_integer_dtype } # Check column presence assert set ( data . columns . values ) . issuperset ( set ( required_columns . keys ())) for col_name , format_verification_funct in required_columns . items (): assert format_verification_funct ( data [ col_name ]), f \"Column { col_name } failed test { format_verification_funct } \" def test_column_ranges ( data ): \"\"\" Assert the age ranges are correct \"\"\" ranges = { \"age\" : ( 0 , 120 ), . . . } for col_name , ( minimum , maximum ) in ranges . items (): assert data [ col_name ] . dropna () . between ( minimum , maximum ) . all (), ( f \"Column { col_name } failed the test. Should be between { minimum } and { maximum } , \" f \"instead min= { data [ col_name ] . min () } and max= { data [ col_name ] . max () } \" ) Non-deterministic Tests Non-deterministic test evolves measuring quantity with uncertainty, such as dealing with random variable or a distribution of a column within a dataset. Non-deterministic test use hypothesis testing and, for the most part, are used to compare one dataset to another. For example, measuring the mean or standard deviation of items within the dataset. The null hypothesis is that the average is the roughly the same, the alternative hypothesis is that they deviate a lot from eachother. You can see how this is useful in preventing misleading model predictions, as you can use the rejection of the null hypothesis as a trigger to spin up a new training pipeline run. This is done because the data used to train our model doesn't look like the data in it's current state. You can read more about hypothesis testing here . The example below shows an example of using a statistical measure to test our new sample data against the previous dataset. test_data.py def test_kolmogorov_smirnov ( data , ks_alpha ): \"\"\" The Kolmogorov-Smirnov test is used to test whether or not or not a sample comes from a certain distribution. \"\"\" sample1 , sample2 = data columns = [ \"col1\" , \"col2\" , . . . ] alpha_prime = 1 - ( 1 - ks_alpha ) ** ( 1 / len ( columns )) for col in columns : ts , p_value = scipy . stats . ks_2samp ( sample1 [ col ], sample2 [ col ]) # NOTE: as always, the p-value should be interpreted as the probability of # obtaining a test statistic (TS) equal or more extreme that the one we got # by chance, when the null hypothesis is true. If this probability is not # large enough, this dataset should be looked at carefully, hence we fail assert p_value > alpha_prime Kolmogorov-Smirnov tests are primarily used on smaller datasets. For larger datasets you can use Wasserstein distance for numeric features. Jensen\u2013Shannon divergence and Chi-squared test can be used for categorical features. You can read an article by Evidently.ai comparing different types of tests here Third-party Tools The examples above show how you can implement some data validation heuristics, but you can also make use of thirdparty tools to do some of the heavy lifting for you. Some examples of tools such as: Evidently.ai GreatExpectation TFDV - Read our docs on TFDV here 2. Data Versioning Data versioning is defined as organising data by assigning some unique identifier to state that it's in. Here are some of the reasons why data versioning is important: As Data Scientists we know that to develop a Machine Learning model, is not all about code, but also about data and the right parameters. In order to get the optimal result experimentation is required, which makes the process highly iterative and extremely important to keep track of the changes made as well as their impacts on the end results. This becomes even more important in a complex environment where multiple data scientists are collaborating. It is crucial for reproducibility to have a snapshot of the data used to develop a certain instance of the model and have it versioned. It makes the iterative process of model development not only easier but also trackable. The ability to track data version provided by version control tools is the first step to have companies data sources ready for compliance, and an essential step in maintaining a strong and robust audit train and risk management processes around data. Third-party Tools DVC : The concept of data versioning is very similary to version control, as seen with Git and Github. Infact, the most popular data version control software, suitably named Data Version Control, was built on top of git and shares similar syntax. Git DVC Usage git init dvc init Initialize the project git add dvc add Track item in staging area git push dvc push Upload tracked files or directories to remote location git pull dvc pull Download tracked files or directories from remote location Some other tools include: Git LFS , LakeFS , FastDS 3. Experiment Tracking Experiment tracking is the process of saving all experiment related information that you care about for every experiment you run. This \u201cmetadata you care about\u201d will strongly depend on your project, but it may include: Scripts used for running the experiment Environment configuration files Versions of the data used for training and evaluation Parameter configurations Evaluation metrics Model weights Performance visualizations (confusion matrix, ROC curve) So why do we do this? Model training is an iterative process, and shouldn't just be confined to using a single set of parameters or version of the data. You will need to experiment with a plethora of different configurations. Therefore its crucial to persist this information to have a data-centric approach for choosing which model gets deployed to production. Third-party Tools MLFlow Weights & Biases Neptune 4. Artifact Tracking Components and Artifacts You can think of ML Pipelines as an Direct Acyclic graphic (DAG). Conceptually, parts of the pipeline can be classified as one of the two categories: components and artifacts. Terminology Description Components Think of an action or verb - something that performs a task. Components can be scripts, notebooks or executables. Artifacts An artifact is any data file or directory produced within a run of the pipeline. Artifacts from one component may be used as the input to the next component. Every artifact should be automatically tracked and versioned. Artifact Store The Artifact Store is a central component in any MLOps stack. As the name suggests, it acts as a data persistence layer where artifacts (e.g. datasets, models, images etc) generated by a component are stored. Its often the case that an artifact from one component (step) is fed into the next component. For example: The output artifact from a data preprocessing component would be the input artifact for a train component. It's not only a place to store items necessary for the ML pipeline to run successfully, we can store supplementary artifacts also. Let's say we periodically perform EDA in our ML Pipeline, and we have some graphs we want to keep for each experiment. These should be tracked and stored for future inspection. 6. Deployment Deployment isn't as simple as exposing a model binary through some API or having a CRON job perform batch inference to generate a csv. Not having a process where you can deploy a model to production with minimal friction will acrue technical debt as projects scale and the number of projects grows. Manually deploying models increases risk of downtime, which isn't acceptable for a commercial product. To avoid these hurdles there needs to be a robust deployment strategy. Regardless of your deployment type, be it Batch, Service or Real-time (streaming) these concepts still apply: Inference Pipeline So you've written the code, trained the model and evaluated the results and now want to put a model into production. Firstly you need to be aware of inference pipelines. In machine learning, it's expected the data your model is exposed to is prepared in the same way as the data that was used to train it. This means you need to ensure all the same data preprocessing operations from development are applied to inference data in production. This is called Development Production Symmetry . So if we need to perform the same actions during training/inference time then we SHOULD reuse the same code. Don't duplicate the code. An Inference Pipeline constitutes two items: the preprocessing pipeline the model Inference Artifact When training is complete then the preprocessing pipeline and model is serialized ( eg. saved to disk ). Inference Artifact is an instance of the Inference Pipeline containing the preprocessing code and a trained model. This inference artifact is then marked for production and deployed as a working component in the project pipeline. The reason why we make instances of an \"inference artifact\" rather than just reference code from the training pipeline is so that we can version our preprocessing code and model for each iteration. Sometimes the code or model maybe what changes, sometimes it's both. Model Bias When learning about ML you will have likely gained a habit of believing you should just optimization your model to have good metrics. Low error on test set isn't the be-all and end-all. If performance on disproportionately important samples aren't good, then the model is not fit for production. Bias is a systematic error from an erroneous assumption in the machine learning algorithm\u2019s modeling. The algorithm tends to systematically learn the wrong signals by not considering all the information contained within the data. Model bias may lead an algorithm to miss the relevant relationship between data inputs (features) and targeted outputs (predictions). In essence, bias arises when an algorithm has insufficient capability in learning the appropriate signal from the dataset. For decades, bias in machine learning has been recognized as a potential concern, but it remains a complex and challenging issue for machine learning researchers and engineers when deploying models into production. You can read more on the different types of bias here One way of checking for bias is through Data Slicing . Data slicing is when you computing a performance metric holding a feature or output fixed. So rather than evaluating performance on the entire test set, you extend it by evaluating breaking down by the feature you're concern with. For example: Gender, ethnicity, rare classes. Given the example below, we can see a hypothetical model that performs very well. However, after performing data slicing on features such as race we can see that hispanics are disproportionately scoring lower than others. This form of auditing can prevent potential legal or productional issues down the line. Race Model Accuracy Black 87.7% South East Asian 91.4% Hispanic 58% White 88.3% Model Cards Beside data versioning, artifact and experiment tracking - another important method of logging information about a model is by writing a \"model card\". Model cards aim to encapsulate all the key details about the model for future reference. It isn't a hard requirement, but it should be done for models that are deployed to production. The information could include: Data used How model should be interacted with Caveats, warnings or any shortcomings Metrics Parameters When to deploy a model Before deploying a model into production, you should first go through the predeployment checklist: \u2705 Test Performance (Choose the best performing model) \u2705 Check for Bias \u2705 Write a Model Card If you believe everything looks good, then you can mark that model for production. If you aren't using a model registry then you can use a dedicated folder for the inference pipeline to point to. If you are using a model registry, you would most likely need to tag the model with a :prod or :latest tag. Additionally, you should aim to automate the items in this checklist so you can reduce the amount of manual interactions needed. Third-party Tools Aequitas - Bias and Fairness Audit Toolkit FairML - Python library that audits black-box predictive models 7. Monitoring & Maintenance The cascading affects of something changing upstream can be catostrophic for processes that are downstream. By monitoring aspects of the project we can see trends of our Infrastructure compute Statistics from data validation Concept and Data drift etc. The first thing you should do is a team-brainstorm of all the possible scenarios where things can go wrong, like Number of missing values over time, server load over time and memory usage, to mention a few. It's common practice to set alerts for thresholds for certain metrics. For example: we can see a trend that the an increase in requests to a model API is putting strain on the server's performance. By using monitoring we can see in advance that we should scale-up the compute power of the API. Model Decay The definition of model decay is the occurence of a model in production, when evaulated over time, performs worse that is previously did. The cause of this is due to \"data drift\" as nothing is actually changing in the model itself - but just to differentiate it from the data drift seen in the Data validation section lets refer to it as model decay. There are 3 ways to assess for model drift: Raw comparison test A raw comparison is just a simple check to see if the target evaluation metric performed worse the previous version(s) of the model. Parametric Significance test Raw comparison test may indicate model drift has occurred, but this may be due to random chance or the score difference is so minuscule that it really isn\u2019t significant (ie: not worth training another model). Using a paremteric significance test provides more reasonable condition to check against for model drift. To do this you must: 1. Calculate mean and STD of all previous model scores (bell curve) 2. If the new model score is at least 2 standard deviations from the mean, we can conclude model drift has occurred. Non-parametric outlier test Paremteric significance tests assume that the distribution going to be in a bell curve (paremetric). This isn't appropraite for all cases, as it's senstive to outliers. A non-parametric measurment such as the Interquartile Range Rule (IQR) to measure how far a datapoint is from the rest of the data. Essentially, this is to find outliers (in this case, the new model results if model drift occurred). Note: depending on what your target evaluation metric is the way you use IQR will be different. For metrics where higher values indicate worse performance, you need to look for upper outliers For metrics where lower values indicate worse performance, you need to look out for lower outliers. Handling Decay with Continuous Deployment Okay so you know how to assess for model decay, now what? The purpose of this is to have a data-centric approach to deciding when your model needs to be updated. For example, lets say once a month you check for model drift. New data is given to the model and it's performance is evaluated. Using the the evaluation metric(s) we perform a model decay test to compare to older evaluations. You should make an automated process to spin off run of the training pipeline if the model scores considerably worse. Having the ability to continously deliver new models without manual interaction speeds up \"time-to-production\" and reduces down-time. Third-party Tools Prometheus Evidently.ai Grafana 8. Pipeline Orchestration Traditionally, using Cronjobs was the default for scheduling events. However, some issues with this approach include poor failure handling, difficulty monitoring current and past processes and lack of scalability. Airflow aims to remedy these issues. See our Airflow documentation here Third-party Tools Airflow Offical Documentations Github Actions Suggested GitHub Actions My MLOps - Stack Builder Machine Learning Operations (MLOps): Overview, Definition, and Architecture - Kreuzberger et al. Categories of MLOps Tools Hidden Technical Debt in Machine Learning Systems Netune AI MLOps Blog Types of Data Drift","title":"Overview"},{"location":"developer_notes/operations/mlops/overview/#machine-learning-operations-mlops","text":"Note The topic of MLOps is an entire discipline on top of Machine Learning. This article aims to condense all of the core concepts into a single story.","title":"Machine Learning Operations (MLOps)"},{"location":"developer_notes/operations/mlops/overview/#the-ml-lifecycle","text":"Inspired by the Software Development Life Cycle (SDLC), the ML lifecycle aims to provide a framework around planning and development machine learning projects. ML practitioners who don't have any experience with MLOps may be familiar with a few of these steps such as EDA, data preprocessing, training and evaluation. However, this is only a portion of what it takes to make a production grade machine learning project. Steps Data collection EDA Data cleaning (not model specific) Preprocessing (model specific, data segregation, feature engineering etc.) Model training Evaluation Deployment Batch: setup environment for periodic runs for throughput Service: a REST API environment can be set up to produce ML value on demand Real-time (streaming): used for continous inference on a low latency data stream Monitoring Performance drop? Repeat step 1","title":"The ML Lifecycle"},{"location":"developer_notes/operations/mlops/overview/#what-is-mlops","text":"Software engineers have done a great job of establishing best practices for software engineering projects through the use of DevOps. However, as machine learning is a more iterative process, this needs to be altered to suit our needs. This is where ML Operations comes in. ML Operations, or commonly referred to as MLOps, is a set off best practices for efficientend-to-end development and productionalization of machine learning projects. When dealing with MLOps, you will hear the word \" pipeline \" a lot. In essence, an ML Pipeline is just a sequences of independent, modular and reusable steps to achieve each part in the machine learning life cycle. This is essential for making a machine learning model usable. It's often the case the ML models, no matter how good the performance, never get used in the real world. \u201c70% accuracy on a model is infinitely better than 95% accuracy on a model that can\u2019t be used in production\u201d But by using MLOps principles we can avoid \"racing to a redlight\". The attributes of a good ML pipeline are: high performance scalability reliability automatation reproducibility There are three \"pillars\" to a project that makes use of MLOps principles: The Data Pipeline (data ingestion, cleaning, sorting) The ML Pipeline (processing, experimentation, testing) The Deployment Pipeline (Inference, monitoring)","title":"What is MLOps?"},{"location":"developer_notes/operations/mlops/overview/#ml-pipeline-stack","text":"With respect to ML pipelines, there are a number of crucial concepts that need to be understood before attempting to build an MLOps Stack. The code used for modelling is only a small part in the grand scheme of things. Below are just brief descriptions of the concepts. We'll dig a little deeper into how these are implemented within this article. Obviously we can't test out every tool listed, but the following sections will show what we used. The concepts will remain the same so don't need to use the exact tools we did. These are the best practices that should be put in place so you can ensure you have a durable and robust MLOps pipeline. Attribute Purpose Description Data Validator Automated Validation This involved checking for data integrity and data drift , Which is defined as a variation in the production data from the data that was used to test and validate the model before deploying it in production. Data Versioning Reproducibility Needed for tracking different snapshots of your data for experiments. Lets say you train a model 10 times, using different feature engineering techniques each time - if your data is version controlled then you can easily roll back to previous version of your data Experiment Tracker Reproducibility The concept of keeping track of all parameters related to model training such as hyperparameters, metrics, configurations, dataset paths etc. This is crucial for keeping a record of past experiments, and should never be deleted unless 100% centrain you won't need it again. Artifact Store Reproducibility Artifact tracking involves storing a record of the outputs from each component (step) in the pipeline such as models, images, datasets etc. Model Registry Deployment This is a centralized repository for storing your trained models. In production the inference pipeline will reference the lastest model that have been promoted to this registry. Model Monitor Automated Validation Model monitoring involved checking for model performance drift due to changes in the data. Pipeline Orchestrator Orchestration Now that you have defined your stack, you need something that can actually define which componenets will be used, and when. Using an orchestrator tool can enable you to schedule automated runs of your data or ML pipeline There are also some optional attributes that can be integrated into your ML Stack. These are more of a \"nice-to-have's\" or are only required in certain circumstances. We wont cover these in the rest of the article, but they're good to be aware of. Attribute Purpose Description Alerter Automated Validation Alerters allow you to send messages to chat services (like Teams, Slack, etc.) from within your pipelines. This is useful to immediately get notified when failures happen, for general monitoring/reporting, and also for building human-in-the-loop ML. Feature store Reproducibility Feature stores allow data teams to serve data via an offline store and an online low-latency store where data is kept in sync between the two. It also offers a centralized registry where features (and feature schemas) are stored for use within a team or wider organization. Its important to note that there are many tools used to achieve a good MLOps pipeline. Sometimes it may be difficult to find out what you should use. Also, a lot of these tools are not mutual exclusive, meaning that some tools can do multiple stack operations. For example, MLFlow can act as an experiment tracker and a model registry. A good website for reading up on the different options is https://mymlops.com . The list isn't extensive but its a good place to start and to see what integrates nicely together.","title":"ML Pipeline Stack"},{"location":"developer_notes/operations/mlops/overview/#levels-of-mlops-readiness","text":"To understand where you are within the stages of bringing a ML project to production, its useful categorize the readiness of your ML Pipeline. For this we have 3 levels: Level 0: Proof-of-concept In this stage the project is in proof-of-concept or expoloratory analysis phase Code is standalone, normally consisting of just notebooks Think \u201cKaggle competition\u201d Infrastructure is minimal or nonexistent at this stage There is requirement for model monitoring, productionatization or inference, therefore MLOps not neccessary This is the lowest form of readiness, suited for smaller teams in the early stages Level 1: At this level, there is some sort of pipeline emerging. More time is spent thinking about reusability and automation. Scripts are organized reusable components (utilities) Process of training models are standardised, making it easier for adding new models. Artifacts are tracked, meaning there is some type of versioning on datasets or models Ability to retrain models easily can prevent model drift. Time to go to production is reduced. This level of readiness is required for small to medium sized teams and ML projects that are in development Level 2: Continuous integration and continuous deployment (CICD) Every change in the components that passes the tests is merged automatically Every change that passes the tests is released in the production pipeline Continuous training New pipeline is automatically triggered when model/data drift is found within the model monitoring tests Rapid iterations on prod pipelines. Customers see continuous improvements This level of pipeline readiness is essential for large teams or projects that are supposed to go into production.","title":"Levels of MLOps Readiness"},{"location":"developer_notes/operations/mlops/overview/#1-data-validation","text":"When you're at level 0 of pipeline readiness, you would have likely extracted data manually from some datasource and done some exploratory data analysis to ensure quality and integrity of the data. This may include finding columns that are redundant, have erroneous or missing data. You would have gotten a sense of what the data should or shouldn't look like for your usecase. The same way that models training is iterative, so too should your data validation. Each time an extract occurs you should make sure you setup a process to preventatively catch problems in your data that may cause issues further down in the pipeline. This can be done in a number of ways:","title":"1. Data Validation"},{"location":"developer_notes/operations/mlops/overview/#types-of-drift","text":"Data drift: Data drift is when the input data has changed and distribution of the variables is significantly different. Generally, a consequence of seasonal changes or changes in consumer preferences over time. For instance, educational data collected before Covid shows a lesser preference for online learning than post-covid. Data drift can also occur at the expense of being trained on specific data but being exposed to a wider scope of data upon production. Spam detection is a good case in point. Concept drift: Contrary to data drift, where the data changes, concept drift occurs when the model\u2019s predicted target or its statistical properties change over time. The distributions might remain the same. Instead, the relationships between the model inputs and outputs change. Concept drift can happen suddenly or gradually. For example, spammer's trying to change how they make spam emails over time.","title":"Types of Drift"},{"location":"developer_notes/operations/mlops/overview/#deterministic-tests","text":"Deterministic when we are measuring something that can be measured without uncertainty. Some examples of this could be: The number of columns, its a definite number The amount of unique values for a categorical variable Numeric values are within expected ranges (like age being greater than 0 or less than 120) It will depend on what language you want to use, but this can be done in python using PyTest. Here is an example of a deterministic test using fixtures to load in data. Here is another example of data validation using PyTest. conftest.py ''' \u2022 conftest is used to share fixtures across all tests \u2022 Also used to host code needed to handle command line params ''' import pytest import pandas as pd @pytest . fixture ( scope = \"session\" ) def data ( request ): \"\"\" Import data as a fixture to be importing within testing \"\"\" reference_data = pd . read_csv ( ref_path ) # previous dataset sample_data = pd . read_csv ( sample_path ) # new data return reference_data , sample_data test_data.py def test_column_presence_and_type ( data ): \"\"\" Assert the types of columns \"\"\" # ignore reference data _ , data = data required_columns = { \"patient_id\" : pd . api . types . is_integer_dtype , \"body_weight\" : pd . api . types . is_float_dtype , \"name\" : pd . api . types . is_string_dtype , \"age\" : pd . api . types . is_integer_dtype } # Check column presence assert set ( data . columns . values ) . issuperset ( set ( required_columns . keys ())) for col_name , format_verification_funct in required_columns . items (): assert format_verification_funct ( data [ col_name ]), f \"Column { col_name } failed test { format_verification_funct } \" def test_column_ranges ( data ): \"\"\" Assert the age ranges are correct \"\"\" ranges = { \"age\" : ( 0 , 120 ), . . . } for col_name , ( minimum , maximum ) in ranges . items (): assert data [ col_name ] . dropna () . between ( minimum , maximum ) . all (), ( f \"Column { col_name } failed the test. Should be between { minimum } and { maximum } , \" f \"instead min= { data [ col_name ] . min () } and max= { data [ col_name ] . max () } \" )","title":"Deterministic Tests"},{"location":"developer_notes/operations/mlops/overview/#non-deterministic-tests","text":"Non-deterministic test evolves measuring quantity with uncertainty, such as dealing with random variable or a distribution of a column within a dataset. Non-deterministic test use hypothesis testing and, for the most part, are used to compare one dataset to another. For example, measuring the mean or standard deviation of items within the dataset. The null hypothesis is that the average is the roughly the same, the alternative hypothesis is that they deviate a lot from eachother. You can see how this is useful in preventing misleading model predictions, as you can use the rejection of the null hypothesis as a trigger to spin up a new training pipeline run. This is done because the data used to train our model doesn't look like the data in it's current state. You can read more about hypothesis testing here . The example below shows an example of using a statistical measure to test our new sample data against the previous dataset. test_data.py def test_kolmogorov_smirnov ( data , ks_alpha ): \"\"\" The Kolmogorov-Smirnov test is used to test whether or not or not a sample comes from a certain distribution. \"\"\" sample1 , sample2 = data columns = [ \"col1\" , \"col2\" , . . . ] alpha_prime = 1 - ( 1 - ks_alpha ) ** ( 1 / len ( columns )) for col in columns : ts , p_value = scipy . stats . ks_2samp ( sample1 [ col ], sample2 [ col ]) # NOTE: as always, the p-value should be interpreted as the probability of # obtaining a test statistic (TS) equal or more extreme that the one we got # by chance, when the null hypothesis is true. If this probability is not # large enough, this dataset should be looked at carefully, hence we fail assert p_value > alpha_prime Kolmogorov-Smirnov tests are primarily used on smaller datasets. For larger datasets you can use Wasserstein distance for numeric features. Jensen\u2013Shannon divergence and Chi-squared test can be used for categorical features. You can read an article by Evidently.ai comparing different types of tests here","title":"Non-deterministic Tests"},{"location":"developer_notes/operations/mlops/overview/#third-party-tools","text":"The examples above show how you can implement some data validation heuristics, but you can also make use of thirdparty tools to do some of the heavy lifting for you. Some examples of tools such as: Evidently.ai GreatExpectation TFDV - Read our docs on TFDV here","title":"Third-party Tools"},{"location":"developer_notes/operations/mlops/overview/#2-data-versioning","text":"Data versioning is defined as organising data by assigning some unique identifier to state that it's in. Here are some of the reasons why data versioning is important: As Data Scientists we know that to develop a Machine Learning model, is not all about code, but also about data and the right parameters. In order to get the optimal result experimentation is required, which makes the process highly iterative and extremely important to keep track of the changes made as well as their impacts on the end results. This becomes even more important in a complex environment where multiple data scientists are collaborating. It is crucial for reproducibility to have a snapshot of the data used to develop a certain instance of the model and have it versioned. It makes the iterative process of model development not only easier but also trackable. The ability to track data version provided by version control tools is the first step to have companies data sources ready for compliance, and an essential step in maintaining a strong and robust audit train and risk management processes around data.","title":"2. Data Versioning"},{"location":"developer_notes/operations/mlops/overview/#third-party-tools_1","text":"DVC : The concept of data versioning is very similary to version control, as seen with Git and Github. Infact, the most popular data version control software, suitably named Data Version Control, was built on top of git and shares similar syntax. Git DVC Usage git init dvc init Initialize the project git add dvc add Track item in staging area git push dvc push Upload tracked files or directories to remote location git pull dvc pull Download tracked files or directories from remote location Some other tools include: Git LFS , LakeFS , FastDS","title":"Third-party Tools"},{"location":"developer_notes/operations/mlops/overview/#3-experiment-tracking","text":"Experiment tracking is the process of saving all experiment related information that you care about for every experiment you run. This \u201cmetadata you care about\u201d will strongly depend on your project, but it may include: Scripts used for running the experiment Environment configuration files Versions of the data used for training and evaluation Parameter configurations Evaluation metrics Model weights Performance visualizations (confusion matrix, ROC curve) So why do we do this? Model training is an iterative process, and shouldn't just be confined to using a single set of parameters or version of the data. You will need to experiment with a plethora of different configurations. Therefore its crucial to persist this information to have a data-centric approach for choosing which model gets deployed to production.","title":"3. Experiment Tracking"},{"location":"developer_notes/operations/mlops/overview/#third-party-tools_2","text":"MLFlow Weights & Biases Neptune","title":"Third-party Tools"},{"location":"developer_notes/operations/mlops/overview/#4-artifact-tracking","text":"","title":"4. Artifact Tracking"},{"location":"developer_notes/operations/mlops/overview/#components-and-artifacts","text":"You can think of ML Pipelines as an Direct Acyclic graphic (DAG). Conceptually, parts of the pipeline can be classified as one of the two categories: components and artifacts. Terminology Description Components Think of an action or verb - something that performs a task. Components can be scripts, notebooks or executables. Artifacts An artifact is any data file or directory produced within a run of the pipeline. Artifacts from one component may be used as the input to the next component. Every artifact should be automatically tracked and versioned.","title":"Components and Artifacts"},{"location":"developer_notes/operations/mlops/overview/#artifact-store","text":"The Artifact Store is a central component in any MLOps stack. As the name suggests, it acts as a data persistence layer where artifacts (e.g. datasets, models, images etc) generated by a component are stored. Its often the case that an artifact from one component (step) is fed into the next component. For example: The output artifact from a data preprocessing component would be the input artifact for a train component. It's not only a place to store items necessary for the ML pipeline to run successfully, we can store supplementary artifacts also. Let's say we periodically perform EDA in our ML Pipeline, and we have some graphs we want to keep for each experiment. These should be tracked and stored for future inspection.","title":"Artifact Store"},{"location":"developer_notes/operations/mlops/overview/#6-deployment","text":"Deployment isn't as simple as exposing a model binary through some API or having a CRON job perform batch inference to generate a csv. Not having a process where you can deploy a model to production with minimal friction will acrue technical debt as projects scale and the number of projects grows. Manually deploying models increases risk of downtime, which isn't acceptable for a commercial product. To avoid these hurdles there needs to be a robust deployment strategy. Regardless of your deployment type, be it Batch, Service or Real-time (streaming) these concepts still apply:","title":"6. Deployment"},{"location":"developer_notes/operations/mlops/overview/#inference-pipeline","text":"So you've written the code, trained the model and evaluated the results and now want to put a model into production. Firstly you need to be aware of inference pipelines. In machine learning, it's expected the data your model is exposed to is prepared in the same way as the data that was used to train it. This means you need to ensure all the same data preprocessing operations from development are applied to inference data in production. This is called Development Production Symmetry . So if we need to perform the same actions during training/inference time then we SHOULD reuse the same code. Don't duplicate the code. An Inference Pipeline constitutes two items: the preprocessing pipeline the model","title":"Inference Pipeline"},{"location":"developer_notes/operations/mlops/overview/#inference-artifact","text":"When training is complete then the preprocessing pipeline and model is serialized ( eg. saved to disk ). Inference Artifact is an instance of the Inference Pipeline containing the preprocessing code and a trained model. This inference artifact is then marked for production and deployed as a working component in the project pipeline. The reason why we make instances of an \"inference artifact\" rather than just reference code from the training pipeline is so that we can version our preprocessing code and model for each iteration. Sometimes the code or model maybe what changes, sometimes it's both.","title":"Inference Artifact"},{"location":"developer_notes/operations/mlops/overview/#model-bias","text":"When learning about ML you will have likely gained a habit of believing you should just optimization your model to have good metrics. Low error on test set isn't the be-all and end-all. If performance on disproportionately important samples aren't good, then the model is not fit for production. Bias is a systematic error from an erroneous assumption in the machine learning algorithm\u2019s modeling. The algorithm tends to systematically learn the wrong signals by not considering all the information contained within the data. Model bias may lead an algorithm to miss the relevant relationship between data inputs (features) and targeted outputs (predictions). In essence, bias arises when an algorithm has insufficient capability in learning the appropriate signal from the dataset. For decades, bias in machine learning has been recognized as a potential concern, but it remains a complex and challenging issue for machine learning researchers and engineers when deploying models into production. You can read more on the different types of bias here One way of checking for bias is through Data Slicing . Data slicing is when you computing a performance metric holding a feature or output fixed. So rather than evaluating performance on the entire test set, you extend it by evaluating breaking down by the feature you're concern with. For example: Gender, ethnicity, rare classes. Given the example below, we can see a hypothetical model that performs very well. However, after performing data slicing on features such as race we can see that hispanics are disproportionately scoring lower than others. This form of auditing can prevent potential legal or productional issues down the line. Race Model Accuracy Black 87.7% South East Asian 91.4% Hispanic 58% White 88.3%","title":"Model Bias"},{"location":"developer_notes/operations/mlops/overview/#model-cards","text":"Beside data versioning, artifact and experiment tracking - another important method of logging information about a model is by writing a \"model card\". Model cards aim to encapsulate all the key details about the model for future reference. It isn't a hard requirement, but it should be done for models that are deployed to production. The information could include: Data used How model should be interacted with Caveats, warnings or any shortcomings Metrics Parameters","title":"Model Cards"},{"location":"developer_notes/operations/mlops/overview/#when-to-deploy-a-model","text":"Before deploying a model into production, you should first go through the predeployment checklist: \u2705 Test Performance (Choose the best performing model) \u2705 Check for Bias \u2705 Write a Model Card If you believe everything looks good, then you can mark that model for production. If you aren't using a model registry then you can use a dedicated folder for the inference pipeline to point to. If you are using a model registry, you would most likely need to tag the model with a :prod or :latest tag. Additionally, you should aim to automate the items in this checklist so you can reduce the amount of manual interactions needed.","title":"When to deploy a model"},{"location":"developer_notes/operations/mlops/overview/#third-party-tools_3","text":"Aequitas - Bias and Fairness Audit Toolkit FairML - Python library that audits black-box predictive models","title":"Third-party Tools"},{"location":"developer_notes/operations/mlops/overview/#7-monitoring-maintenance","text":"The cascading affects of something changing upstream can be catostrophic for processes that are downstream. By monitoring aspects of the project we can see trends of our Infrastructure compute Statistics from data validation Concept and Data drift etc. The first thing you should do is a team-brainstorm of all the possible scenarios where things can go wrong, like Number of missing values over time, server load over time and memory usage, to mention a few. It's common practice to set alerts for thresholds for certain metrics. For example: we can see a trend that the an increase in requests to a model API is putting strain on the server's performance. By using monitoring we can see in advance that we should scale-up the compute power of the API.","title":"7. Monitoring &amp; Maintenance"},{"location":"developer_notes/operations/mlops/overview/#model-decay","text":"The definition of model decay is the occurence of a model in production, when evaulated over time, performs worse that is previously did. The cause of this is due to \"data drift\" as nothing is actually changing in the model itself - but just to differentiate it from the data drift seen in the Data validation section lets refer to it as model decay. There are 3 ways to assess for model drift: Raw comparison test A raw comparison is just a simple check to see if the target evaluation metric performed worse the previous version(s) of the model. Parametric Significance test Raw comparison test may indicate model drift has occurred, but this may be due to random chance or the score difference is so minuscule that it really isn\u2019t significant (ie: not worth training another model). Using a paremteric significance test provides more reasonable condition to check against for model drift. To do this you must: 1. Calculate mean and STD of all previous model scores (bell curve) 2. If the new model score is at least 2 standard deviations from the mean, we can conclude model drift has occurred. Non-parametric outlier test Paremteric significance tests assume that the distribution going to be in a bell curve (paremetric). This isn't appropraite for all cases, as it's senstive to outliers. A non-parametric measurment such as the Interquartile Range Rule (IQR) to measure how far a datapoint is from the rest of the data. Essentially, this is to find outliers (in this case, the new model results if model drift occurred). Note: depending on what your target evaluation metric is the way you use IQR will be different. For metrics where higher values indicate worse performance, you need to look for upper outliers For metrics where lower values indicate worse performance, you need to look out for lower outliers.","title":"Model Decay"},{"location":"developer_notes/operations/mlops/overview/#handling-decay-with-continuous-deployment","text":"Okay so you know how to assess for model decay, now what? The purpose of this is to have a data-centric approach to deciding when your model needs to be updated. For example, lets say once a month you check for model drift. New data is given to the model and it's performance is evaluated. Using the the evaluation metric(s) we perform a model decay test to compare to older evaluations. You should make an automated process to spin off run of the training pipeline if the model scores considerably worse. Having the ability to continously deliver new models without manual interaction speeds up \"time-to-production\" and reduces down-time.","title":"Handling Decay with Continuous Deployment"},{"location":"developer_notes/operations/mlops/overview/#third-party-tools_4","text":"Prometheus Evidently.ai Grafana","title":"Third-party Tools"},{"location":"developer_notes/operations/mlops/overview/#8-pipeline-orchestration","text":"Traditionally, using Cronjobs was the default for scheduling events. However, some issues with this approach include poor failure handling, difficulty monitoring current and past processes and lack of scalability. Airflow aims to remedy these issues. See our Airflow documentation here","title":"8. Pipeline Orchestration"},{"location":"developer_notes/operations/mlops/overview/#third-party-tools_5","text":"Airflow Offical Documentations Github Actions Suggested GitHub Actions My MLOps - Stack Builder Machine Learning Operations (MLOps): Overview, Definition, and Architecture - Kreuzberger et al. Categories of MLOps Tools Hidden Technical Debt in Machine Learning Systems Netune AI MLOps Blog Types of Data Drift","title":"Third-party Tools"},{"location":"developer_notes/operations/project_planning/agile/overview/","text":"AGILE What is AGILE? Agile project management is a project framework or \"principles\" that take an iterative approach towards the completion of a project. Such as ambiguity, scope creep, team burnout etc. Having a set of principles to base our project management structure helps combat these operational issues. AGILE Manifesto The agile manifesto is a great starting point for anyone looking to familiarize themselves with the agile methodology. The manifesto outlines the 4 values and 12 principles of agile software development and was actually created by a group of software developers in an effort to provide a clear and alternative set of processes for developing software. Read the manifesto here The agile values include prioritizing: Individuals and Interactions Over Processes and Tools. Working Software Over Comprehensive Documentation. Customer Collaboration Over Contract Negotiation. Responding to Change Over Following a Plan. AGILE Methodologies Agile refers to the methods and best practices for organizing projects based on the values and principles documented in the Agile Manifesto. However, depending on the type of project you are working on you amy want to implement Agile in different ways. There are many different types of methodologies to choose from. Here are some of the most common Agile frameworks: Scrum Kanban Extreme Programming (XP) Feature-driven development (FDD) Dynamic Systems Development Method (DSDM) Crystal Lean Scrum and Agile are related, but they are not the same thing. It's a specific methodology that provides a structure for implementing Agile principles in a team setting. In summary, Agile is a mindset, a philosophy, a set of values and principles, while Scrum is a methodology, a way of implementing Agile, with a specific set of roles, events and artifacts. Scrum is the preferred methodology for most cases, and is also highly used within Optum. To learn more about about the different Agile methodologies you can read here . Why Use Scrum? According to the PMI Pulse of the Profession annual report , 48% of projects are not finished within the scheduled time, 43% go over their original budget, and 31% of projects don't meet the initial goals and business intent. Agile Srum is particularly useful for projects with rapidly changing or highly uncertain requirements, as it allows teams to respond to change and adapt to new information throughout the development process. Faster delivery of working software Increased flexibility and ability to adapt to changing requirements Improved communication and collaboration among team members Greater visibility into the development process, which facilitates better planning and decision making Better customer and end-user involvement in the development process, which helps to ensure that the final product meets their needs and expectations. The Different Scrum Ceremonies Scrum has a set of ceremonies, or meetings, that are designed to help the team stay organized and focused on the goals of the project. These usually are encapsulated in iterative time-boxes known as sprints, which last 2 weeks. Ceremony Purpose Daily Stand-Up A daily stand-up, also known as a daily scrum, is a short meeting that takes place every day at the same time during a sprint in an Agile software development process. The meeting is usually held in the morning and is attended by all the members of the development team, as well as the Scrum Master or project manager. The purpose of the daily stand-up is for the team members to quickly share what they did yesterday, what they plan to do today, and to report any obstacles or issues that are blocking their progress. The meeting is conducted in a round-robin format, with each team member taking a turn to speak. The meeting should be time-boxed to 15 minutes or less. The goal of the daily stand-up is to ensure that the team stays on track, that everyone is aware of what the other team members are working on, and that any issues or problems that arise can be addressed quickly. The daily stand-up helps to keep the team members accountable, It also helps to improve communication and coordination among the team members Sprint Planning The purpose of the sprint planning meeting is to plan the work that the team will complete during the upcoming sprint. The meeting is typically facilitated by the Scrum Master or project manager and is attended by the entire development team. During the sprint planning meeting, the team reviews the items in the product backlog and selects the highest-priority items to be included in the sprint. They also discuss the details of each item and break them down into smaller, manageable tasks. The team then estimates the effort required to complete each task and creates a sprint backlog, which is a list of all the tasks that need to be done during the sprint. Once the sprint backlog is created, the team sets a sprint goal, which outlines the objectives that need to be achieved during the sprint. Sprint Review/Retrospective A sprint retrospective, also known as a sprint retro, is a meeting that is held at the end of a sprint (a fixed period of time, typically two to four weeks, during which a specific amount of work is completed) in an agile software development process. The purpose of the sprint retrospective is for the team to reflect on their work during the sprint and identify areas for improvement. The team members discuss what went well during the sprint, what challenges they faced, and what could be done differently in the future. The goal is to identify and implement improvements that will lead to better performance and efficiency in future sprints. This meeting is usually facilitated by the Scrum Master or the project manager and it is open for all the team members. The outcome of the retrospective should be a list of action items that the team commits to implementing in the next sprint. Backlog Refinement Backlog refinement (also known as backlog grooming or story time) is a process in which the team responsible for a software development project reviews and updates the items in the product backlog, which is a prioritized list of work that needs to be done. During the refinement process, the team assesses the user stories in the backlog, clarifies requirements, estimates the effort required to complete each item, and reprioritizes the backlog as needed. The goal of backlog refinement is to ensure that the highest-priority items are clearly defined and ready for the team to work on, and that the backlog as a whole remains aligned with the overall goals of the project. User Stories In Scrum, a user story is a brief, informal description of a feature or requirement from the perspective of an end user. It is used to describe the desired outcome of a piece of work and serves as the basis for creating tasks and determining acceptance criteria. User stories are typically written in the format \"As a [ user ], I want [ feature ], so that [ benefit ].\" They are used in the Scrum framework to help team members understand the customer's needs and priorities, and to guide the development process. Informally, it's a bit pedantic to write stories like this all the time. Usually you can just write the tasks and a brief description. a user story typically goes through the following states or stages during its lifecycle: Backlog: The user story is first added to the product backlog, which is a prioritized list of features and requirements that the team needs to work on. Grooming: The team discusses and refines the user story during regular grooming sessions. They may break it down into smaller tasks, add acceptance criteria, and clarify any ambiguities or uncertainties. Defined: The team selects user stories from the product backlog to work on during the upcoming sprint. They also create tasks and assign them to team members. In Progress: The team starts working on the user story and its associated tasks during the sprint. They may update the status of the story as they make progress. Completed: The team finishes working on the user story and its associated tasks. The story is then reviewed and accepted by the product owner or team memebers Done/Accepted: The user story is considered done when it has been accepted by the product owner or team lead and the code has been deployed to production. Being accepted can also depend on the Definition of Done (DoD) . It's worth to mention that there are different variations of the process and some organizations may use slightly different states, but overall this is the common flow. Glossary Glossary Velocity: Velocity is a measure of the amount of work a Team can tackle during a single Sprint and is the key metric in Scrum. Velocity is calculated at the end of the Sprint by totaling the Points for all fully completed User Stories. Capacity: Capacity represents the amount of work that can be completed within a given time frame and is based on the number of hours a person or team has available to complete that work. Scrum Master: A scrum master oversees the development process and acts as a problem solver for the team; preventing roadblocks and enforcing the agile way of doing things. Backlog: The backlog is the ever changing list of the software's requirements. It's not to be seen as a to do list so much as a prioritized list of desired features of the product provided by the stakeholders. User Story: A user story is an informal, general explanation of a software feature written from the perspective of the end user. Sprint: A sprint is a short development phase for a specified time period. Sprints help prevent projects from feeling overwhelming and allows feedback to be given at appropriate junctures. Timeboxing: Timeboxing is when you set a limit in the amound of time spent to accomplish a goal. This is extremely helpful in terms of productivity, and controlling the scale of a project. Epic: An agile epic is a body of work that can be broken down into specific tasks (called user stories) based on the needs/requests of customers or end-users Feature Creep: When the features of a project expand, or new additional features are requested beyond the original scope. See here Scope Creep: This is loosely related to feature creep, but there is a distinction. This is when the scope of a project seems to be continuously changing due to poorly planned and documented scoping. see here Product Owner: The person responsible for creating and prioritizing the product backlog, and ensuring that the final product meets the needs of the stakeholders. Definition of Done (DoD): A set of criteria that must be met before a product backlog item can be considered complete and ready for delivery. References What Are Epics in Agile? Definition, Examples, and Tracking Agile vs. waterfall project management What Are Agile Ceremonies? Top 10 Agile Terms What are the Different Types of Agile Methodologies?","title":"AGILE"},{"location":"developer_notes/operations/project_planning/agile/overview/#agile","text":"","title":"AGILE"},{"location":"developer_notes/operations/project_planning/agile/overview/#what-is-agile","text":"Agile project management is a project framework or \"principles\" that take an iterative approach towards the completion of a project. Such as ambiguity, scope creep, team burnout etc. Having a set of principles to base our project management structure helps combat these operational issues.","title":"What is AGILE?"},{"location":"developer_notes/operations/project_planning/agile/overview/#agile-manifesto","text":"The agile manifesto is a great starting point for anyone looking to familiarize themselves with the agile methodology. The manifesto outlines the 4 values and 12 principles of agile software development and was actually created by a group of software developers in an effort to provide a clear and alternative set of processes for developing software. Read the manifesto here The agile values include prioritizing: Individuals and Interactions Over Processes and Tools. Working Software Over Comprehensive Documentation. Customer Collaboration Over Contract Negotiation. Responding to Change Over Following a Plan.","title":"AGILE Manifesto"},{"location":"developer_notes/operations/project_planning/agile/overview/#agile-methodologies","text":"Agile refers to the methods and best practices for organizing projects based on the values and principles documented in the Agile Manifesto. However, depending on the type of project you are working on you amy want to implement Agile in different ways. There are many different types of methodologies to choose from. Here are some of the most common Agile frameworks: Scrum Kanban Extreme Programming (XP) Feature-driven development (FDD) Dynamic Systems Development Method (DSDM) Crystal Lean Scrum and Agile are related, but they are not the same thing. It's a specific methodology that provides a structure for implementing Agile principles in a team setting. In summary, Agile is a mindset, a philosophy, a set of values and principles, while Scrum is a methodology, a way of implementing Agile, with a specific set of roles, events and artifacts. Scrum is the preferred methodology for most cases, and is also highly used within Optum. To learn more about about the different Agile methodologies you can read here .","title":"AGILE Methodologies"},{"location":"developer_notes/operations/project_planning/agile/overview/#why-use-scrum","text":"According to the PMI Pulse of the Profession annual report , 48% of projects are not finished within the scheduled time, 43% go over their original budget, and 31% of projects don't meet the initial goals and business intent. Agile Srum is particularly useful for projects with rapidly changing or highly uncertain requirements, as it allows teams to respond to change and adapt to new information throughout the development process. Faster delivery of working software Increased flexibility and ability to adapt to changing requirements Improved communication and collaboration among team members Greater visibility into the development process, which facilitates better planning and decision making Better customer and end-user involvement in the development process, which helps to ensure that the final product meets their needs and expectations.","title":"Why Use Scrum?"},{"location":"developer_notes/operations/project_planning/agile/overview/#the-different-scrum-ceremonies","text":"Scrum has a set of ceremonies, or meetings, that are designed to help the team stay organized and focused on the goals of the project. These usually are encapsulated in iterative time-boxes known as sprints, which last 2 weeks. Ceremony Purpose Daily Stand-Up A daily stand-up, also known as a daily scrum, is a short meeting that takes place every day at the same time during a sprint in an Agile software development process. The meeting is usually held in the morning and is attended by all the members of the development team, as well as the Scrum Master or project manager. The purpose of the daily stand-up is for the team members to quickly share what they did yesterday, what they plan to do today, and to report any obstacles or issues that are blocking their progress. The meeting is conducted in a round-robin format, with each team member taking a turn to speak. The meeting should be time-boxed to 15 minutes or less. The goal of the daily stand-up is to ensure that the team stays on track, that everyone is aware of what the other team members are working on, and that any issues or problems that arise can be addressed quickly. The daily stand-up helps to keep the team members accountable, It also helps to improve communication and coordination among the team members Sprint Planning The purpose of the sprint planning meeting is to plan the work that the team will complete during the upcoming sprint. The meeting is typically facilitated by the Scrum Master or project manager and is attended by the entire development team. During the sprint planning meeting, the team reviews the items in the product backlog and selects the highest-priority items to be included in the sprint. They also discuss the details of each item and break them down into smaller, manageable tasks. The team then estimates the effort required to complete each task and creates a sprint backlog, which is a list of all the tasks that need to be done during the sprint. Once the sprint backlog is created, the team sets a sprint goal, which outlines the objectives that need to be achieved during the sprint. Sprint Review/Retrospective A sprint retrospective, also known as a sprint retro, is a meeting that is held at the end of a sprint (a fixed period of time, typically two to four weeks, during which a specific amount of work is completed) in an agile software development process. The purpose of the sprint retrospective is for the team to reflect on their work during the sprint and identify areas for improvement. The team members discuss what went well during the sprint, what challenges they faced, and what could be done differently in the future. The goal is to identify and implement improvements that will lead to better performance and efficiency in future sprints. This meeting is usually facilitated by the Scrum Master or the project manager and it is open for all the team members. The outcome of the retrospective should be a list of action items that the team commits to implementing in the next sprint. Backlog Refinement Backlog refinement (also known as backlog grooming or story time) is a process in which the team responsible for a software development project reviews and updates the items in the product backlog, which is a prioritized list of work that needs to be done. During the refinement process, the team assesses the user stories in the backlog, clarifies requirements, estimates the effort required to complete each item, and reprioritizes the backlog as needed. The goal of backlog refinement is to ensure that the highest-priority items are clearly defined and ready for the team to work on, and that the backlog as a whole remains aligned with the overall goals of the project.","title":"The Different Scrum Ceremonies"},{"location":"developer_notes/operations/project_planning/agile/overview/#user-stories","text":"In Scrum, a user story is a brief, informal description of a feature or requirement from the perspective of an end user. It is used to describe the desired outcome of a piece of work and serves as the basis for creating tasks and determining acceptance criteria. User stories are typically written in the format \"As a [ user ], I want [ feature ], so that [ benefit ].\" They are used in the Scrum framework to help team members understand the customer's needs and priorities, and to guide the development process. Informally, it's a bit pedantic to write stories like this all the time. Usually you can just write the tasks and a brief description. a user story typically goes through the following states or stages during its lifecycle: Backlog: The user story is first added to the product backlog, which is a prioritized list of features and requirements that the team needs to work on. Grooming: The team discusses and refines the user story during regular grooming sessions. They may break it down into smaller tasks, add acceptance criteria, and clarify any ambiguities or uncertainties. Defined: The team selects user stories from the product backlog to work on during the upcoming sprint. They also create tasks and assign them to team members. In Progress: The team starts working on the user story and its associated tasks during the sprint. They may update the status of the story as they make progress. Completed: The team finishes working on the user story and its associated tasks. The story is then reviewed and accepted by the product owner or team memebers Done/Accepted: The user story is considered done when it has been accepted by the product owner or team lead and the code has been deployed to production. Being accepted can also depend on the Definition of Done (DoD) . It's worth to mention that there are different variations of the process and some organizations may use slightly different states, but overall this is the common flow.","title":"User Stories"},{"location":"developer_notes/operations/project_planning/agile/overview/#glossary","text":"Glossary Velocity: Velocity is a measure of the amount of work a Team can tackle during a single Sprint and is the key metric in Scrum. Velocity is calculated at the end of the Sprint by totaling the Points for all fully completed User Stories. Capacity: Capacity represents the amount of work that can be completed within a given time frame and is based on the number of hours a person or team has available to complete that work. Scrum Master: A scrum master oversees the development process and acts as a problem solver for the team; preventing roadblocks and enforcing the agile way of doing things. Backlog: The backlog is the ever changing list of the software's requirements. It's not to be seen as a to do list so much as a prioritized list of desired features of the product provided by the stakeholders. User Story: A user story is an informal, general explanation of a software feature written from the perspective of the end user. Sprint: A sprint is a short development phase for a specified time period. Sprints help prevent projects from feeling overwhelming and allows feedback to be given at appropriate junctures. Timeboxing: Timeboxing is when you set a limit in the amound of time spent to accomplish a goal. This is extremely helpful in terms of productivity, and controlling the scale of a project. Epic: An agile epic is a body of work that can be broken down into specific tasks (called user stories) based on the needs/requests of customers or end-users Feature Creep: When the features of a project expand, or new additional features are requested beyond the original scope. See here Scope Creep: This is loosely related to feature creep, but there is a distinction. This is when the scope of a project seems to be continuously changing due to poorly planned and documented scoping. see here Product Owner: The person responsible for creating and prioritizing the product backlog, and ensuring that the final product meets the needs of the stakeholders. Definition of Done (DoD): A set of criteria that must be met before a product backlog item can be considered complete and ready for delivery.","title":"Glossary"},{"location":"developer_notes/operations/project_planning/agile/overview/#references","text":"What Are Epics in Agile? Definition, Examples, and Tracking Agile vs. waterfall project management What Are Agile Ceremonies? Top 10 Agile Terms What are the Different Types of Agile Methodologies?","title":"References"},{"location":"developer_notes/operations/project_planning/scoping/hlp/","text":"Human Level Performance (HLP) You often see academic papers rely on beating humans to perform specific tasks as a measure of success. However, there are some things to be aware of before doing this type performance evaluation. What is HLP? HLP is the evaluation of humans in the loop performing a task, that the proposed ML model(s) may be replacing or assisting. This is often used as baseline metric that the model must beat to prove value. This is done by having the humans perform the same task as the proposed model. Their answers are recorded and the evaluation metrics are calculated. Issues with HLP Let's take an example where several human evaluators classify cases of inpatient admission risk, where the outcome y is either a zero or one. A consensus is done: For a given sample 70% of the evaluators agree the person is of high risk, and 30% agree they are of low risk. We can calculate that 2 random humans will agree with eachother 58% of the time. consensus rate = 0.7 ^ 2 + 0.3 ^ 2 = 0.58 Now lets say that the ML model agrees with humans 70% of the time, that is a 12% increase in performance compared to HLP. However, this measure of comparing model vs human can be a red-herring as it can distract from the more significant errors the model may be producing. Improving HLP If there is a disagreement (lets say +20% of the time) this could be due to the defintion of the label. It's worth checking how the label was defined because it could have just been made by another human, who have their own set of biases. When the ground truth is truly defined and humans share the same criteria for labelling a sample, the evaulation must be taken again. Once the consensus rate is high enough only then will it safe to use HLP as a measure of success. Going back and getting them to redefine how they would label a sample can raise HLP to 100%. In certain circumstances, like when a model is being requested due to HLP being so low, there is a possibility that an ML model may not even be needed when it's discovered that the humans in the loop are given inconsistent labelling instructions. Summary HLP is a common way of evaluating model performance, but is often misused in academic papers Done by comparing accuracy of model vs humans Before comparing the model to HLP, compare humans to themselves If there is a high disagreement rate among human evaluators then its worth checking the labelling criteria as ambiguous instructions can lead to inconsistent results Once the consensus rate is high enough its safe to use HLP as a baseline measure for success.","title":"Human Level Performance (HLP)"},{"location":"developer_notes/operations/project_planning/scoping/hlp/#human-level-performance-hlp","text":"You often see academic papers rely on beating humans to perform specific tasks as a measure of success. However, there are some things to be aware of before doing this type performance evaluation.","title":"Human Level Performance (HLP)"},{"location":"developer_notes/operations/project_planning/scoping/hlp/#what-is-hlp","text":"HLP is the evaluation of humans in the loop performing a task, that the proposed ML model(s) may be replacing or assisting. This is often used as baseline metric that the model must beat to prove value. This is done by having the humans perform the same task as the proposed model. Their answers are recorded and the evaluation metrics are calculated.","title":"What is HLP?"},{"location":"developer_notes/operations/project_planning/scoping/hlp/#issues-with-hlp","text":"Let's take an example where several human evaluators classify cases of inpatient admission risk, where the outcome y is either a zero or one. A consensus is done: For a given sample 70% of the evaluators agree the person is of high risk, and 30% agree they are of low risk. We can calculate that 2 random humans will agree with eachother 58% of the time. consensus rate = 0.7 ^ 2 + 0.3 ^ 2 = 0.58 Now lets say that the ML model agrees with humans 70% of the time, that is a 12% increase in performance compared to HLP. However, this measure of comparing model vs human can be a red-herring as it can distract from the more significant errors the model may be producing.","title":"Issues with HLP"},{"location":"developer_notes/operations/project_planning/scoping/hlp/#improving-hlp","text":"If there is a disagreement (lets say +20% of the time) this could be due to the defintion of the label. It's worth checking how the label was defined because it could have just been made by another human, who have their own set of biases. When the ground truth is truly defined and humans share the same criteria for labelling a sample, the evaulation must be taken again. Once the consensus rate is high enough only then will it safe to use HLP as a measure of success. Going back and getting them to redefine how they would label a sample can raise HLP to 100%. In certain circumstances, like when a model is being requested due to HLP being so low, there is a possibility that an ML model may not even be needed when it's discovered that the humans in the loop are given inconsistent labelling instructions.","title":"Improving HLP"},{"location":"developer_notes/operations/project_planning/scoping/hlp/#summary","text":"HLP is a common way of evaluating model performance, but is often misused in academic papers Done by comparing accuracy of model vs humans Before comparing the model to HLP, compare humans to themselves If there is a high disagreement rate among human evaluators then its worth checking the labelling criteria as ambiguous instructions can lead to inconsistent results Once the consensus rate is high enough its safe to use HLP as a baseline measure for success.","title":"Summary"},{"location":"developer_notes/operations/project_planning/scoping/overview/","text":"Scoping a Project Many challenges come with scoping out an ML project, such as ambiguity around requirements, how the solution should work or whether its even feasible to implement it. Having a good set of tools for navigating these operational speedbumps in development can help make your work more efficient How to scope a project? 1. Identify the Business Problem Firstly you should meet with a business or product owner, not someoneone technical. It's important to get as much information as possible from them, preferably in writing so there can be no ambiguity on the requirements. Here is a template list of questions to ask when beginning the scoping phase of a project. Some of these may seem very simple, but they can drastically reduce ambiguity and confusion: What is the \"elevator pitch\" for your problem? (one-liner) Can you give a detailed description of the problem? Is there an existing solution for this problem or is this a completely new project? How is the problem currently being addressed, are there humans currently performing this task? What are the business metrics for success? How many users do you expect to use this model? Should it be a batch report, an API or real-time solution? Given a reasonable timeline, when would you expect this project to be in production? At what cadence should we meet? (weekly, bi-weekly, etc.) At the end of the project, what would you like to see? When you have a clear idea of what the business want, you should then meet with a technical SME or development team that works with the business. This will give you an opportunity for gathering technical information. Some questions may include: What data is needed for this project and how can we get access? What is the latency of the data? Are there any known data quality issues? Are there PHI/PII concerns? How frequently can we request new data? If there is a technical solution already in place, can you give a walk through? Is there any record of data lineage or data provenance? 2. Brainstorm Solutions Your team should review the answers to the questions in the previous section and then to meet up. The purpose of this meeting should be to: - a. Identify possible ML models for this problem. This can be achieved by reviewing the current academic literature and using that as a goal to strive towards, or pitching a novel approach. - b. Address the feedback from step 1 and use that to plan the subsequent steps It's important to also be self-critical before committing to building a solution. Some questions to ask amongst yourselves are: Do we have the capacity or bandwidth to tackle a project of this size? Do we have the skill set needed for this specific problem? How long would it take to up-skill? (estimate) Do we have time constraints? 3. Evaluate Technical Feasability Assessing feasability - Before building a prototype of the project, you should use external benchmarks to assess the feasability of a project. This would inlcude reviewing recent academic literature, finding examples online or seeing if something similar was already implemented within with company. After you have brainstormed ideas, search through github and search the common terms associated with those ideas. Benchmarking - Depending on the type and maturity of project you are attempting to tackle, there are different ways of benchmarking. For unstructured data problems, assessing how humans perform on a task can be a good future predictor of project success. If a human, given the same set of data, can perform a task well then that will give you an early indication that the problem will be easy or hard to solve. Read more about how to assess using HLP here . For structured data problems, HLP may not be suitable as humans are generally worse at assessing structured data. When you have a set of features and a label, you should perform some predictive feature analysis to get a sense whether the data you have is useful or not For already existing projects, using the project history can give a good benchmark. Lets say you have a system that is already in place to perform some task for humans. One way of seeing trends of the current system would be to assess their performance for different time periods (like once a quarter), as well as HLP within those time periods. The idea here is to see how the existing system in place has reduced error percentage relative to HLP. For example, if their current system has a 30% reduction in error each quarter relative to humans, then that is a potential benchmark that we want to beat. When you have completed identifying the business problem, came up with possibles solutions and have assessed the feasability, you should write a technical feasability document. This is essentially a write up of your understanding of the problem and how you aim to solve it. It's useful because the development team can have all the specifications on paper to review and add suggestions where suitable, and can lower ambiguity and misunderstandings. Copy the contents of the Technical Feasability Document Template and paste to a markdown file. 4. Determine Milestones After understanding the problem well, determining how to approach the problem and assess feasability it's time to set milestones. Determinining milestones for your project will require laying our key specifications: ML Metrics (Accuracy, F1, Precision, Recall, etc.) Software metrics (latency, throughput, etc) Business Metrics (revenue, cost reduction, time saved, etc.) Resources needed (data, people, external team collaboration) Timeline (when you hope to complete milestones) If you are unsure about any of these, consider some benchmarking or building a ad-hoc model to use that information for a estimating a larger scale execution. 5. Writing a Project Proposal When you have completed the scoping of the project you will now need to write a project proposal. The purpose of this document is to highlight the purpose of the project, what is needed from a technical and functional aspect, define deliverables and establish times and dates when to touch base and discuss progress and to discuss potential project risks. This should reduce ambiguity on what should be delivered and set reasonable expectations. Download the Project Proposal Document Template . Explanations of each section are highlighted in green within the document. References IDP Technical Feasability Document Template IDP Project Proposal Document Template How to Choose a Feature Selection Method For Machine Learning","title":"Overview"},{"location":"developer_notes/operations/project_planning/scoping/overview/#scoping-a-project","text":"Many challenges come with scoping out an ML project, such as ambiguity around requirements, how the solution should work or whether its even feasible to implement it. Having a good set of tools for navigating these operational speedbumps in development can help make your work more efficient","title":"Scoping a Project"},{"location":"developer_notes/operations/project_planning/scoping/overview/#how-to-scope-a-project","text":"","title":"How to scope a project?"},{"location":"developer_notes/operations/project_planning/scoping/overview/#1-identify-the-business-problem","text":"Firstly you should meet with a business or product owner, not someoneone technical. It's important to get as much information as possible from them, preferably in writing so there can be no ambiguity on the requirements. Here is a template list of questions to ask when beginning the scoping phase of a project. Some of these may seem very simple, but they can drastically reduce ambiguity and confusion: What is the \"elevator pitch\" for your problem? (one-liner) Can you give a detailed description of the problem? Is there an existing solution for this problem or is this a completely new project? How is the problem currently being addressed, are there humans currently performing this task? What are the business metrics for success? How many users do you expect to use this model? Should it be a batch report, an API or real-time solution? Given a reasonable timeline, when would you expect this project to be in production? At what cadence should we meet? (weekly, bi-weekly, etc.) At the end of the project, what would you like to see? When you have a clear idea of what the business want, you should then meet with a technical SME or development team that works with the business. This will give you an opportunity for gathering technical information. Some questions may include: What data is needed for this project and how can we get access? What is the latency of the data? Are there any known data quality issues? Are there PHI/PII concerns? How frequently can we request new data? If there is a technical solution already in place, can you give a walk through? Is there any record of data lineage or data provenance?","title":"1. Identify the Business Problem"},{"location":"developer_notes/operations/project_planning/scoping/overview/#2-brainstorm-solutions","text":"Your team should review the answers to the questions in the previous section and then to meet up. The purpose of this meeting should be to: - a. Identify possible ML models for this problem. This can be achieved by reviewing the current academic literature and using that as a goal to strive towards, or pitching a novel approach. - b. Address the feedback from step 1 and use that to plan the subsequent steps It's important to also be self-critical before committing to building a solution. Some questions to ask amongst yourselves are: Do we have the capacity or bandwidth to tackle a project of this size? Do we have the skill set needed for this specific problem? How long would it take to up-skill? (estimate) Do we have time constraints?","title":"2. Brainstorm Solutions"},{"location":"developer_notes/operations/project_planning/scoping/overview/#3-evaluate-technical-feasability","text":"Assessing feasability - Before building a prototype of the project, you should use external benchmarks to assess the feasability of a project. This would inlcude reviewing recent academic literature, finding examples online or seeing if something similar was already implemented within with company. After you have brainstormed ideas, search through github and search the common terms associated with those ideas. Benchmarking - Depending on the type and maturity of project you are attempting to tackle, there are different ways of benchmarking. For unstructured data problems, assessing how humans perform on a task can be a good future predictor of project success. If a human, given the same set of data, can perform a task well then that will give you an early indication that the problem will be easy or hard to solve. Read more about how to assess using HLP here . For structured data problems, HLP may not be suitable as humans are generally worse at assessing structured data. When you have a set of features and a label, you should perform some predictive feature analysis to get a sense whether the data you have is useful or not For already existing projects, using the project history can give a good benchmark. Lets say you have a system that is already in place to perform some task for humans. One way of seeing trends of the current system would be to assess their performance for different time periods (like once a quarter), as well as HLP within those time periods. The idea here is to see how the existing system in place has reduced error percentage relative to HLP. For example, if their current system has a 30% reduction in error each quarter relative to humans, then that is a potential benchmark that we want to beat. When you have completed identifying the business problem, came up with possibles solutions and have assessed the feasability, you should write a technical feasability document. This is essentially a write up of your understanding of the problem and how you aim to solve it. It's useful because the development team can have all the specifications on paper to review and add suggestions where suitable, and can lower ambiguity and misunderstandings. Copy the contents of the Technical Feasability Document Template and paste to a markdown file.","title":"3. Evaluate Technical Feasability"},{"location":"developer_notes/operations/project_planning/scoping/overview/#4-determine-milestones","text":"After understanding the problem well, determining how to approach the problem and assess feasability it's time to set milestones. Determinining milestones for your project will require laying our key specifications: ML Metrics (Accuracy, F1, Precision, Recall, etc.) Software metrics (latency, throughput, etc) Business Metrics (revenue, cost reduction, time saved, etc.) Resources needed (data, people, external team collaboration) Timeline (when you hope to complete milestones) If you are unsure about any of these, consider some benchmarking or building a ad-hoc model to use that information for a estimating a larger scale execution.","title":"4. Determine Milestones"},{"location":"developer_notes/operations/project_planning/scoping/overview/#5-writing-a-project-proposal","text":"When you have completed the scoping of the project you will now need to write a project proposal. The purpose of this document is to highlight the purpose of the project, what is needed from a technical and functional aspect, define deliverables and establish times and dates when to touch base and discuss progress and to discuss potential project risks. This should reduce ambiguity on what should be delivered and set reasonable expectations. Download the Project Proposal Document Template . Explanations of each section are highlighted in green within the document.","title":"5. Writing a Project Proposal"},{"location":"developer_notes/operations/project_planning/scoping/overview/#references","text":"IDP Technical Feasability Document Template IDP Project Proposal Document Template How to Choose a Feature Selection Method For Machine Learning","title":"References"},{"location":"project_documentation/scoping/v0.0.1/","text":"Scoping Project v0.0.1 26th March 2023 Note This is done without any field research done. Its just initial brainstorming and building an intuition of how the system might look. Brainstorm of Feature Ideas Lets say a blind or visually imparied person is looking for something in a market or a library, imagine they have an AI assistant that aids them in their daily tasks. The user could talk to their assistant and say \"Hey, i'm going to go to the market to pick up some milk and bread\". The assistant should have the ability to maintain that information and understand the context in which way it should help. IE: When the assistant sees that the user is in the store, it should be able to recall the previous prompt or conversation when the user was talking about the items they required. For this a a few technologies would be required: Computer vision for classifying video feed. Its uncertain if this inference can be done on the edge device or if it must be done in the cloud. Most likely will be cloud, A Large Language Model . This is used for maintaining a conversational aspect to the assistant. LLM's such as ChatGPT have blown all other methods of producing chatbots we have ever seen. Big bets were placed on previous attempts such as Facebooks Chatbot developer SDK which allowed developers to create chatbots using Messenger. However, these chatbots where mostly rule based and did not provide a natural feeling experience. It really just slowed done being able to find the information you need that could have been easily solved by a simple google search. Therefore, using a generalized pretrained model is crucial for providing the feeling of authenticity within the conversations the user may have with it. Some alteratioins will be needed to this model. The details are unkown as of yet, but this LLM will need to have the ability to intake data from the output of the computer vision algorithm. This would require either two LLMs (one for conversations and another for interpreting what the CV algorithm is classifying in the moment), alternatively; a refactored model that takes two inputs: user input or classification input. There would also need to be a system that ties all of these together in real-time. Speech To Text will be needed to interpret what is being said by the user into data that the LLM can work with (tokens). Its currently unknown if this could be done on edge or if it would require transcription to be done in the cloud. Text To Speech is required to provide a vocal output to the user. The output response from the LLM system would be synthesized by some speech synthesis model. It would need to be omptimized for quick inference times to improve latency. Possible the audio could be generated on edge by sending instructions to the device. Its also possible that the audio could be transferred from the cloud, via VoIP Personality Module: To provide a unique experience and the illusion of the assistant having a personality would require some regulatory system running in the background. It should be able to have a configurable personality setting, where the user can select on which scale that what certain aspects of the assistants \"personality\" should be. This should be based on a valid personality traid model. There are many personality models out there, but the The Big Five Personality Traits model is generally considered the most widely accepted and used model for measuring personality. The Big Five model has gained popularity due to its ability to provide a comprehensive understanding of personality and its applicability in various fields, including psychology, organizational behavior, and marketing. Moreover, research has supported the validity and reliability of the Big Five model across cultures and languages. The configurations of the assistants persona should be interpretable by the LLM, and should influence the type of response the LLM produces. This system should be dynamic, meaning the parameters for the persona should be updating consistently throughout the span of conversations had with the assitant. The responses returned from the LLM should be analysis using sentiment analysis. Some system should be in place the use the output of the sentiment analysis to regulate the persona parameters. For example: If the user speaks with apathy towards the assistant then the responses wouldn't be as cheerful. Doing so would break the illusion of an authentic artificial personality. Edge Device: Ideally, to provide the best experience this should be wearable device such as glasses. Proposed system Architecture User speaks to edge device. It is unknown as of yet whether or not speech to text happens on the edge device or on the cloud. Right now it's assumed that is doesn't. The request reaches an endpoint, to where its sends the speech audio to a Text-To-Speech model to be turne ...","title":"v0.0.1"},{"location":"project_documentation/scoping/v0.0.1/#scoping-project-v001","text":"26th March 2023 Note This is done without any field research done. Its just initial brainstorming and building an intuition of how the system might look.","title":"Scoping Project v0.0.1"},{"location":"project_documentation/scoping/v0.0.1/#brainstorm-of-feature-ideas","text":"Lets say a blind or visually imparied person is looking for something in a market or a library, imagine they have an AI assistant that aids them in their daily tasks. The user could talk to their assistant and say \"Hey, i'm going to go to the market to pick up some milk and bread\". The assistant should have the ability to maintain that information and understand the context in which way it should help. IE: When the assistant sees that the user is in the store, it should be able to recall the previous prompt or conversation when the user was talking about the items they required. For this a a few technologies would be required: Computer vision for classifying video feed. Its uncertain if this inference can be done on the edge device or if it must be done in the cloud. Most likely will be cloud, A Large Language Model . This is used for maintaining a conversational aspect to the assistant. LLM's such as ChatGPT have blown all other methods of producing chatbots we have ever seen. Big bets were placed on previous attempts such as Facebooks Chatbot developer SDK which allowed developers to create chatbots using Messenger. However, these chatbots where mostly rule based and did not provide a natural feeling experience. It really just slowed done being able to find the information you need that could have been easily solved by a simple google search. Therefore, using a generalized pretrained model is crucial for providing the feeling of authenticity within the conversations the user may have with it. Some alteratioins will be needed to this model. The details are unkown as of yet, but this LLM will need to have the ability to intake data from the output of the computer vision algorithm. This would require either two LLMs (one for conversations and another for interpreting what the CV algorithm is classifying in the moment), alternatively; a refactored model that takes two inputs: user input or classification input. There would also need to be a system that ties all of these together in real-time. Speech To Text will be needed to interpret what is being said by the user into data that the LLM can work with (tokens). Its currently unknown if this could be done on edge or if it would require transcription to be done in the cloud. Text To Speech is required to provide a vocal output to the user. The output response from the LLM system would be synthesized by some speech synthesis model. It would need to be omptimized for quick inference times to improve latency. Possible the audio could be generated on edge by sending instructions to the device. Its also possible that the audio could be transferred from the cloud, via VoIP Personality Module: To provide a unique experience and the illusion of the assistant having a personality would require some regulatory system running in the background. It should be able to have a configurable personality setting, where the user can select on which scale that what certain aspects of the assistants \"personality\" should be. This should be based on a valid personality traid model. There are many personality models out there, but the The Big Five Personality Traits model is generally considered the most widely accepted and used model for measuring personality. The Big Five model has gained popularity due to its ability to provide a comprehensive understanding of personality and its applicability in various fields, including psychology, organizational behavior, and marketing. Moreover, research has supported the validity and reliability of the Big Five model across cultures and languages. The configurations of the assistants persona should be interpretable by the LLM, and should influence the type of response the LLM produces. This system should be dynamic, meaning the parameters for the persona should be updating consistently throughout the span of conversations had with the assitant. The responses returned from the LLM should be analysis using sentiment analysis. Some system should be in place the use the output of the sentiment analysis to regulate the persona parameters. For example: If the user speaks with apathy towards the assistant then the responses wouldn't be as cheerful. Doing so would break the illusion of an authentic artificial personality. Edge Device: Ideally, to provide the best experience this should be wearable device such as glasses.","title":"Brainstorm of Feature Ideas"},{"location":"project_documentation/scoping/v0.0.1/#proposed-system-architecture","text":"User speaks to edge device. It is unknown as of yet whether or not speech to text happens on the edge device or on the cloud. Right now it's assumed that is doesn't. The request reaches an endpoint, to where its sends the speech audio to a Text-To-Speech model to be turne ...","title":"Proposed system Architecture"}]}
# Project Scoping v0.0.9

*17th April 2023*

### Summary of progress in LLM space so far

- ChatGPT released by OpenAI in 2022, with a free web interface or a python API
- **Fine-tuning:** In 2020, Google released the T5 model, which was state-of-the-art at the time. The showed that you can make the model perform very well on a specific using fine-tuning. They showcased this using the FLAN-5 model. It was later discovered that LLMs perform in-context learning. 
- **In-Context Learning (ICL)**: Rather than fine tuning a model, LLMs can take a small sample of your data and, within the context of your the data you provide, you it temporarily learns the data. The difference here between fine tuning and ICL is that you don't update the weights with ICL. 

- In 2023, Meta released the LLaMa model 
- Standford used what is known as the self-instruct method. Essentially using GPT4 to create data to train a smaller model (LLaMa 7B), as it was not feasible to use thousands of GPUs, which is only possible for the likes of Microsoft and Google. The called this model Alpaca. This is done only using hundreds of dollars.
- This spun off a frenzy of ChatGPT-like models: Vicuna, GPT4ALL, etc.
- Different ways of fine tuning a model:
    1. Parameter Efficient Fine Tuning (LORA): training 1% of the models weights. Is very cost efficient when you can't afford GPU infrastructure.
    2. Classifical Fine Tuning: where you actually update the weights based on your own data. Classical fine-tuning is used when you have the money for paying for a lot of GPUs. 
    3. Instruction fine tuning: Rather than using syntethic data with the self-instruct method, you can use complex proprietary data (such as in the case of being in a company). This can be used for finding "hidden patterns" in the data. Imagine you have thousands of documents
    


<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/qu-vXAFUpLE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>

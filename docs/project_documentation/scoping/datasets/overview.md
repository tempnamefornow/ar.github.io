# Data




### alpaca_data.json

- [Link](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)

`alpaca_data.json` contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields:

- `instruction`: str, describes the task the model should perform. Each of the 52K instructions is unique.
- `input`: str, optional context or input for the task. For example, when the instruction is "Summarize the following article", the input is the article. Around 40% of the examples have an input.
- `output`: str, the answer to the instruction as generated by text-davinci-003.

### laion/OIG

- [Link](https://huggingface.co/datasets/laion/OIG)

A large instruction dataset of medium quality along with a smaller high quality instruciton dataset (OIG-small-chip2). The data is in the form of jsonl objects, with at least a 'text' field. Some datasets may also include a 'metadata' field. The 'text' field contains a string of the form of one or more of:

```bash
<human>: instruction\n<bot>: response
<human>: instruction\n<bot>: response .. <human>: instruction\n<bot>: response
```

It is best to download the individual jsonl files directly that you wish to use instead of using HF load_datasets. https://huggingface.co/datasets/laion/OIG/tree/main



### pacovaldez/stackoverflow-questions

- [Link](https://huggingface.co/datasets/pacovaldez/stackoverflow-questions)

The dataset contains the title and body of stackoverflow questions and a label value(0,1,2,3) that was calculated using thresholds defined by SO badges.
Every software developer in the world has dealt with Stack Overflow (SO); the amount of shared knowledge there is incomparable to any other website. Questions in SO are usually annotated and curated by thousands of people, providing metadata about the quality of the question. This dataset aims to provide an accurate prioritization for programming questions.


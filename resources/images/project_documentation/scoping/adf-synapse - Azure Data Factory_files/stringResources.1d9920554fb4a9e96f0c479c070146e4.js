var shellStringResources={SynapseNotebook:"Notebook",SynapseNotebooks:"Synapse notebooks",SynapseScopeJobDefinition:"SCOPE job definition",SynapseScopeJobDefinitions:"Synapse SCOPE job definitions",SynapseSparkJobDefinition:"Spark job definition",SynapseSparkJobDefinitions:"Synapse Spark job definitions",SynapseSqlScript:"SQL script",SynapseSqlScripts:"Synapse SQL scripts",SynapseSqlPools:"SQL Analytics pools",SynapseSQLDatabases:"SQL Analytics databases",SynapseDatabases:"Lake databases",SynapseTables:"Synapse tables",SynapseRelationships:"Synapse relationships",SynapseSqlPool:"SQL Analytics pool",SynapseSparkPools:"Synapse Spark pools",SynapseKqlScripts:"Synapse KQL scripts",Activities:"Activities",Activity:"Activity",Concurrency:"Concurrency",CancelAfter:"Cancel After",ElapsedTimeMetric:"Elapsed time metric",Name:"Name",InvalidJSONValidationMessage:"{0} has invalid JSON syntax.",Activity_Error_Configuration_InvalidName:"Parameter name can't be empty",Activity_Error_Configuration_InvalidType:"Parameter type can't be empty",Activity_Error_Configuration_InvalidValue:"Parameter '{0}' value can't be empty",ParameterConfiguration_Error_DuplicateName:"Parameters with duplicate name will be overwritten.",NotebookActivityLabel:"Notebook",NotebookActivitySparkPoolLabel:"Spark pool",NotebookParametersLabel:"Notebook parameters",SparkJobDefinitionActivityMainDefinitionFile:"Main definition file",SparkJobDefinitionActivityMainClassName:"Main class name",SparkJobDefinitionActivityReferenceFiles:"Reference files",SparkJobDefinitionActivitySparkPoolLabel:"Apache Spark pool",SparkJobDefinitionActivityExecutorSizeLabel:"Executor size",SparkJobDefinitionActivityDynamicExecutorLabel:"Dynamically allocate executors",SparkJobDefinitionActivityMaxExecutorsLabel:"Max executors",SparkJobDefinitionActivityMinExecutorsLabel:"Min executors",SparkJobDefinitionActivityDriverSizeLabel:"Driver size",File:"File",ClassName:"Class name",Configuration:"Configuration",Configurations:"Configurations",Notes:"Notes",Arguments:"Arguments",ScopeCustomProperties:"SCOPE custom properties",ScopeReferenceAndResourceFiles:"SCOPE reference and resource files",ScopeJobDefinitionJobProperties:"SCOPE job properties",ScopeJobDefinitionPublishUnsupported:"Publishing SCOPE job definition is not supported yet. Please save it first.",ScopeJobDefinitionTargetScopePool:"Target SCOPE pool",ScopeScriptFilePath:"SCOPE script file path",ScopeScriptParameters:"SCOPE script parameters",RuntimeName:"Runtime name",Token:"Token",PrincipalId:"Principal Id",StandardToken:"Standard token",StandardTokenPercentage:"Standard token percentage",EcoToken:" ECO token",EcoTokenPercentage:"ECO token percentage",NotificationRecipients:"Notification recipients",Priority:"Priority",MaxUnavailability:"Max unavailability",NebulaArguments:"Nebula arguments",KqlScriptPublishUnsupported:"Publishing KQL script is not supported yet. Please save it first.",DatabasePublishUnsupported:"Publishing Azure Synapse Database is not supported yet. Please save it first.",SparkJobDefinitionTargetSparkCompute:"Target Spark pool",SparkJobDefinitionTargetSparkConfiguration:"Target Spark configuration",SparkJobDefinitionRequiredSparkVersion:"Required Spark version",SparkJobDefinitionJobProperties:"Spark job properties",SparkJobDefinitionLanguage:"Spark job definition language",SparkJobPropertiesJars:"Jars",SparkJobPropertiesPyFiles:"Python files",SparkJobPropertiesFiles:"Files",SparkJobPropertiesArchives:"Archives",SparkJobPropertiesDriverMemory:"Driver memory",SparkJobPropertiesDriverCores:"Driver cores",SparkJobPropertiesExecutorMemory:"Executor memory",SparkJobPropertiesExecutorCores:"Executor cores",SparkJobPropertiesNumExecutors:"Number of executors",LinkTable:"Link table",LinkTableNameValidationError:"A table name in a link connection mustn't conatin '[' or ']', and its length must be between 1 - 128.",LinkConnection:"Link connection",LinkTopicNameValidationError:"Link connection name may only contain lowercase letters, numbers, and hyphens, and must begin with a letter or a number. Each hyphen must be preceded and followed by a non-hyphen character. The name must also be between 3 and 63 characters long.",CDCConnection:"CDC Connection",SQLPoolRequired:"SQL pool is required",SQLAnalyticsPoolRequired:"Activity '{0}' SQL Analytics pool is required",SQLDatabaseRequired:"SQL database is required",IntegrationRuntime:"Integration runtime",IntegrationRuntimes:"Integration runtimes",ModelName_AvroDataset:"Avro",ModelName_DelimitedTextDataset:"DelimitedText",ModelName_JsonDataset:"Json",ModelName_ParquetDataset:"Parquet",ModelName_LinkedService:"Linked service",ModelName_LinkedServices:"Linked services",AzureSynapseAnalytics:"Azure Synapse Analytics",ModelName_Encoding:"Encoding",ModelName_CompressionCodec:"Compression codec",ModelName_CompressionLabel:"Compression level",StoredProcedureName:"Stored procedure name",StoredProcedureParameters:"Stored procedure parameters",StoredProcedure:"Stored procedure",ModelName_ActivityTemplate:"{0} activity",ModelName_ActivitiesTemplate:"{0} activities",ModelName_ColumnDelimiter:"Column delimiter",ModelName_Template:"Template",ModelName_Templates:"Templates",Dataset_FileFormat_RowDelimiterFieldLabel:"Row delimiter",ModelName_QuoteChar:"Quote char",Dataset_FileFormat_EscapeCharFieldLabel:"Escape character",ModelName_CharToEscapeQuoteEscaping:"Char to escape quote escaping",ModelName_FirstRowAsHeader:"First row as header",NullValueFieldLabel:"Null value",Dataset_FileFormat_ColumnDelimiterShouldBeNoneError:'Column delimiter must be "No delimiter" when row delimiter is "No delimiter"',Dataset_FileFormat_QuoteCharShouldBeNoneError:'Quote character should be "No quote character" when "No escape character" is set.',Dataset_enableVertiParquet:"Enable Verti-Parquet",Named_Keyword_ValidationError:"{0} cannot be used as a name",Named_StartsWithNumber_ValidationError:"{0} cannot start with a number",TimespanValidatorInvalidFormat:"{0} is invalid. The metric must be formatted as D.HH:MM:SS",TimespanValidatorBetween:"{0} cannot be '{1}'. It has to be between {2} and {3}.",TimespanValidatorMinimum:"{0} cannot be '{1}'. It must be a minimum value of {2}.",TimespanValidatorMaximum:"{0} cannot be '{1}'. It can have a maximum value of {2}.",Retry:"Retry",Timeout:"Timeout",ModelName_Policy:"policy",ModelName_Policies:"policies",SecureOutput:"Secure output",SecureInput:"Secure input",UserPropertiesLabel:"User properties",NullNamesInUserPropertiesNotAllowed:"Name is required in user properties",NullValuesInUserPropertiesNotAllowed:"Value is required in user properties",MultipleUserPropertiesWithSameName:"Only unique names in user properties allows. Multiple properties named {0} were found.",MaximumUserPropertiesExceeded:"The maximum number of User Properties allowed per activity is {0}, please remove extra properties.",LinkCriteriaValidationMessage:"Link from '{0}' to '{1}' activity has invalid criteria, remove and put link again.",Trigger:"Trigger",Triggers:"Triggers",Activated:"Activated",ScheduleLabel:"Schedule",ModelName_ScheduledTriggers:"Schedule triggers",StartTime:"Start time",EndTime:"End time",Frequency:"Frequency",ModelName_Interval:"Interval",Timezone:"Timezone",Trigger_ActivationErrorNoPipeline:"Trigger '{0}' cannot be activated and contain no pipelines",Trigger_PipelinesLengthError:"Trigger '{0}' can only contain one pipeline",Trigger_Dependency:"Trigger dependency",ModelName_TumblingWindowTrigger:"Tumbling window",ModelName_TumblingWindowTriggers:"Tumbling window triggers",ModelName_Delay:"Delay",Trigger_EndTimeAfterStart:"End time cannot be before start time",Trigger_InvalidFrequencyError:"Selected Frequency not supported for trigger type",Trigger_TumblingInvalidIntervalError:"Tumbling Window trigger with frequency 'Minutes' must have interval of at least 5.",Trigger_Interval_Hour:"For Frequency type Hour, interval has to be in the range 1 - 12000 inclusive",Trigger_Interval_Minute:"For Frequency type Minute, interval has to be in the range 1 - 720000 inclusive",ModelName_RerunTumblingWindowTrigger:"Rerun tumbling window",ModelName_RerunTumblingWindowTriggers:"Rerun tumbling window triggers",ModelName_ParentTrigger:"Parent trigger",ModelName_RequestedStartTime:"Requested start time",ModelName_RequestedEndTime:"Requested end time",ModelName_RerunConcurrency:"Rerun concurrency",Trigger_StartTimeInvalid:"Start time is invalid",Trigger_EndTimeInvalid:"End time is invalid",Trigger_CannotContainDuplicateRecurrenceScheduleOccurence:"Trigger '{0}' cannot contain duplicate monthly occurence types",Trigger_Interval_Month:"For Frequency type Month, interval has to be in the range 1 - 16 inclusive",Trigger_Interval_Week:"For Frequency type Week, interval has to be in the range 1 - 71 inclusive",Trigger_Interval_Day:"For Frequency type Day, interval has to be in the range 1 - 500 inclusive",ModelName_Extension:"Extension",ModelName_Extensions:"Extensions",ModelName_CmdKeyCommandExtension:"Cmdkey Command Extension",ModelName_CmdKeyCommandExtensions:"Cmdkey Command Extensions",ModelName_ComponentExtension:"Install 3rd Party Component Extension",ModelName_ComponentExtensions:"Install 3rd Party Component Extensions",ModelName_Content:"Content",ModelName_Description:"Description",ModelName_Parent:"Parent",ModelName_MsdtcConfigurationExtension:"Configure MSDTC Extension",ModelName_MsdtcConfigurationExtensions:"Configure MSDTC Extensions",ModelName_EnvVariableExtension:"Environment Variable Extension",ModelName_EnvVariableExtensions:"Environment Variable Extensions",ModelName_SqlServerAliasExtension:"SQL Server Alias Extension",ModelName_SqlServerAliasExtensions:"SQL Server Alias Extensions",ModelName_LicensedComponentExtension:"Install Licensed Component Extension",ModelName_LicensedComponentExtensions:"Install Licensed Component Extensions",ModelName_AzPowerShellExtension:"Install Azure PowerShell Extension",ModelName_AzPowerShellExtensions:"Install Azure PowerShell Extensions",ModelName_ExpressCustomSetupError:"Error of express custom setup",ModelName_ExpressCustomSetupErrors:"Errors of express custom setup",IRCreation_SSIS_CustomSetup_AzPowerShell_Format:"Azure PowerShell {0}",IRCreation_SSIS_CustomSetup_CmdKey_Format:"{0},{1}",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Format:"{0}={1}",IRCreation_SSIS_CustomSetup_Msdtc:"Configure MSDTC",IRCreation_SSIS_CustomSetup_MsdtcEnabled:"Enable MSDTC",IRCreation_SSIS_CustomSetup_SqlServerAlias_Format:"SqlServerAlias: {0}/{1}:{2}",IRCreation_SSIS_CustomSetup_Install:"Install licensed component",IRCreation_SSIS_CustomSetup_Install_ComponentName_Label:"Component name",IRCreation_SSIS_CustomSetup_Install_ComponentName_Description:"Select the name of component to install",IRCreation_SSIS_CustomSetup_Install_ComponentName_IntegrationToolkit:"KingswaySoft's SSIS Integration Toolkit",IRCreation_SSIS_CustomSetup_Install_ComponentName_ProductivityPack:"KingswaySoft's Productivity Pack",IRCreation_SSIS_CustomSetup_Install_ComponentName_TaskFactory:"SentryOne's Task Factory",IRCreation_SSIS_CustomSetup_Install_ComponentName_HeddaIO:"oh22's HEDDA.IO",IRCreation_SSIS_CustomSetup_Install_ComponentName_SQLPhoneticsNet:"oh22's SQLPhonetics.NET",IRCreation_SSIS_CustomSetup_Install_ComponentName_XtractIS:"Theobald Software's Xtract IS",IRCreation_SSIS_CustomSetup_Install_ComponentName_CDataStandard:"CData's SSIS Standard Package",IRCreation_SSIS_CustomSetup_Install_ComponentName_CDataExtended:"CData's SSIS Extended Package",IRCreation_SSIS_CustomSetup_Install_ComponentName_AsIntegrationService:"AecorSoft's Integration Service",IRCreation_SSIS_CustomSetup_Install_ComponentName_Unknown:"Unknown component",IRCreation_SSIS_CustomSetup_Install_ComponentName_Error_Duplicated:"The component '{0}' already exists.",IRCreation_SSIS_CustomSetup_Install_ComponentLicense_Label:"License key",IRCreation_SSIS_CustomSetup_Install_ComponentLicense_PlaceHolder:"Your component license key",IRCreation_SSIS_CustomSetup_Install_ComponentLicense_Description:"Enter the license key for additional component to be installed on your Azure-SSIS Integration Runtime",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_Label:"License file",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_PlaceHolder:"Drag & drop your license file here",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_Description:"Upload the license file for additional component to be installed on your Azure-SSIS Integration Runtime",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_Uploaded:"License file is uploaded, drag & drop a new one here",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_UploadMessage:"The license file {0} is uploaded successfully at {1}",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_ErrorMessage:"License file must be provided",IRCreation_SSIS_PackageStore_TYPE_AzureFileStorage:"Azure Files",IRCreation_SSIS_PackageStore_Type_Unsupported:"Unsupported Type",IR_ComputeNodes:"Compute nodes",IR_ParallelExecutionPerNode:"Parallel execution per node",UseSparkGateway:"Use spark gateway",DataFlowComputeSizeLabel:"Compute size",DataFlowComputeTypeLabel:"Compute type",DataFlowCoreCountLabel:"Core count",DataFlowCleanupLabel:"Quick re-use",DataFlowCustomPropertiesLabel:"Compute Custom Properties",ModelName_HdiOnDemand_TimeToLive:"Time to live",DataIntegrationUnitLabel:"Data integration unit",DataBricks_ExternalComputeInformation:"External compute information",FileSystem_DisplayText:"File system",ModelName_Host:"Host",FileSystem_HostStartWithHttp_ErrorMessage:"File server does not support host started with 'http:', please consider using http linked service.",FileSystem_HostStartWithFtp_ErrorMessage:"File server does not support host started with 'ftp:', please consider using ftp linked service.",Error:"Error",ProvisioningState:"Provisioning state",PrivateEndpointPrivateIP:"private endpoint private IP",PrivateEndpointResourceId:"private endpoint resourceId",PrivateLinkResourceId:"private link resourceId",PrivateLinkReserved:"Reserved",PrivateEndpointGroupID:"Group ID",PrivateEndpointConnectionState:"connection state",PrivateEndpointStatus:"status",RequiredValidatorMessage:"{0} {1} requires a {2}",RequiredValidatorMessageNoVowel:"{0} {1} {2} is required.",PatternValidatorMessage:"{0} {1} cannot be '{2}'. It should be in the form {3}.",PatternValidatorListMessage:"{0} {1} cannot be '{2}'.\n",AllowedValidatorMessage:"{0} {1} cannot be '{2}'. It should be one of the values in [{3}].",WholeNumberValidatorMessage:"{0} {1} cannot be '{2}'. It should be a whole number.",MinMaxValidatorMessageBetween:"{0} {1} cannot be '{2}'. It has to be between {3} and {4}.",MinMaxValidatorMessageMinimum:"{0} {1} cannot be '{2}'. It has to be a minimum value of {3}.",MinMaxValidatorMessageMaximum:"{0} {1} cannot be '{2}'. It can be a maximum value of {3}.",OccursValidatorMessageMinOne:"{0} '{1}' should have at least one {2}.",OccursValidatorMessageMinOneSwitch:"{0} '{1}' should have at least one {2} for each case.",OccursValidatorMessageMin:"{0} '{1}' should have at least {2} {3}.",OccursValidatorMessageMaxOne:"{0} '{1}' can have at most one {3}.",OccursValidatorMessageMax:"{0} '{1}' can have at most {2} {3}.",Pipelines:"Pipelines",Pipeline:"Pipeline",Pipeline_Policy:"Policy",Pipeline_Policies:"Policies",PipelineValidationMessage_Cyclic:"Pipeline '{0}' cannot have cycles. Activity '{0}' is in a cycle.",Parameterization_ValidIntegerErrorMessage:"Parameter value should be a valid integer",Parameterization_ValidFloatErrorMessage:"Parameter value should be a valid float",Parameterization_ValidBooleanErrorMessage:"Parameter value should be a valid boolean",Parameterization_ValidArrayErrorMessage:"Parameter value should be a valid array",Parameterization_SecureStringWarningInGit:"Parameters of type SecureString cannot have a default value when using Git mode, they must be fetched at runtime through Azure Key Vault or set when triggering the pipeline manually.",PowerBI:"Power BI",MaxNumberOfActivitiesError:"Pipeline '{0}' has more than the max number of 40 activities allowed per pipeline.",VariableLabel:"Variable",VariablesLabel:"Variables",Invalid_Variable_Array:'Variable value should be a valid array; e.g. ["1","2","3"]',Variable_NotValidType:"Variable does not have a valid type, please choose one.",ValidatorMessageNoneUniqueNestedNames:"Inner activities within the same pipeline cannot have the same name. Multiple activities named {0} were found within {1}.",Column:"Column",Columns:"Columns",AirflowEntity:"Airflow Entity",AirflowEntities:"Airflow Entities",Dataset:"Dataset",Datasets:"Datasets",Schema:"Schema",Table:"Table",UniqueValidatorMessage:"The name '{0}' is duplicated in {1} {2}.",ModelName_LinkedServiceTemplate:"{0} linked service",ModelName_LinkedServicesTemplate:"{0} linked services",AzureKeyVault_DisplayText:"Azure Key Vault",AzureStorage_DisplayText:"Azure Storage",AzureSql_DisplayText:"Azure SQL Database",AzureFileStorage_DisplayText:"Azure File Storage",ModelName_Snapshot:"Snapshot",ModelName_PackageStore:"Model of Package store",ModelName_PackageStores:"Model of Package stores",ModelName_PackageStoreName:"Package Store Name",ModelName_PackageStoreLinkedService:"Package Store Linked Service",FileShare_DisplayText:"File share",KeyStoreAuthType:"Key store authentication type",AlwaysEncrypted:"Always encrypted",ModelName_BaseUrl:"Base URL",ModelName_AKVLinedService:"AKV linked service",ModelName_SecretName:"Secret name",ModelName_SecretVersion:"Secret version",ModelName_StorageAccountName:"Storage account name",ModelName_StorageAccountKey:"Storage account key",ModelName_EndpointSuffix:"Endpoint suffix",ModelName_PrimaryEndpoint:"Partitioned DNS endpoint",enablePartitionedDns:"Partioned DNS enabled",PartitionedDnsTooltip:"This allows you to connect to your storage account with Azure DNS Zone endpoint type, which has a up to 5000 storage accounts limit per region per subscription.",ModelName_SasURI:"Storage SAS URI",ModelName_SasToken:"SAS token",ModelName_Tenant:"Tenant",ModelName_ServicePrincipalID:"Service principal ID",ModelName_ServicePrincipalKey:"Service principal key",ModelName_Url:"URL",ModelName_ServerName:"Server name",ModelName_DatabaseName:"Database name",ModelName_SqlPoolName:"SQL pool name",ModelName_UserName:"User name",ModelName_Password:"Password",ModelName_PowerBIWorkspaceID:"Power BI Workspace",ModelName_DatasetTemplate:"{0} dataset",ModelName_DatasetsTemplate:"{0} datasets",ModelName_AuthenticationType:"Authentication type",ModelName_AzureCloudType:"Azure cloud type",AzureBlobFS_DisplayText:"Azure Data Lake Storage Gen2",Lakehouse_DisplayText:"Lakehouse",WorkspaceId:"Workspace ID",ArtifactId:"Artifact ID",ModelName_ServicePrincipalCredentialType:"Service principal credential type",ModelName_ServicePrincipalCert:"Service principal certificate",ModelName_UamiResourceID:"User Assigned Managed Identity resource ID",Credential:"Credential",Credentials:"Credentials",ServicePrincipal:"ServicePrincipal",PipelineParameter:"Pipeline parameter",GlobalParameter:"Global parameter",GlobalParametersLabel:"Global parameters",DataFactory:"Data Factory",DataFactories:"Data Factories",FactoryGlobalParameterValueError:"Global Parameter with the name {0} is missing required property value.",ModelName_RetryPolicy:"Retry policy",ModelName_RetryCount:"Retry policy: count",ModelName_RetryInterval:"Retry policy: interval in seconds",Trigger_ParameterValueRequired:"Parameter '{0}' has no default value, must be specified",Trigger_Parameterization_ValidIntegerErrorMessage:"Parameter '{0}' should be a valid integer",Trigger_Parameterization_ValidFloatErrorMessage:"Parameter '{0}' should be a valid float",Trigger_Parameterization_ValidBooleanErrorMessage:"Parameter '{0}' should be a valid boolean",Trigger_Parameterization_ValidArrayErrorMessage:"Parameter '{0}' should be a valid array",ModelName_RecurrenceSchedule:"Recurrence schedule",ModelName_Minutes:"Minutes",ModelName_Hours:"Hours",ModelName_Weekdays:"Weekdays",ModelName_MonthDays:"Month days",ModelName_MonthlyOccurences:"Monthly occurences",ModelName_RecurrenceScheduleOccurence:"Recurrence schedule occurence",ModelName_Day:"Day",ModelName_Occurence:"Occurence",Trigger_Event:"Event",ModelName_EventTriggers:"Event triggers",ModelName_Scope:"Storage account",ModelName_BlobPathBeginsWith:"Blob path begins with",ModelName_BlobPathEndsWith:"Blob path ends with",Trigger_IgnoreEmptyBlobs:"Ignore empty blobs",ModelName_MaxConcurrency:"Max concurrency",Trigger_CustomEvents:"Custom events",ModelName_AdvancedFilters:"Advanced filters",ModelName_StorageEvents:"Storage events",ModelName_StorageEventTriggers:"Storage event triggers",ModelName_CustomEventsTrigger:"Custom event trigger",ModelName_CustomEventsTriggers:"Custom event triggers",ModelName_ScopeLabel:"Scope",ModelName_SubjectBeginsWith:"Subject begins with",ModelName_SubjectEndsWith:"Subject ends with",EventTriggerValidationMessage:"Blob events trigger requires a blob path input.",Trigger_EventInvalidScopeFormat:"Invalid scope format, the storage account name must be in lowercase and the scope must be written in the format: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/{storageaccountname}",ModelName_ChainingTrigger:"Chaining",ModelName_ChainingTriggers:"Chaining triggers",Trigger_Chaining_DependentPipelines:"Dependent pipelines",RunDimensionLabel:"Run Dimension",ModelName_ServicePrincipalCredentialProperties:"Service Principal Credential Properties",ModelName_UserAssignedManagedIdentityCredentialProperties:"User Assigned Managed Identity Credential Properties",ModelName_UserAuthCredentialProperties:"User Auth Credential Properties",AzureSQLManagedInstance_DisplayText:"Azure SQL Database Managed Instance",Error_InvalidAKVLSReferenced:"Cannot reference to an invalid or deleted Azure Key Vault Linked Service.",Error_InvalidCredentialReferenced:"Cannot reference to an invalid or deleted Credential object.",AzureEventHubs_DisplayText:"Event Hubs",ModelName_AzureEventHubsNamespace:"Event Hub namespace",ModelName_AzureEventHubsHubName:"Event Hub name",ModelName_AzureEventHubsPolicyName:"Event Hub policy name",ModelName_AzureEventHubsPolicyKey:"Event Hub policy key",SubscriptionId:"Subscription ID",ResourceGroup:"Resource group",CognitiveService_DisplayText:"Azure Cognitive Service",ModelName_CognitiveServiceName:"Azure Cognitive Service name",ModelName_CognitiveServiceKind:"Azure Cognitive Service kind",ModelName_CognitiveServiceLocation:"Azure Cognitive Service location",ModelName_CognitiveServiceEndpoint:"Azure Cognitive Service endpoint",ModelName_CognitiveServiceKey:"Azure Cognitive Service key",Validator_NamePattern_StartEnd_Alphanumeric:"{0} name should start and end with letter or number.",Validator_NamePattern_All_AlphanumericUnderscore:"{0} name should only contain letters, numbers or underscores (_).",Validator_NamePattern_StartEnd_AlphanumericUnderscore:"{0} name should start and end with letter, number or underscore (_).",Validator_NamePattern_Start_AlphanumericUnderscore:"{0} name should start with letter, number or underscore (_).",Validator_NamePattern_All_AlphanumericUnderscoreDashSpace:"{0} name should only contain letters, numbers, dashes (-), underscores (_), or spaces.",Validator_NameMaxLength:"{0} name has exceeded the limit of {1}.",Validator_NameMinLength:"{0} name should contain at least {1} characters.",Validator_NameRequired:"{0} name is required.",Validator_HourRange:"Hour value not in range 0-23",Validator_MinSecRange:"Minute or second value not in range 0-59",Validator_ExceedDayLimit:"Maximum amount of time cannot exceeds 7 days.",Validator_Database_Name_Blocked_Git:"master, model, tempdb, msdb, distributor are reserved and not allowed as database names",Validator_Column_NameRequired:"A column name is required in {0}(Table).",Validator_Column_TypeRequired:"A column data type is required in {0}(Table).",Validator_Column_AlreadyExist:"A column with the same name already exists in the {0}(Table).",Validator_Column_DecimalFormatError:"Precision and scale values should only contain whole numbers and commas (,) in the form 18,2 in {0}(Table).",Validator_Column_StringLengthError:"Length value is required in {0}(Table).",Validator_Column_MaxStringLengthError:"Maximum length value for datatype string is {0}. Choose a smaller length value",Validator_Relationship_ColumnNameRequired:"A column name is required in a relationship for {0}(Table).",Validator_Relationship_TableNameRequired:"A table name is required in a relationship for {0}(Table).",Validator_Relationship_AlreadyExist:"A relationship between tables {0} and {1} already exists on columns {2} and {3}. To create a relationship, choose a different table name or column name.",Validator_Relationship_DatatypeMisMatch:"Datatype mismatch between {0} in {1}(Table) and {2} in {3}(Table). To create a relationship, choose columns of the same datatype.",Validator_Table_NameNotUnique:"A table with the same name already exists in the database.",ManagedResources:"Managed resources",LinkTopicSourceTargetTablesUniqueValidationError:"{0} table name {1} is duplicated in link connection {2}",Source:"Source",Target:"Target",ApacheSparkConfiguration:"Apache Spark configuration",ApacheSparkConfigurations:"Apache Spark configurations",MappingArtifact:"Mapping Artifact",MappingArtifacts:"Mapping Artifacts",InvalidMappingType:"Invalid mapping type",InvalidSourceColumn:"Invalid source column",InvalidTargetColumn:"Invalid target column",InvalidMappings:"Invalid mappings",AzureSynapseAnalyticsArtifacts:"Azure Synapse Analytics (Artifacts)",Authentication:"Authentication",ModelName_SynapseWorkspaceEndpoint:"Azure Synapse Analytics workspace URL"},stringResources={lhErrorInDf:'Only Lakehouse with root folder "{0}" is supported for dataset type {1}.',appGateway:"Application gateway",convertDec2Int:"Convert decimal to integer",convertDec2IntDesc:"Define whether convert Number into Integer(short, int and long) data type or not.",npsQuestion:"How likely are you to recommend Azure Data Factory to others?",CreatedOn:"CreatedOn",AdfBanner:"Azure Data Factory logo",A365Banner:"Azure Synapse Analytics logo",ActiveUsers:"Active users",Vcores:"vCores",NumberOfNodes:"Number of nodes",ViewAllNApplicationsFormat:"View all {0} applications",AllocatedvCores:"Allocated vCores",AllocatedMemoryGb:"Allocated memory (GB)",FailedApplicationsLastMinutesFormat:"Failed applications (last {0} min)",NovCoresCurrentlyAllocated:"No vCores currently allocated",NoRunningApplications:"No running applications",ThereAreNoVCoresAllocatedAtThisTime:"There are no vCores allocated at this time.",ThereAreNoRunningApplicationsAtThisTime:"There are no running applications at this time.",TheChartCannotBeShownAtThisTime:"The chart cannot be shown at this time.",Other:"Other",PrivacyPolicy:"Privacy policy",AccessControl:"Access control",AddActivityOnLabel:"Add activity on:",AddAdmin:"Add admin",AddRoleAssignment:"Add role assignment",AddUser:"Add user",AddNewVariable:"Add new variable",GitSwitcherMessage:"Editing Git resources.",Advanced:"Advanced",AdvancedProperties:"Advanced properties",AdvancedFilters:"Advanced filters",AdvancedFiltersValidKeysTooltip:'Valid keys for currently selected event schema includes id, topic, subject, custom properties inside the data payload, using "." as the nesting separator. (e.g. data, data.key, data.key1.key2)',AdvancedFiltersTooltip:"Filter on attributes of each event. Only events that match all filters get delivered. Up to 25 filters can be specified. All string comparisons are case-insensitive.",AdvancedFilterMaxValues:"Exceeded limit of 25",AdvancedFilterMax:"Exceeded 25 filter limit",EnterKey:"Enter key",EnterNumericValue:"Enter a numeric value",EnterAValue:"Enter a value",Add:"Add",AddFilter:"Add filter",Adf:"Adf",Adding:"Adding",Action:"Action",Actions:"Actions",Activate:"Activate",Activating:"Activating",Activated:"Activated",Activities:"Activities",Activity:"Activity",ActivityId:"Activity ID",ActivityLog:"Activity Log",About_Template:"About this template",LogLevel:"Log level",Level:"Level",Log:"Log",LogFiles:"Log files",About:"About",Please:"Please",BabylonAccount:"Purview account",BrowseBabylonAssets:"Browse assets (Purview)",ConnectToBabylon:"Connect to a Purview account",BabylonAccountName:"Purview account name",BabylonResourceId:"Purview account resource ID",EnterBabylonResourceId:"Enter Purview resource ID",AddOutput:"Add output",AdvancedSettings:"Advanced settings",And:"and",Additional:"Additional",Application:"Application",ApplyToAllSelected:"Apply to all selected",Argument:"Argument",AverageLabel:"Average",MicrosoftAzure:"Microsoft Azure",DataFactory:"Data Factory",DataFactoryProperties:"Data Factory properties",DataFactories:"Data Factories",Factory:"Factory",FactoryLowerCase:"factory",Factories:"factories",For:"For",Workspace:"Workspace",WorkspaceLowerCase:"workspace",WorkspacesLowerCase:"workspaces",WorkspaceItem:"Workspace item",Appfigure_EntityType_Products:"Products",Appfigure_EntityType_Sales:"Sales",Appfigure_EntityType_Ads:"Ads",Entity:"entity",Synapse:"Synapse",SynapseLive:"Synapse live",Empty:"Empty",property:"property",SynapseAnalytics:"Synapse Analytics",All:"All",AllCategories:"All categories",AllStatus:"All status",AutoGenerate:"Auto generate",Monitoring_JobDetail_GetJobGraphInfoFailed:"Fetch Spark Job Graph failed.",Monitoring_Interactive:"Interactive",Monitoring_SparkApplication_Spark_Session:"Spark session",Monitoring_SparkApplication_Batch_Job:"Batch job",State:"State",SearchSelect:"Select a filter",Monitoring_MemoryUtilization:"Memory utilization",Compute:"Compute",ComputeTime:"Compute time",Au:"AU",Aus:"AUs",ConsumedAUHours:"Consumed AU-hours\u200b",Efficiency:"Efficiency",pool:"Pool",Pools:"Pools",RequestType:"Request type",RequestId:"Request ID",WorkloadGroup:"Workload group",Importance:"Importance",Verified:"Verified",NotVerified:" Not verified",Unsupported:"Unsupported",ConfigNotFound:"Configuration not found",CannotConnect:"Cannot connect",Connected:"Connected",Disconnected:"Disconnected",Classifier:"Classifier",Import:"Import",Importing:"Importing",Impact:"Impact",SampleDataset:"Sample dataset",SampleDataset_PluralLabel:"Sample datasets",SampleDataset_Invalid_Configuration:"Invalid sample dataset. Please review the pipeline configuration or browse to update it.",SampleDataset_Browse_Description:"Start building with Azure Open Datasets. Save time on data discovery and preparation.",SampleDataset_Icon_Description:"Sample dataset icon",SampleDataset_Select_Header:"Select a dataset",Preview10Rows_Desc:"The data in the preview only shows the first 10 rows.",Monitoring_Timeline_Radio_Label:"radio button {0} of {1}",Monitoring_JobType:"Job type",Monitoring_SparkAppId:"Spark application ID",Monitoring_SparkJobLivyId:"Livy ID",Monitoring_SparkJob_Timing:"Timing",Monitoring_SparkApplication_Batch:"Batch",Monitoring_JobQueuedDuration:"Queued duration",Monitoring_JobRunningDuration:"Running duration",Monitoring_JobTotalDuration:"Total duration",Monitoring_SparkJobSpecNameLabel:"Spec name",Monitoring_SparkJobStageLabel:"Job stage",Monitoring_JobIdLabel:"Job ID",Monitoring_SparkJobResultLabel:"Job result",Monitoring_JobNameLabel:"Job name",Monitoring_SparkJobDefintion:"Spark Job Defintion",Monitoring_SubmitterName:"Submitter name",Monitoring_Tokens:"Tokens",Monitoring_DeleteSparkOnCosmosCluster:"Delete spark cluster",Monitoring_DeleteSparkOnCosmosClusterConfirmationContent:"Are you sure you want to delete the spark cluster {0} created by {1}?\n\nAny applications on it will be terminated and this operation is irreversible.",Monitoring_FailedToStartJob:'Failed to start job "{0}".',Monitoring_FailedToStopJob:'Failed to stop job "{0}".',Monitoring_FailedToDeleteJob:'Failed to delete job "{0}".',Monitoring_OpenInNewTab:"Open in new tab",Monitoring_OpenInNewTabs:"Open in new tabs",Monitoring_OpenInNewTabsTooltip:"Open in new tabs. To open more than 1 tab at once, please allow popup windows in the browser settings when prompted.",Monitoring_OpenInNewWindow:"Open in new window",Monitoring_OpenManyTabs:"Are you sure you want to open {0} tabs?",Monitoring_OpenManyTabsContent:"If you would like to open {0} tabs, press continue.",Monitoring_ExportCurrentPage:"Export current page",Monitoring_ExportTopThousand:"Export top 1000",ExportTopThousandInProgress:"Exporting top 1000 activity runs...",ExportTopThousandFinished:"Successfully exported activity runs",ExportDescription:"This may take a few moments.",ExportDownloadDescription:"Please check your browser's downloads",NoUserPropertiesFound:"No user properties found",NullNamesInUserPropertiesNotAllowed:"Name is required in user properties",NullValuesInUserPropertiesNotAllowed:"Value is required in user properties",here:"here",AutoRefreshLabel:'Auto refresh every 20 seconds for 5 minutes. Click "Refresh" manually for runs taking more than 5 minutes.',DataFlowAutoRefreshDescription:"Auto refresh every 60 seconds until run is complete.",AutoRefresh:"Auto refresh",cancel:"cancel",Cancel:"Cancel",Canceling:"Canceling",CancelTheJob:"Cancel the job",CancelScopeJobSucceeded:"Succeeded to cancel the job",CancelScopeJobFailed:"Failed to cancel the job",CancelCopyData:"Cancel copy data?",CancelCopyDataText:"If you cancel now, you will lose all progress and will need to start from the beginning. Are you sure you want to cancel?",yield:"yield",Yield:"Yield",Yielding:"Yielding",YieldTheJob:"Yield the job",YieldScopeJobSucceeded:"Succeeded to yield the job",YieldScopeJobFailed:"Failed to yield the job",Resubmit:"Resubmit",Pause:"Pause",Paused:"Paused",DescriptionLink:"Click to view description",GetDatabricksAccessTokenLink:"Click for docs to generate access token",GatewayTimeOutError:"The cloud service request timed out. Please retry.",DatabricksWorkspaceURLDescription:"Workspace URL can be found in Azure portal under workspace overview.",DatabricksWorkspaceURLPlaceholder:"Databricks workspace URL",DatabricksRegionFromWorkspaceDescription:"This value is auto-populated based on your Databricks workspace.",DatabricksWorkspaceIdDescription:"The resource ID of the Databricks workspace. This can be found in the properties of the Databricks workspace, and it should be of the format: /subscriptions/{subscriptionID}/resourceGroups/{resourceGroup}/providers/Microsoft.Databricks/workspaces/{workspaceName}",DiscardAndContinue:"Discard and continue",CapacityUsed:"Capacity used",Clear:"Clear",ClearFilters:"Clear filters",CancelSession:"Cancel session",Cancelled:"Cancelled",Cancelling:"Cancelling",CancelFailed:"Failed to cancel pipeline.",CancelSessionTitle:"Cancel session?",CancelSessionContent:"The SQL session associated with this request will be cancelled. Any future requests in this session will be rejected. Would you like to continue and cancel the session? ",CancelOptions:"Cancel options",CancelRecursive:"Cancel recursive",CancelOptionsTooltip:"Cancel the current pipeline run. Please check back to ensure proper cancellation.",CancelTooltip:"Cancel the pipeline run",CancelTriggerRun:'Cancel the trigger run "{0}"?',FilterTooltip:"Filter items",ClearFilterTooltip:"Clear and dismiss filters",CancelRecursiveTooltip:"Cancel the pipeline run including child runs",ClearAll:"Clear all",Clone:"Clone",Close:"Close",Confirm:"Confirm",ConfirmChanges:"Confirm changes",CloseSidenav:"Close sidenav",Collapse:"Collapse",CollapseAll:"Collapse all",ColumnMapping:"Mapping",Comments:"Comments (optional)",ComingSoon:"Coming soon",Completion:"Completion",Concurrency:"Concurrency",Config:"Configuration",Target:"Target",Column:"Column",Operator:"Operator",CustomColumnsLabel:"{0} Columns",RulesForTransform:"Rules to Transform",Core:"Core(s)",JobURL:"Job URL",JobResourcesPath:"Job resources path",CopyJobURL:"Copy job URL",CopyJobUrl:"Copy job URL to clipboard",CopySummary:"Copy Summary",Sinks:"Sinks",DataStreams:"All streams",Date:"Date",Derived:"Derived",Deactivate:"Deactivate",Default:"Default",DefaultLowercased:"default",Delete:"Delete",DeleteData:"Delete data",DeleteFilter:"Delete filter",DeleteTag:"Delete tag",Deleting:"Deleting",Description:"Description",Detach:"Detach",Detaching:"Detaching",DetachTrigger:"Detach Trigger",TermsofUse:"Terms of use",DataFlows:"Data flows",DataFlow:"Data flow",DataFlowSingleWord:"Dataflow",DataFlowRefresh:"PBI Dataflow",DataFlowsSingleWord:"Dataflows",OverrideCustomizedCheckpointKey_Example:"e.g <pipeline-name>-<activity-name>",OverrideCheckpointKey:"Override",DuplicateCheckpointKey_Error:"A {0} with the same value already exists in the factory.",CustomizedCheckpointKey:"Checkpoint Key",CustomizedCheckpointKey_Description:"Checkpoint key used for this activity's cdc workflow",CustomizedCheckpointKey_NoSpaces:"Custom checkpoint key should not contain any spaces",DataMappings:"Data mappings",DataMapping:"Data mapping",DataMappingName:"Data mapping name",ChangeDataCaptureName:"CDC name",Flowlet:"Flowlet",Flowlets:"Flowlets",PowerQuery:"Power Query",WranglingCanvas_NewPowerQueryNameLabel:"{0} name",DataFlowCompatible:"Data flow compatible",PipelineRunDetails:"Pipeline run details",Details:"Details",RunBy:"Run by",InternalDiagnostics:"Internal Diagnostics",DiagnosticCode:"Diagnostic Code",LineNumber:"Line Number",StartOffset:"Start Offset",EndOffset:"End Offset",Documentation:"Documentation",ViewDocumentation:"View documentation",DropdownFilterInput:"Dropdown filter input",EditCopy:"Edit a copy",EditInMonaco:"Edit in code editor",ExportQuery:"Export query",ExportQueryPlan:"Export query plan",End:"End",Ended:"Ended",Submitted:"Submitted",ActiveApplications:"Active applications",EndedApplications:"Ended applications",Every:"Every",Folder:"Folder",AddFolder:"New folder",AddSubFolder:"New subfolder",CreateFolder:"Create a folder",CreateSubFolder:"Create a subfolder",Rename:"Rename",RenameFolder:"Rename folder",DeleteFolder:"Delete folder",DeleteFolderConfirmationMessage:"Deleting the folder will delete all the resources underneath it. Consider moving the resources to other folders if you don't want to delete them.\n\n Are you sure you want to delete {0} and its content?",DeleteFolderConfirmationMessageA365:"Deleting the folder will delete all the resources within it. Do you want to continue?",NewFolderPlaceholder:"Enter folder name...",NewWorkspacePlaceholder:"Enter workspace name",FolderNameCantBeEmpty:"Folder name cannot be empty.",FolderNameExists:"A folder with the same name already exists",FolderNameHint:"Hit enter to set the folder name",FolderTip_ItemAlreadyHere:"The item is already in this location.",FolderTip_SameFolder:"The destination folder is the same as the source folder.",FolderTip_TargetIsSubFolder:"The destination folder is a subfolder of the source folder.",FolderTip_ChildNameConflict:"The destination folder already contains a folder with the same name as the source folder.",ResourceName:"Resource name",ResourceNameCannotBeEmpty:"Resource name cannot be empty.",ResourceType:"Resource type",ResourceNameHint:"Hit enter to set the resource name",ResourceNameExists:"Resource name already exists",InvalidFolderName:"Folder names cannot have '/' character",Move:"Move",MoveItem:"Move item",MoveItems:"Move items",MoveNItems:"Move {0} selected items",MoveHere:"Move here",ChooseADestination:"Choose a destination",AllPipelines:"All pipelines",AllDatasets:"All datasets",AllDataFlows:"All data flows",Connections:"Connections",LinkedService:"Linked service",LinkedServices:"Linked services",SourceTypes:"Source type(s)",SourceLinkedServices:"Source linked service(s)",TargetTypes:"Target type(s)",TargetLinkedServices:"Target linked service(s)",SchemaLinkedService:"Schema linked service",ExternalConnections:"External connections",LinkedServicesDefaultOption:"All linked services",Triggers:"Triggers",Tag:"Tag",Dark:"Dark (Preview)",TriggersDefaultOption:"All triggers",Total:"Total",IntegrationRuntime:"Integration runtime",IntegrationRuntimes:"Integration runtimes",Email:"Email (optional)",MailLabel:"Mail",EmailLabel:"Email",Edit:"Edit",EditAll:"Edit all",Error:"Error",Errors:"Errors",ErrorCode:"Error code",ErrorMessage:"Error message",Abort:"Abort",FailDataFlow:"Fail data flow",Export:"Export",Expand:"Expand",ExpandAll:"Expand all",Expression:"Expression",ValueExpression:"Value expression",ExpressionElements:"Expression elements",ExpressionValues:"Expression values",ExpressionTypes:"Expression types",HierarchyLevel:"Hierarchy level",HierarchyLevelExpression:"Hierarchy level expression",HierarchyLevelDescription:"Choose the hierarchy level below which subcolumns are matched.",HierarchyExpandTooltip:"Add hierarchy level expression",ToggleHierarchy:"Toggle hierarchy view",Incompatible:"Incompatible",Key:"Key",Keys:"Keys",KeyType:"Key type",ValueType:"Value type",KeyTypePlaceholder:"mapKeyType",ValueTypePlaceholder:"mapValuetype",From:"From",To:"To",Tags:"Tags",ExpressionTypeOnlyPropertyPlaceHolder:"This property should be parameterized.",Failure:"Failure",FeedbackLabel:"Feedback",Notifications:"Notifications",File:"File",FileSet:"File set",Wildcard:"Wildcard",FormatType:"Format type",OverviewTooltip:"Data Factory overview",AuthorTooltip:"Author",InformationTooltip:"Help / information",Help:"Help",BackButtonText:"Back",Completed:"Completed",OnCompletion:"On completion",GlobalParameter:"Global parameter",NewGlobalParameter:"New global parameter",EditGlobalParameters:"Edit global parameters",Parameter:"Parameter",MapParameter:"Map parameter",PrameterName:"Parameter name",ParameterValue:"Parameter value",ParameterizationTemplate:"Parameterization template",Prefix:"Prefix",PipelineSettings:"Pipeline settings (preview)",DefaultDatasetFilePath:"File path in dataset",Parameters:"Parameters",LogOut:"Log out",LogOutTooltip:"Sign out",ViewAccount:"View account",SwitchAccount:"Login with a different account",SigninWithDiffAccount:"Sign in with a different account",ViewDiagnosticsInfo:"View diagnostics",SwitchDataFactory:"Switch Data Factory",SwitchWorkspace:"Switch workspace",SwitchDataFactoryTitle:"Data Factory + subscription",SwitchWorkspaceTitle:"Workspace + subscription",Throttled:"Throttled",LogOut_Confirm_Title:"Are you sure you want to log out?",Integrate:"Integrate",Integration:"Integration",AnalyticsPools:"Analytics pools",SQLPools:"SQL pools",DxPools:"Dx pools",KustoPoolsHeader:"Kusto pools",VCPUs:"vCPUs",InstanceCount:"Instance count",CacheInGB:"Cache (GB)",RAMInGB:"RAM (GB)",QueryResults:"Query results",QueryName:"Query name",IngestionResults:"Ingestion results",KustoPoolCPU:"CPU (last {0} min)",KustoPoolCacheUtilization:"Cache utilization (last {0} min)",KustoPoolIngestionUtilization:"Ingestion utilization (last {0} min)",SQLPool:"SQL pool",SQLDatabase:"SQL database",SQLDatabasev2:"SQL database v2",SQLDatabasev3:"SQL database v3 (preview)",SQLPoolDetailsLabel:"SQL pool details",DWULimit:"DWU limit",DataProcessedLastMinsFormat:"Data processed (last {0} min)",DataProcessedUnits:"Cumulative data processed ({0})",SQLPoolDetailsViewAllSqlRequests:"Showing {0} of {1} running requests",SQLPoolDetailRequestIDToolTipContext:'Requests in "Runnable" state do not have Request ID',SQLRequestsDetailsNoRequestSteps:"There are no steps available for this SQL request.",RunningRequests:"Running requests",QueuedRequests:"Queued requests",FailedRequestsLastMinsFormat:"Failed requests (last {0} min)",DashSymbol:"-",CurrentAllocation:"Current allocation",RunningApplications:"Running applications",AllocationDetails:"Allocation details",History:"History",ApacheSparkPool:"Apache Spark pool",ApacheSparkPools:"Apache Spark pools",LastPosition:"Last position",Computes:"Computes",CreationDate:"Creation date",ComputeName:"Compute name",ScopeJobLabel:"SCOPE job",ScopeJobsLabel:"SCOPE jobs",ScopePool:"SCOPE pool",ScopePoolsLabel:"SCOPE pools",ScopePool_Field_MaxQueuedJobs:"Max queued jobs",ScopePool_Field_MaxRunningJobs:"Max running jobs",ScopePool_Field_MaxTokensPerJob:"Max standard capacity per job (AU)\u200b",ScopePool_Field_AllocatedStandardCapacity:"Allocated standard capacity (AU)\u200b",ScopePool_Field_MaxEcoTokensPerJob:"Max ECO capacity per job (AU)\u200b",ScopePool_Field_AllocatedEcoCapacity:"Allocated ECO capacity (AU)\u200b",ScopePool_Detail_ProcessingUtilizationTrend:"Processing utilization trend",ScopePool_Detail_ProcessingEfficiencyTrend:"Processing efficiency trend",ScopePool_Detail_JobQueueTimeVariance:"Job queue time variance",ScopePool_Detail_JobRunningTimeVariance:"Job running time variance",ScopePool_Detail_JobQueueTime:"Job queue time",ScopePool_Detail_JobRunningTime:"Job running time",ScopePool_Detail_JobCount:"Number of jobs",ScopePool_Detail_QueueTimeX:"Average job queue time (minutes)",ScopePool_Detail_RunningTimeX:"Average job running time (minutes)",ScopePool_Detail_SucceededPnHoursX:"PN hours of successful jobs",ScopePool_Detail_FailedPnHoursX:"PN hours of failed/cancelled jobs",ScopePool_Detail_UtilizedTokenX:"Utilized tokens",ScopePool_Detail_AllocatedTokenX:"Allocated tokens",SparkApplicationsLabel:"Apache Spark applications",SparkClustersAndScopeJobs:"Spark Clusters & SCOPE Jobs",SQLRequestsLabel:"SQL requests",SparkPoolsLabel:"Spark pools",SparkPoolLabel:"Spark pool",KQLRequestsLabel:"KQL requests",SQLPoolRequired:"SQL pool is required",SQLDatabaseRequired:"SQL database is required",Context_Label:"ODP context",ODPName_Label:"ODP name",ShufflePartitions:"Shuffle partitions",UseProlepticGregorianCalendar:"Use proleptic gregorian calendar",Account:"Account",Account_SID:"Account SID",Auth_Token:"Auth token",Messages:"Messages",Calls:"Calls",Ingestion:"Ingestion",Analyze:"Analyze",Any:"Any",V2:" (v2)",V3:" (v3)",Monitor:"Monitor",Manage:"Manage",Management:"Management",Dashboard:"Dashboards",Name:"Name",ID:"ID",ApplicationName:"Application name",SQLReqId:"SQL request ID",Submitter:"Submitter",Scope:"Scope",None:"None",Position:"Position",AllResources:"All resources",Small:"Small",Medium:"Medium",Large:"Large",XLarge:"XLarge",XXLarge:"XXLarge",Resources:"Resources",Resource:"Resource",Retry:"Retry",Repair:"Repair",Repairing:"Repairing",Role:"Role",Time:"Time",LastUpdate:"Last update ({0})",Timeout:"Timeout",TimedOut:"Timed out",TimePickerExpanded:"Time picker expanded",TimePickerCollapsed:"Time picker collapsed",TreatAsNull:"Treat as null",RetryIntervalInSeconds:"Retry interval (sec)",SecureOutput:"Secure output",SecureInput:"Secure input",SecureOutputMessage:"Your output has been secured.",SelectResourceGroup:"Select resource group",NewResourceGroup:"New resource group",CreateNew:"Create new",Create:"Create",Created:"Created",CreateNewParameter:"Create new parameter",CreateNewLocal:"Create new local",EditParameter:"Edit parameter",EditLocal:"Edit local",UseExisting:"Use existing",SelectRegion:"Select region",NotificationTooltip:"Show notifications",Value:"Value",Like:"Like",Between:"Between",Not_Like:"Not like",Not_Between:"Not between",Maximize:"Maximize",Toggle:"Toggle",Restore:"Restore",Reset:"Reset",Recursive:"Recursive",Pending:"Pending",Options:"Options",Transactional:"Transactional",Analytical:"Analytical",Overview:"Overview",Home:"Home",Order:"Order",OK:"OK",CloseAndDiscard:"Close + discard changes",SasUri:"SAS URI",Save:"Save",SaveAndFinish:"Save and finish",Saved:"Saved",Saving:"Saving...",SaveTooltip:"Save the current resource without validation to your git repository",SavingNoDot:"Saving",Services:"Services",Skipped:"Skipped",OnSkip:"On skip",Status:"Status",LineageStatus:"Lineage status",Progress:"Progress",Start:"Start",NotStarted:"Not started",NotSpecified:"Not specified",Cancel_Start:"Cancel start",Retry_Start:"Retry start",Retry_Stop:"Retry stop",Submit:"Submit",Asserts:"Asserts",AssertType:"Assert type",AssertId:"Assert Id",AssertDescription:"Assert description",ExpectTrueExpression:"Expect true expression",ExpectUniqueExpressions:"Expect unique expressions",ExpectExistsLeftExpression:"Expect exists left expression",ExpectExistsRightExpression:"Expect exists right expression",ExpectTrue:"Expect true",ExpectUnique:"Expect unique",ExpectExists:"Expect exists",ExpectCompare:"Expect compare",SubmittedSuccessfully:"Submitted successfully.",RegistrationInProgressTitle:"Connected and adding role assignment",RegistrationInProgressSubTitle:"Connected the {0} to Purview account {1}. Granting Data Curator role to the {2}'s managed identity.",DisconnectionInProgressTitle:"Disconnected and revoking role assignment",DisconnectionInProgressSubTitle:"Disconnected Purview account {0} from the {1}. Revoking Data Curator role from {2}'s managed identity.",RegistrationSucceededTitle:"Successfully connected",RegistrationSucceededMessage:"Successfully connected the {0} to Purview account {1}.",DisconnectSucceededTitle:"Successfully disconnected",DisconnectSucceededMessage:"Successfully disconnected Purview account {0} from the {1}.",ConnectedWithWarningTitle:"Connected with warning",ConnectedWithWarningSubTitle:"Connected the {0} with Purview account {1}. Failed to grant Data Curator role to {2}'s managed identity.",DisconnectedWithWarningTitle:"Disconnected with warning",DisconnectedWithWarningSubTitle:"Disconnected Purview account {0} from the {1}. Failed to revoke Data Curator role from {2}'s managed identity.",PurviewProtectedByFirewall_Error:"Cannot access the Purview account from your current network as the account is behind the firewall.",GoToPurviewAndGrant_Info:"Go to your Purview account and grant Data Curator role to {0}'s managed identity.",GoToPurviewAndRevoke_Info:"Go to your Purview account and revoke Data Curator role from {0}'s managed identity.",Contact_Purview_AdminToDoSo:"Contact Purview admin to do so if you don't have permission.",DatasourceProvisionTitle:"Registering data source",DatasourceProvisionSubtitle:"Registering workspace data source in Purview account {0}.",DatasourceProvisionSucceededTitle:"Successfully registered data source",DatasourceProvisionSucceededMessage:"Successfully registered data source in Purview account {0}.",DatasourceProvisionFailedTitle:"Data source registering failed",DatasourceProvisionFailedMessage:"Failed to register data source in in Purview account {0}.",WorkspaceDataStore:"Workspace data store",WorkspaceDataStoreType:"Workspace data store type",NoRoleAssignmentPermissionToCheckBabylon:"You don't have permission to check role assignments on the Purview account.",NoRoleAssignmentPermissionToUpdateBabylon:"You don't have permission to update role assignment on the Purview account.",Contact_Purview_AdminToGrant:"Contact Purview admin to grant Data Curator role to {0}'s managed identity.",Contact_Purview_AdminToRevoke:"Contact Purview admin to revoke Data Curator role from {0}'s managed identity.",Done:"Done",Degraded:"Degraded",Stop:"Stop",stop:"stop",Stopping:"Stopping",Resuming:"Resuming",Pausing:"Pausing",Search:"Search",SearchLowerCase:"search",Search_ADF:"Factory search",Search_Synapse:"Workspace search",Search_FoundItemsForString:"Found {0} items with filter {1}.",PurviewSearch:"Purview search",Exact_Match:"Exact match",Searching:"Searching...",DataFactoryName:"Data Factory name",PurviewLabel:"Microsoft Purview",ContainerUri:"URI to storage account containing linked ARM templates",ContainerSasToken:"Sas token to storage account containing linked ARM templates",IRName:"Integration runtime name",IRUpdateNotice:"The changes to the integration runtime will be published right away.",Update:"Update",Overwrite:"Overwrite",Updates:"Updates",Unknown:"Unknown",Send:"Send",Next:"Next",Previous:"Previous",GoTo:"Go to",Type:"Type",Attempts:"Attempts",AllAttempts:"All Attempts",SubType:"Sub-type",SelectFrom:"Select from...",SelectType:"Select type",Statistics:"Statistics",TypeSettings:"Type settings",Typecast:"Typecast",ColumnTypeSettings:"Type settings for column:",Shared:"Shared",Linked:"Linked",Link_Lower:"link",DefaultValue:"Default value",Recommendations:"Recommendations",SecureStringParameter:"Secure string for '{0}' of '{1}'",ArmTemplateGenerationNotificationTitle:"Generating ARM templates",TemplateGenerationNotificationTitle:"Generating templates",ArmTemplateGenerationNotificationSubTitle:"Generating ARM templates and saving them in Git",TemplateGenerationNotificationSubTitle:"Generating templates and saving them in Git",ARMTemplateGenerationFailureMessage:"Failed to generate ARM template. One or more resources are bad, fix them and retry the operation again.",TemplateGenerationFailureMessage:"Failed to generate template. One or more resources are bad, fix them and retry the operation again.",TemplateCreationFailureA365Desc:"Failed to create a template. One or more of the resources are bad. Please fix them and try again.",TemplateCreationFailureADFDesc:"ADF: Failed to create an ARM template. One or more of the resources are bad. Please fix them and try again.\u200b",ARMTemplateResourceCountExceededLinked:"Cannot proceed because the resource {0} exceeds the limit of {1} resources in a linked ARM template",ARMTemplateResourceCountExceededPublished:"Cannot proceed because the resource {0} exceeds the limit of {1} resources in an ARM template",ARMTemplateCharCountExceeded:"Cannot proceed because the resource {0} exceeds the limit of {1} characters in an ARM template",ARMTemplateSizeWarningTitle:"Exceeded size limit",ARMTemplateSizeWarningMessage:"The single ARM template exceeds the 20 MB limit, so linked templates were published instead. If CI/CD is required, use linked templates.",ARMTemplateSizeWarningMessageA365:"The size limit has been exceeded. The maximum size of 20 MB for Synapse template has been reached.",ARMTemplate:"ARM template",A365Template:"Synapse template",ARMTemplateParameterizedTemplateDescription:"This configuration determines which properties are parameterized when generating the Azure Resource Manager template of this data factory.",ARMTemplateParamterizedTemplateShortDescription:"Determine which properties are parameterized when generating the ARM Template of this Data Factory.",ARMTemplateConfigDescription:"The following options will allow for more advanced ARM template configuration of this data factory",ARMTemplateParameterizedTemplateCommit:"Update arm-template-parameters-definition.json",ARMTemplateParameterizedTemplateUnavailableGitEnabled:"The configuration can only be edited through git mode, so switch to git mode for editing ARM parameters.",ARMTemplateParameterizedTemplateUnavailableGitDisabled:"The ARM template parameter configuration is only available for git enabled data factories. Please enable git to enable this.",ARMManagementJSONDesc:"The Azure Resource Manager (ARM) template is a JavaScript Object Notation (JSON) file that defines the infrastructure and configuration for your project.",ARMManagementDesc:"Azure Data Factory can be exported and updated as an ARM template artifact.",ARMParameterConfiguration:"ARM parameter configuration",ExportArmTemplateDisabled:"Waiting for resources to load or ongoing export operation to finish.",ImportArmTemplate:"Import ARM template",ImportArmTemplateDesc:"Update your Azure Data Factory environment by importing an ARM template.",ImportArmTemplateCta:"Import on Azure portal",ExportArmTemplate:"Export ARM template",ExportArmTemplateDesc:"Export your Azure Data Factory as an ARM template.",ExportArmTemplateStartNotification:"Started loading resources and validation.",ExportArmTemplateValidationSucceededNotification:"Validation of model(s) completed. Generating the ARM templates.",ExportArmTemplateValidationFailedNotification:'Validation of model(s) failed. Check the violation(s) using "Validate all" action, fix it before exporting.',ExportArmTemplateSuccessNotification:"ARM template downloaded.",ExportArmTemplateFailureNotification:"Failed. Error: {0}",EditParameterConfiguration:"Edit parameter configuration",ImportTemplate:"Import template",ExportTemplate:"Export template",DownloadInProgress:"Download in progress",DownloadLog:"Download {0} log",DownloadLogFailed:"Download log failed",DownloadSuccessful:"Download log successful",DownloadSupportFiles:"Download support files",ImportFromSupportFiles:"Import resources from support files",ImportFromSupportFilesError:"Failed to import resources from support files",DownloadSqlScript:"Download SQL script",EditControlTable:"Edit control table",ImportTemplateError:"Import template error, please check the zip file.",UseTemplateError:"Invalid template, please check the template file.",ExportTemplateError:"Failed to prepare a zip for the gallery template",DownloadSupportFilesError:"Failed to download support files",DownloadSupportFilesWarning:"Support files may contain sensitive information. Please review the files before sending to external parties.",ImportTemplateErrorDetail:"Failed to get template from a zip file, Error: {0}",ExportTemplateErrorDetail:"Failed to prepare a zip for the gallery template, {0}. Error: {1}",DownloadSupportFilesErrorDetail:"Failed to download support files for {0}. Please ensure that the resource and all dependent resources are valid. Error: {1}",UseLocalTemplate:"Use local template",ImportTemplateInfo:"Unavailable. To import a local template to the gallery please switch to Git mode.",CannotParseZipToTemplate:"Cannot parse the uploaded zip file to a template, please checkout the content of the zip file.",PublishFailures:"Detailed publish errors",PublishFailuresDescription:"Error while publishing: {0}",PublishingMultipleErrorsMsg:"Publishing failed with multiple errors. Please review the details for more information",PublishingResourceLockedErrorMsg:"Unable to publish because the resource group or the {0} contains a lock. Please remove or edit the lock.",CrossPublishErrorMsg:"Publish failed due to invalid references. View details for more information",DuplicatePropertiesErrorMsg:"Publish failed due to resources with duplicate properties. View details for more information",CrossPublishSidenavErrorMsg:'Invalid references or dependencies found. This is likely due to publishing outside of Git mode or editing and deleting linked services in other branches. To recover from this state, please refer to the errors below and our <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2169248">Git troubleshooting guide</a>',DuplicatePropertiesSidenavErrorMsg:"Found resources with duplicate properties in the payload. Please verify that the following resources do not contain duplicate properties in their JSON. Property names are case insensitive.",InvalidReferenceErrorMsgFix:"Please ensure '{0}' exists in data factory mode and recreate it in Git mode if already present.",DuplicatePropertiesErrorMsgFix:"Using the code editor, please ensure '{0}' does not contain duplicate properties.",ReferencedByErrorMsgFix:"Unable to delete '{0}' because it is being referenced by '{1}'. Please ensure '{1}' is not referencing '{0}' in Git and data factory mode.",InvalidResourcePrincipalIdErrorMsg:'The {0} does not contain a managed identity, this could be caused by moving resources across tenants. Please refer to <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/data-factory-service-identity#generate-managed-identity">Generate managed identity</a> to create one.',IncrementalPublishNotificationMsg:"Deployed {0} of {1} resources",PublishingChangesNotificationMsg:"Deploying changes to the {0}",SavingChangesWorkspaceNotificationMsg:"Saving changes to the workspace",UnexpectedErrorWhilePublishingLogsLink:'Unexpected error has occured. Some resources may have been published successfully. Please review the <a target="_blank" href="{0}">Activity log</a> of your Data Factory in the Azure portal.',UnableToGetPublishOperationStatus:"Unable to get publish operation status. The publish operation may still be in progress, please track it manually or try to publish again later.",NotAuthorizedToGetPublishOperationStatus:'Cannot fetch publish status due to invalid authorization. The resources may or may not have been published correctly. Your permissions need the \'Microsoft.Resources/deployments/operationStatuses/readMicrosoft.Resources/deployments/operationStatuses/read\' action to perform this operation.\n Please refer to <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/concepts-roles-permissions#set-up-permissions">set up permissions</a> to learn more.',NotAuthorizedToDeployInRg:'You are not authorized to perform deployments in your resource group. Please verify that you have the \'Microsoft.Resources/deployments/write\' permission at the resource group level. Learn more <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/concepts-roles-permissions#roles-and-requirements">here</a>',NotAuthorizedToSubscribeToEvents:'You are not authorized to create storage events subscriptions using Event Grid. Please verify that you have the \'Microsoft.EventGrid/EventSubscriptions/Write\' permission. Learn more about event trigger <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/how-to-create-event-trigger">here</a> and the required permissions in Data Factory <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/concepts-roles-permissions#set-up-permissions">here</a>',PublishOutOfSyncResourceWarningText:"The following items have been modified by others while you are editing:\n{0}\n\nIf publishing continues, they will be overwritten by your change. Would you like to continue?",HttpTransportFailureErrorMsg:"HTTP transport fails. Please check your browser connectivity to Internet.",Validate:"Validate",ValidateTooltip:"Validate the current resource",ValidateAll:"Validate all",ValidateAllTooltip:"Validate all resources",ValidatingText:"Validating...",Run:"Run",Debug:"Debug",DataFlowDebug:"Data flow debug",DebugUntil:"Debug until",DeployToDebugEnv:"Deploying pipelines to debug environment.",Code:"Code",CodeTooltip:"View the JSON code representation of this resource",CodeLibraries:"Code libraries",HideGraph:"Hide graph",ShowGraph:"Show graph",Rerun:"Rerun",RerunEntireDataPipeline:"Rerun entire data pipeline",RerunTooltip:"Rerun the pipeline with the same configuration. The new run will be linked to previous runs",RerunFromActivity:"Rerun from selected activity",RerunFromActivityDescription:"Select an activity from below pipeline diagram to rerun from activity. \n Note: Set Variable, Append Variable, and Wait Activity will always be ran. Container activities such as Foreach and Switch will be ran but inner activities will be skipped.",RerunFromFailedActivity:"Rerun from failed activity",RerunFromFailedActivityDescription:"Rerun from failed, cancelled or timeout activities",RerunRange:"Rerun range",StartRunningDesc:"Start running {0}\xa0({1}).",RunningDesc:"Successfully running {0}\xa0({1}).",RunningPipelineFailedDesc:"Failed to run {0} ({1}).",RerunningTriggerFailedDesc:"Failed to rerun {0} ({1}).",RerunningTriggerDesc:"Successfully rerunning {0} ({1}).",RerunningTriggerSucceededDesc:"Successfully rerun {0} ({1}).",Running:"Running",FailedToRun:"Failed to run",FailedToRerun:"Failed to rerun",Runnable:"Runnable",Suspended:"Suspended",Waiting:"Waiting",WaitingOnDependency:"Waiting on dependency",CancelRunFailed:"Failed to cancel the run",PipelineAlreadyFailedRunDesc:"Failed to cancel {0} ({1}) run because the pipeline already ran it",PipelineCancelRunFailedDesc:"Failed to cancel the {0} ({1}) run",PipelineFailureDesc:"An error occured when running {0} ({1}).",PipelineSubmittingDesc:"Pipeline request is submitting for {0} ({1}).",PipelineInProgressDesc:"Pipeline request is in progress for {0} ({1}).",PipelineSucceededDesc:"Successfully ran {0} ({1}).",PipelineSkippedDesc:"Skipped the pipeline activity for {0} ({1}).",PipelineNotStartedDesc:"Pipeline run has not been started for {0} ({1}).",PipelineCancelledDesc:"Cancelled the pipeline request for {0} ({1}).",PipelineCancellingDesc:"Cancelling the pipeline request for {0} ({1}).",PipelineQueuedDesc:"Queued the pipeline request for {0} ({1}).",PipelineTimedOutDesc:"Pipeline request has timed out for {0} ({1}).",PipelineYieldedDesc:"The pipeline request yielded for {0} ({1}).",PipelineScheduledDesc:"Pipeline request scheduled for {0} ({1}).",PipelineStoppedDesc:"Stopped the pipeline request for {0} ({1}).",PipelineStoppedSessionDesc:"Stopped the pipeline request due to timeout for {0} ({1}).",PipelineRunningDesc:"Running {0} ({1}).",PipelineWaitingOnDependency:"Pipeline run is waiting on another activity to complete before starting for {0} ({1}).",TriggerCancellingDesc:"Cancelling request for {0} ({1}).",TriggeredCancelledDesc:"Cancelled request for {0} ({1}).",DependencyWindow:"Dependency window",DependentRuns:"Dependent runs",DependencyStatus:"Dependency status",DependencyStartTime:"Dependency start time",DependencyEndTime:"Dependency end time",DependentTriggers:"Dependent triggers",TriggeredPipelineRun:"Triggered pipeline run",TriggeredPipelinesLabel:"Triggered",DependenciesLabel:"{0} dependencies",UpstreamRunDependencies:"Upstream run dependencies",ExpectedRun:"Expected run",SingleDependencyLabel:"1 dependency",Preparing:"Preparing",Selection:"Selection",Queued:"Queued",Scheduled:"Scheduled",Remove:"Remove",Unlink:"Unlink",Modify:"Modify",Removing:"Removing",Enable:"Enable",Enabling:"Enabling",Disabling:"Disabling",Enable_Failed:"Failed to enable",Disable_Failed:"Failed to disable",Enabled:"Enabled",Disabled:"Disabled",Enable_Confirm_Title:"Enable?",Disable_Confirm_Title:"Disable?",Enter:"Enter",Disable:"Disable",DisableActivity:"Disable activity",MarkActivityAs:"Mark activity as",Browse:"Browse",Root:"Root",BrowseFromRoot:"From root",BrowseFromCurrentPath:"From specified path",BrowseSap:"Browse SAP cubes",BrowseTables:"Browse tables",Region:"Region",Subscription:"Subscription",Summary:"Summary",ClassicVirtualNetWork:"Classic Virtual Network",ArmVirtualNetwork:"Azure Resource Manager Virtual Network",Choose:"Choose",ChooseFile:"Select a file or folder.",Theme:"Theme",ChooseFileOnly:"Choose a file",ChooseFolderOnly:"Choose a folder",ChooseDatasetId:"Choose a dataset ID",SelectAll:"Select all",SelectAllRows:"Select all rows",SelectRow:"Select row",SelectUser:"Select user",MultiSelect:"Multi select",RangeSelect:"Range select",Categories:"Categories",Cut:"Cut",Copy:"Copy",CopyLink:"Copy link",CopyAllErrors:"Copy all errors",CopySelectedError:"Copy selected error",Duplicate:"Duplicate",DownloadDiagnostics:"Download diagnostics",CopyDescription:"Click to copy",Paste:"Paste",Shortcut:"Shortcut",SelectAllShortcut:"Ctrl+A",CutShortcut:"Ctrl+X",CopyShortcut:"Ctrl+C",PasteShortcut:"Ctrl+V",Finish:"Finish",Failed:"Failed",OnFail:"On fail",Loading:"Loading...",LoadingFailed:"Loading failed",LoadingKey:"Loading key...",LoadingError:"Loading error",LoadingResourceID:"Loading resource ID...",LoadingDataPreview:"Loading data preview ...",FailedToLoad:"Failed to load",FailedToLoadResource:"Failed to load {0} ({1})",FailedToFindConnection:"Failed to find connection info '{0}' for {1} table: '{2}'",FailedToFindConnectionTable:"Failed to find {0} table '{1}' for connection '{2}' info",FailedToLoadEmptySourceSchema:"No columns exist in the source schema. Please try again with a schema-defined source.",NoConfigurationForSchemaImport:"Error: no import confguration provided while importing schema",FailedToFindDatasetType:"Failed to find dataset handler for dataset type: '{0}'",InvalidDatasetType:"Invalid dataset type: '{0}' for source: '{1}'",NoSourceDataset:"No source dataset",NoSourceDatasetSuggestion:"Add a source dataset below to start editing your Power Query",FailedToLoadSourceSuggestion:"Please ensure that all source datasets have a valid schema",FailedToLoadSourceParameters:"Please define parameters for all source datasets",AndMore:"and {0} more",FailedToLoadDescription:"Failed to load role assignments due to {0}, error code {1}.",FailedToLoadDescriptionWithoutStatus:"Failed to load role assignments due to {0}.",FailedToLoadResourceDescription:"Failed to load one or more resources due to {0}, error code {1}.",FailedToLoadResourceDescriptionWithoutStatus:"Failed to load one or more resources due to {0}.",NetworkErrorDescription:"Request failed due to network issues.",RefreshOrTroubleshoot:"Refresh or troubleshoot the issue.",RefreshOrTroubleshootConnectivity:"Please refresh or troubleshoot the connectivity issue.",Stack:"Stack",ServerError:"server error",NoResponse:"no response",ForbiddenIssue:"forbidden issue",UnknownErrorInLowercase:"unknown error",NoAccessInLowercase:"no access",Required:"Required",ViewActivityRuns:"View activity runs",ViewPipelineRunsHistory:"View rerun history",ViewIntegrationRuntime:"View integration runtime detail",ViewIntegrationRuntimeActivity:"View activity runs on the integration runtime",ViewActivityRunsFromIR:"View activity runs in this pipeline run",ViewLinkConnectionDetail:"View link connection detail",ViewDetails:"View details",ViewRoleAssignment:"View role assignment",ViewRoleAssignments:"View role assignments",ViewSourceCode:"View source code",ViewScriptFlow:"View Scope Flow",ViewMore:"View more",ViewLess:"View less",ScopeFlow:"Scope Flow",Dismiss:"Dismiss",DismissAll:"Dismiss all",UpgradeAvailable:"Upgrade available",TestConnection:"Test connection",VNetValidation:"VNet validation",ValidatingLabel:"Validating",PreviewingLabel:"Previewing",Success:"Success",OnSuccess:"On success",Succeeded:"Succeeded",Forbidden:"Forbidden",Recurrence:"Recurrence",Submitting:"Submitting",Yielded:"Yielded",Timezone:"Time zone",EndTimeRange:"End time range",StartTimeRange:"Start time range",SubmitTimeRange:"Submit time range",TimeRange:"Time range",CreatedTimeRange:"Created time range",LastUpdatedTimeRange:"Last updated time range",LastUpdated:"Last updated",LastReplicated:"Last replicated",LastReplicatedTimeRange:"Last replicated time range",Trigger:"Trigger",Offset:"Offset",ZeroTime_Placeholder:"0.00:00:00",WindowSize:"Window size",TriggerNow:"Trigger now",TriggerNowTooltip:"Trigger on-demand run of the last published pipeline",TriggerTooltip:"Add a new trigger on a schedule or trigger this pipeline now",DebugTooltip:"Trigger test runs of the current pipeline without publishing your changes to the service",TriggerSavingHintFormat:'Make sure to "{0}" for trigger to be activated after clicking "{1}"',TriggerNowErrorFormat:'Sync and publish your changes before clicking "{0}".',TestConnectionSuccessMessage:"Connection successful",TestConnectionFailedMessage:"Connection failed",TestConnectionAKVFailedMessageInGitMode:"This is caused by inconsistency between the Azure Key Vault linked service in your git repo and the one in your {0}. Please make sure they are in sync.",TestConnectionAKVFailedMessageInLiveMode:"This is caused by inconsistency between the modified Azure Key Vault linked service and the one in your {0}. Please make sure the modified Azure Key Vault linked service is published.",MakeSureTestConnectionSucceedMessage:"Please make sure the data store can be successfully connected before navigating to the next page.",TestConnectionEditButtonText:"Edit connection",TestingConnectionText:"Testing connection...",Creating:"Creating...",Updating:"Updating...",UpdatingWithoutEclipse:"Updating",AddNew:"+ New",New:"New",More:"More",Continue:"Continue",Emulator:"Emulator",OpenScript:"Open script",Connect:"Connect",NA:"N/A",NotSupport:"Not support",NotSupportPreview:'Preview data is not supported with the current "File settings".',Publishing:"Publishing",PublishingError:"Publishing error",PublishingCancelled:"Publishing cancelled",PublishingCompleted:"Publishing completed",SavingCancelled:"Saving cancelled",SavingCompleted:"Saving completed",SavingError:"Saving error",PublishingPending:"Publishing pending",SearchToFilterItems:"Search to filter items...",Select:"Select...",Serverless:"Serverless",LatestVersion:"Latest version",DedicatedSqlPool:"Dedicated SQL pool",DedicatedSqlPoolv3:"Dedicated SQL poolv3 (Preview)",VersionFormat:"Version {0}",v3Preview:"v3 preview",SQL_on_demand:"SQL on-demand",SQL_built_in:"Built-in",QueryTextNotAvailable:"*** Query text not available ***",Selected:"selected",NonSelected:"not selected",TableType:"Table type",TableTypeParameterName:"Table type parameter name",NoAccount:"No accounts found",NoCluster:"No clusters found",NoDMR:"No data management runtime found",NoConnect:"No connect found",NoEventHubNamespaces:"No namespaces",NoEventHubs:"No event hubs",EventHub:"Event Hub",AddEventHub:"Add EventHubs",ImportFileTypeJson:"json",ImportFileTypeAvro:"avro",ImportFileTypePassthrough:"passthrough",EventHubNamePattern:"Event hub name pattern",EventHubSender:"Event hub sender*",NoWorkspace:"No workspace",NoServer:"No server",NoSqlPool:"No SQL pool",NoResource:"No resource",CannotGetResourceKey:"Cannot get resource key",TagsAddPlaceholder:"Add tag...",Server:"Server",Database:"Database",Database_URL:"Database URL",Settings:"Settings",NoSecret:"No secret",NoSecretVersion:"No secret version",NoDatabase:"No database",NoFileShares:"No file share",NoDataFactory:"No Data Factory",NoRegion:"No available region",NoResourceGroup:"No resource group",NoResultsFound:"No results found",NoResultsFoundFor:"No results found for",NoUamiFound:"No User Assigned Managed Identities found",UamisAssociatedWithDF:"Assigned to the {0}",UamisNotAssociatedWithDF:"Not assigned to the {0}",UamiNavigateHyperLinkText:"Manage User Assigned Managed Identity in Azure portal",ResultsFor:"Results for",Bubble_Of:" of ",SinkType:"Sink type",Inline:"Inline",InputFolderPlaceHolderForBlob:"Container/Folder path",InputBucketPlaceHolder:"Bucket",WildcardPathPlaceHolderForBlob:"Sample: container/*.csv",WildcardPathPlaceHolder:"Sample: /**/*.csv",WildcardPathPlaceHolderCsv:"Samples: *.csv or **/*.csv",WildcardPathPlaceHolderParquet:"Sample: /*.parquet",WildcardCheckLabel:"Use wildcard",WildcardBlobContainerError:"Wildcard is not allowed for blob container, /Container/*.csv is allowed",WildcardFileNameAdditionalInstruction:"Please note that this will overwrite the file name defined in dataset",WildcardFolderPathAdditionalInstruction:"Please note that this will overwrite the folder path defined in dataset",InputPrefixKeyPlaceHolder:"Prefix or key",BrowserWithoutLinkedServiceError:"Please select a linked service to browse storage.",PreviewScriptWithoutLinkedServiceError:"Please select a linked service to preview script.",PreviewScriptWithoutSelectionError:"Please select a script to preview.",ScriptFlowWithoutLinkedServiceError:"Please select a linked service to view Script Flow.",ScriptFlowWithoutScriptError:"Please select a script to view Script Flow.",ImportParametersWithoutLinkedServiceError:"Please select a linked service to import parameters.",ImportParametersWithoutSelectionError:"Please select a script to import parameters.",ImportParametersWithoutStoredProcedureError:"Please select a Stored Procedure to import parameters",ImportParametersWithoutSqlPoolError:"Please select a sql pool to import parameters.",ImportSchemasWithoutDatasetError:"Please select a dataset to import schemas",ImportSchemasWithSourceParameterizedColumnError:"Imported source dataset schema is parameterized.",ImportSchemasWithSinkParameterizedColumnError:"Imported sink dataset schema is parameterized.",ImportEmptySchemaError:"Imported schemas are empty",ImportEmptySourceSchemaError:"Imported source schema is empty",ImportEmptySinkSchemaError:"Imported sink schema is empty",ImportSchemaError:"Failed to import schema for {0} table '{1}' with error: ",SourceTargetSchemaImportFailure:"Failed to import schemas",SchemaImportFailure:"{0} schema import failed",StoredProcedureName:"Stored procedure name",Preview:"Preview",PreviewWith:"From files with '{0}'",PreviewSource:"Preview source",DataPreview:"Data preview",Schema:"Schema",ViewSchema:"View schema",Locals:"Locals",Local:"Local",ClearSchema:"Clear schema",LocalVariable:"Local variable",GridEmptyMessage:"No records found",Auto:"Auto",AzureCloudType_Auto:"{0}'s cloud type",AzureCloudType_Description:"For service principal authentication, specify the type of Azure cloud environment to which your AAD application is registered. By default, the {0}'s cloud environment is used.",AzureCloudType_Description_1:"For service principal authentication, specify the type of Azure cloud environment to which your AAD application is registered.",Logging_Mode:"Logging mode",Reliable:"Reliable",Best_effort:"Best effort",AzureSynapseWorkspace:"Azure Synapse workspace",SynapseWorkspace:"Synapse workspace",BabylonDesc:"Connecting {0} to Microsoft Purview will enable you to discover trusted and accurate data across your hybrid environment.",BabylonConnectDesc:"Connecting the {1} to {0} will help you discover, understand, explore and share your organization's data.",BabylonSubscriptionDescription:"This Microsoft Purview account is configured with private access. Make sure to connect your VNet to this Microsoft Purview instance via a private endpoint.",BabylonSubscriptionDescriptionManual:"If this Microsoft Purview account is configured with private access, make sure to connect your VNet to this Microsoft Purview instance via a private endpoint.",DeleteBabylonAccount:"Purview account connection will be deleted immediately.",DeleteAccountQuesetion:"Are you sure you want to delete the connection: {0}",BabylonLineageWarning:"Unable to push lineage to Purview because Purview Data Curator role is not granted to {0}'s managed identity.",PurviewLineageMoreDetails:"Learn more about Microsoft Purview integration",Purview_Integrations:"Integrations",Purview_IncludesByDefault:"Default Purview integration capabilities:",Purview_DataCatalog_Ability:"Discover enterprise data assets using Microsoft Purview powered search",Purview_CheckAdditional:"Additional Purview integration capabilities and status: ",Purview_DataLineageSpark:"Data Lineage - Synapse Spark",Purview_DataLineageSynapsePipeline:"Data Lineage - Synapse Pipeline",Purview_DataLineageADFPipeline:"Data Lineage - Pipeline",Discover_More:"Discover more",Browse_Partners_preview:"Browse partners (preview)",Browse_Partners:"Browse partners",Pipeline_Templates:"Pipeline templates",SAP_Pipeline_Templates:"SAP pipeline templates",SAP_Center:"SAP Knowledge Center",SAP_Template_Banner_Desc:"Azure Data Factory has a comprehensive knowledge center for SAP integration.",VisitPartnerWebsite:"Visit partner website",VisitPartnerSolution:"Visit partner solution",ConnectToPartner:"Connect to partner",LandingZoneSASTokenExpireTime:"Landing zone SAS token expire time",LandingZoneSASTokenExpireError:"Expired! Please rotate it.",LandingZoneSASTokenExpireWarning:"Will expire in {0} days.",LoggingLevelDescription:"Info level will log all the copied files, skipped files and skipped rows. Warning level will log skipped files and skipped rows only.",LoggingModeDescription:"Reliable mode will flush logs immediately once data is copied to the destination. Best effort mode will flush logs with batch of records.",Write_Method_Label:"Write method",Fixed:"Fixed",Percentage:"Percentage",CustomRange:"Custom range",Metadata:"Metadata",Metadata_Description:"Set custom metadata when copy to sink.",Metadata_Invalid_Value:"Customized value of custom metadata cannot start with '$$'",Metadata_LastModified_Desc:"A reserved variable indicates to store the source files' last modified time. Apply to file-based source with binary format only.",Metadata_Not_Allow_EmptyName:"The metadata name cannot be empty.",Metadata_Not_Allow_EmptyValue:"The metadata value cannot be empty.",Metadata_Not_Allow_DuplicateName:"The metadata name cannot be duplicated.",Partition_Name_Not_Allow_EmptyName:"The partition column name cannot be empty.",Partition_Name_Not_Allow_DuplicateName:"The partition column name cannot be duplicated.",Partition_Names_Not_Allow_All_Selected:"The partition column name cannot be all selected.",Partition_Not_Destination_File_Configured:"File name is not allowed when partition is enabled.",Partition_Column_Changed_Need_ReSelected:'Available partition columns may have changed. You can go to "Destination" tab to update them.',Partition_Column_GoToEdit:"Go to edit",Metadata_IR_Validation:"To specify metadata, the version of self-hosted IR must be newer than {0}",Apply:"Apply",Applying:"Applying",Filter:"Filter",ToggleFilterRow:"Toggle filter row",FilterRow:"Filter row",Yes:"Yes",YesCancel:"Yes, cancel",No:"No",Beginning:"Beginning",Now:"Now",Possible:"Possible",NoThanks:"No, thanks",NoKeepAsIs:"No, keep as is",Warning:"Warning",Available:"Available",Expired:"Expired",Info:"Info",SelectAnotherDF:"Select another Data Factory",CurrentDF:"Current Data Factory",CurrentWorkspace:"Current workspace",NameLabel:"Name: ",RegionLabel:"Region: {0}",ResourceGroupLabel:"Resource group: {0}",SubscriptionLabel:"Subscription: ",SubscriptionId:"Subscription ID",TenantNameLabel:"Azure Active Directory: ",SubscriptionIdInvalid:"Subscription ID is invalid.",SessionId:"Session ID",SessionProperties:"Session properties",LoggingKey:"Logging key",SubmitTime:"Submit time",LocalTime:"Local time",TransformationTotalColumns:"{0} total",ShowDetailedErrorMessage:"Show detailed error message",CopyedToClipboard:"Copied to clipboard",Upgrade:"Upgrade",Upgrading:"Upgrading",Public:"Public",VNet:"VNet",RemoveAll:"Remove all",Step1:"1.",Step2:"2.",Option1:"Option 1:",Option2:"Option 2:",Or:"or",Inherited:"Inherited",Sensitive:"Sensitive",ModeSwitcherDropdown:"Mode switcher",StartTimeEditableDropdown:"Start time editable",Regenerate:"Regenerate",Regenerating:"Regenerating",HowToGrantPermission:"How to grant permission",MultiselectSelectedItemsLabel:"{0} selected",StartedBy:"Started by",Minutes:"{0} minutes",OneHour:"1 hour",Hours:"{0} hours",DIU:"{0} DIU",FileUploadError:"Unable to process uploaded file",NoSort:"no sort applied",AscSort:"sort ascending",DescSort:"sort descending",Unauthorized:"Unauthorized",InternalError:"Internal error",TemporarilyUnavailable:"Temporarily unavailable",Unreachable:"Unreachable",Weekday_Sun:"Sun",Weekday_Mon:"Mon",Weekday_Tue:"Tue",Weekday_Wed:"Wed",Weekday_Thu:"Thu",Weekday_Fri:"Fri",Weekday_Sat:"Sat",Weekday_Sunday:"Sunday",Weekday_Monday:"Monday",Weekday_Tuesday:"Tuesday",Weekday_Wednesday:"Wednesday",Weekday_Thursday:"Thursday",Weekday_Friday:"Friday",Weekday_Saturday:"Saturday",Week:"Week",First:"First",Second:"Second",Third:"Third",Fourth:"Fourth",Fifth:"Fifth",Last:"Last",Unit_perS:"/s",Unit_Bytes:"bytes",Unit_Byte:"byte",Unit_KB:"KB",Unit_MB:"MB",Unit_GB:"GB",Unit_TB:"TB",Main_MenuTutorialText:"Guided tour",Main_MenuQuestionText:"Ask questions",KeyboardShortcuts:"Keyboard shortcuts",PrivacyAndCookies:"Privacy and cookies",PrivacyStatement:"Privacy statement",DiagnosticInfo:"Diagnostic info",OnlineTerms:"Online service terms",Refresh:"Refresh",SwitchLanguage:"Changing settings",Refreshing:"Refreshing...",True:"True",False:"False",Download:"Download",DownloadLogs:"Download logs",ButtonMenu:"Button menu",FilterLogs:"Filter errors and warnings",Parameterize:"Parameterize",GettingStartedText:"Getting started",KnowledgeCenterText:"Knowledge center",QuickStart:"Quick Start",Region_AutoResolve:"Auto Resolve",Region_AustraliaEast:"Australia East",Region_AustraliaSoutheast:"Australia Southeast",Region_BrazilSouth:"Brazil South",Region_CanadaCentral:"Canada Central",Region_CanadaEast:"Canada East",Region_CentralIndia:"Central India",Region_CentralUS:"Central US",Region_CentralUSEUAP:"Central US EUAP",Region_EastAsia:"East Asia",Region_EastUS:"East US",Region_EastUS2:"East US 2",Region_EastUS2EUAP:"East US 2 EUAP",Region_EastUS2_No_Space:"East US2",Region_FranceCentral:"France Central",Region_GermanyWestCentral:"Germany West Central",Region_JapanEast:"Japan East",Region_JapanWest:"Japan West",Region_JioIndiaWest:"Jio India West",Region_QatarCentral:"Qatar Central",Region_KoreaCentral:"Korea Central",Region_NorthCentralUS:"North Central US",Region_NorthEurope:"North Europe",Region_SouthCentralUS:"South Central US",Region_SouthIndia:"South India",Region_SoutheastAsia:"Southeast Asia",Region_UKSouth:"UK South",Region_UKWest:"UK West",Region_WestCentralUS:"West Central US",Region_WestEurope:"West Europe",Region_WestIndia:"West India",Region_WestUS:"West US",Region_WestUS2:"West US 2",Region_WestUS3:"West US 3",Region_SouthAfricaNorth:"South Africa North",Region_SouthAfricaWest:"South Africa West",Region_SwitzerlandNorth:"Switzerland North",Region_SwitzerlandWest:"Switzerland West",Region_NorwayEast:"Norway East",Region_UAENorth:"UAE North",Region_USGovVirginia:"USGov Virginia",Region_USGovArizona:"USGov Arizona",Region_USGovIowa:"USGov Iowa",Region_USGovTexas:"USGov Texas",Region_USDoDCentral:"USDoD Central",Region_USDoDEast:"USDoD East",Region_ChinaEast:"China East",Region_ChinaEast2:"China East 2",Region_ChinaNorth:"China North",Region_ChinaNorth2:"China North 2",Region_ChinaNorth3:"China North 3",Region_UsNatEast:"UsNat East",Region_UsNatWest:"UsNat West",Region_UsSecEast:"UsSec East",Region_UsSecWest:"UsSec West",Node_Size_Standard_A4_V2:"Standard_A4_v2",Node_Size_Standard_A8_V2:"Standard_A8_v2",Node_Size_Standard_D1_V2:"Standard_D1_v2",Node_Size_Standard_D2_V2:"Standard_D2_v2",Node_Size_Standard_D3_V2:"Standard_D3_v2",Node_Size_Standard_D4_V2:"Standard_D4_v2",Tier_S0:"S0",Tier_S1:"S1",Tier_S2:"S2",Tier_S3:"S3",Tier_P1:"P1",Tier_P2:"P2",Tier_P4:"P4",Tier_P6:"P6",Tier_P11:"P11",Tier_P15:"P15",Tier_PRS1:"PRS1",Tier_PRS2:"PRS2",Tier_PRS4:"PRS4",Tier_PRS6:"PRS6",Tier_Group_Name_Single_Database:"Single Database",Tier_Group_Name_Elastic_Pool:"Elastic Pool",ActivityPolicy_Tooltip_Timeout:"Maximum amount of time an activity can run. Default is 12 hours, and the maximum amount of time allowed is 7 days. Format is in D.HH:MM:SS",ActivityPolicy_Tooltip_Retry:"Maximum number of retry attempts",ActivityPolicy_Tooltip_Retry_Warning:"Number of retry attempts must be between 0 and 4999 inclusive",ActivityPolicy_Tooltip_SecureOutput:"When checked, output from the activity will not be captured in logging",ActivityPolicy_Tooltip_SecureInput:"When checked, input from the activity will not be captured in logging",ActivityPolicy_Timeout_Error:"Timeout is invalid. The value must be formatted as D.HH:MM:SS and be no more than 7 days",LinkedServicePublishOnFinish:'To avoid publishing immediately to {0}, please use Azure Key Vault to retrieve secrets securely. Learn more <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/source-control#using-passwords-from-azure-key-vault">here</a>',LinkedServicePublishOnFinishConfirm:'As {0} cannot store credentials in a Git repository, this change will be published immediately.\n\nThis may cause issues on the {1} branch and on published resources that depend on this linked service. To avoid immediate publish of linked services, we recommend using Azure Key Vault. Learn more <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/source-control#using-passwords-from-azure-key-vault">here</a>\n\nDo you want to proceed?',LinkedServiceDirectlyPublishOnFinish:"Linked service will be published immediately",linkedServiceIRTooltip:"The Integration Runtime (IR) is the compute infrastructure used by the {0} to provide data integration capabilities across different network environments. Azure Integration Runtime can be used to connect to data stores and compute services in public network with public accessible endpoints. Use a self-hosted Integration Runtime for private/on-premises networks.",linkedServiceIRTooltipForCopyWizard:"The Integration Runtime (IR) is the compute infrastructure used by the {0} to provide data movement capabilities across different network environments. Azure Integration Runtime can be used to connect to data stores in public network with public accessible endpoints. Use a self-hosted Integration Runtime for private/on-premises networks.",LinkedServiceDeleteNow:"Linked service will be deleted immediately, this will not affect your original data store or compute.",LinkedServiceDeleteNowFeatureBranch:"Linked service will be deleted immediately, this can cause issues on the {0} branch and on published resources. We recommend doing this change on {0} branch.",LinkedServiceDeleteOnPublish:"Linked service will be deleted when publishing changes, this will not affect your original data store or compute.",LinkedServiceODBCConnStringDesp:'The ODBC connection string excluding the credential portion. You can specify the connection string or use the system DSN (Data Source Name) you set up on the Integration Runtime machine (you need still specify the credential portion in linked service accordingly)\n.You can also put a password in Azure Key Vault and pull the password configuration out of the connection string. Refer to <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault"> Store credentials in Azure Key Vault </a> with more details.',DefaultLinkedService_Readonly:"Default linked service is readonly.",AccountSelectionMethodLabel:"Account selection method",FromAzureSubscriptionLabel:"From Azure subscription",EnterManuallyLabel:"Enter manually",SelectManually:"Select manually",SetPattern:"Set Patterns",FromSelectionLabel:"From selection",AzureSubscription_SelectLabel:"Azure subscription",Authentication_Method_Label:"Authentication method",Authentication_Reference_Method_Label:"Authentication reference method",Authentication_Reference_Method_Description:"Choose whether you want the above authentication to be inline or refer to a credential object.",Authentication_Method_UseAccountKey_Label:"Account key",AuthenticationType_Select_SQL_Option:"SQL authentication",AuthenticationType_Select_Windows_Option:"Windows authentication",AuthenticationType_Select_Basic_Option:"Basic authentication",AuthenticationType_Select_ServicePrincipalKey_Option:"Service principal with Key",AuthenticationType_Select_ServicePrincipalCert_Option:"Service principal with Cert",AuthenticationType_SASLUsername:"SASL username",AuthenticationType_UsernameAndPassword:"Username and password",AuthenticationType_OAuth2:"OAuth2",AuthenticationType_OAuth:"OAuth",UsernameLabel:"Username",AuthenticationType_WindowsAzureHDInsightService:"Windows Azure HDInsight Service",AuthenticationType_LDAP:"LDAP",MechanismName_DiaplayName:"Mechanism name",AuthenticationType_NoAuthentication:"No authentication",AuthenticationType_UseCredentials:"Credentials",AuthenticationType_Plain:"Plain",SecurityLevel_OnlyUnsecured:"Only unsecured",SecurityLevel_PreferredUnSecured:"Preferred unsecured",SecurityLevel_PreferredSecured:"Preferred secured",SecurityLevel_OnlySecured:"Only secured",SalesforceMarketingCloud_Auth_Method_Description:"OAuth 2.0: Legacy package\nEnhanced sts OAuth 2.0: Enhanced package",Select_Sth:"Select {0}...",AzureStorage_Primary:"Primary",AzureStorage_Storage:"Storage",AzureStorage_DisplayText:"Azure Storage",AzureStorage_SelectMethodDescription:"You can select a storage account from the list of available accounts in your Azure subscriptions, in which case you don\u2019t need to enter account name and account key in free form text fields.",AzureStorage_SubscriptionDescription:"You can select an azure subscription to filter the {0}.",AzureStorage_AccountsDescription:"storage accounts",AzureCosmossDescription:"cosmos accounts",AzureDataExplorerClustersDescription:"clusters",AzureDataExplorerIngestionMappingNameDescription:"The name of a mapping which was pre-created and assigned to Kusto Sink table in advance.",AzureStorage_LinkedServiceLabel:"Azure Storage linked service",Hive_AzureStorageLinkedServiceDescription:"An Azure Storage linked service is required to use Hive linked services for data flow inline source.",AzureStorage_NewLinkedServiceLabel:"New Azure Storage linked service",AzureStorage_CannotGetKey:"Cannot get storage account key",Sample:"sample",AzureStorage_SasUrlDescription:"You can either input the storage service endpoint, or append the SAS token to the URL and leave the below SAS token field blank.",AzureStorage_SasTokenDescription:"The query string that includes all of the information required to authenticate the SAS, as well as to specify the service, resource, and permissions available for access, and the time interval over which the signature is valid. Please leave it blank if SAS Token is already contained in SAS URL",AzureStorage_blobEndpointDescription:"Specify the Azure Blob storage service endpoint with the pattern of https://<accountName>.blob.core.windows.net/.",AzureStorage_ContainerUriDescription:"Specify the Azure Blob storage container URI with the pattern of https://<accountName>.blob.core.windows.net/<containername>.",AzureBatch_CannotGetKey:"Cannot get batch account key",InsertTypeLabel:"Insert type",MergeLabel:"Merge",Azure_Table_InsertType_Replace_Option_Label:"Replace",Azure_Table_Partition_Key_Value_Selection_Label:"Partition key value selection",Azure_Table_Partition_Key_Column_Label:"Partition key column",Default_Partition_Value_Label:"Default partition value",Azure_Table_Use_Source_Column_Option_Label:"Use source column",Azure_Table_Use_Sink_Column_Option_Label:"Use sink column",Azure_Table_Specify_Partition_Value_Option_Label:"Specify partition value",Azure_Table_Row_Key_Value_Selection_Label:"Row key value selection",Azure_Table_Unique_Identifier_Option_Label:"Unique identifier",Azure_Table_Row_Key_Column_Label:"Row key column",Azure_Table_Partition_Key_Value_Selection_Tooltip_Label:"Partition key value can be a fixed value or it can take value from a sink or source column.",Azure_Table_Row_Key_Value_Selection_Tooltip_Label:"Row key value can be an auto generated unique identifier or it can take value from a sink or source column.",AzureSynapseAnalytics:"Azure Synapse Analytics",TridentDatamart:"Trident datamart",TridentDatalake:"Trident data lake",GatewayAdminPortalTitle:"Gateway Admin Portal",AzureSqlDW_Tooltip:"Azure Synapse Analytics can be used with SQL pools available on the specified linked service",AzureSQLDedicatedPool_Tooltip:"Azure Synapse dedicated SQL pool can be used with dedicated SQL pools available in the current workspace",AzureSQLAnalyticsPool_DisplayText:"Azure Synapse dedicated SQL pool",AzureSQLAnalyticsPool_DisplayTextNew:"Azure Synapse dedicated SQL pool (v2)",AzureSQLDedicatedPool_TooltipNew:"Azure Synapse dedicated SQL pool (v2) in the current workspace. For SQL pool v3, use Azure Synapse Analytics connector",AzureSQLSynapseLabel:"Select SQL pool",AzureSQLDatabaseSynapseLabel:"Select a database",AzureSQLDatabaseLabel:"Select SQL database",CopySinkDisableMetricsCollectionLabel:"Disable performance metrics analytics",CopySinkDisableMetricsCollectionDescription:"The is to collect metrics such as DTU, DWU, RU, etc. for copy performance optimization and recommendations. If you are concerned with this behavior, please turn off this feature.",CopySinkEnablePatitionOptionDescription:"Create partitions in folder structure based on one or multiple columns. Each distinct column value (pair) will be a new partition (e.g. year=2000/month=01/file). It supports insert-only mode and requires an empty directory in sink.",CopySinkPatitionColumnNamesDescription:'Select from destination columns in schemas mapping. Supported data types are string, integer, boolean and datetime. Format respects type conversion settings under "Mapping" tab.',RefreshPatititonColumnOptionList:"Refresh partition column name list",AzureSynapseAnalyticsArtifacts:"Azure Synapse Analytics (Artifacts)",AzureSql_DisplayText:"Azure SQL Database",AzureSql_SelectMethodDescription:"Select from a list of available Azure SQL servers and databases in your Azure subscriptions or enter server name and database name manually.",AzureDatabricksDeltaLake_DisplayText:"Azure Databricks Delta Lake",PartnerCenterDescription:"Get help with your data integration project by finding the right migration partners and tools.",PartnerCenterDescription_A365:"Discover new scenarios with partner applications. Learn to use third party tools that connect seamlessly with Azure Synapse with a free trial that connects to your environment.",FileMode_Label:"File mode",FileMode_Description:"File mode selection",WildcardTextboxPlaceholder:"Folder path / file name",AmazonS3_DisplayText:"Amazon S3",AmazonS3Compatible_DisplayText:"Amazon S3 Compatible",AmazonS3_AuthTypeTempToken:"Temporary security credentials",GoogleCloudStorage_AmazonS3API_DisplayText:"Google Cloud Storage (S3 API)",AmazonS3_ServiceUrl_Description:'This value specifies the endpoint to access to the S3 Connector. Change it only if you want to try a different service endpoint or want to switch between https and http. <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service?tabs=data-factory#linked-service-properties">Learn more</a>.',AzureDataBricks_DisplayText:"Azure Databricks",FilePathAndNameDescription:'Specify the file name or file filter. Allowed wildcards are "*" and "?"; use "^" to escape wildcard or escape char in actual file name. (For hybrid copy, it requires self-hosted IR version >= 3.6).',WildcardPathDescription:'Specify the wildcard path. Allowed wildcards are "*", "?", "[]", "{}". (For hybrid copy, it requires self-hosted IR version >= 3.12)',WildcardPathSlider:'Your path contains "*", "?", "[]" or "{}", are you using wildcard in your file path ? Yes or No',AppendSlashForFolder:'Specify the path to the folder and name or filter of the file. Append a slash (/) at the end if the path refers to the folder. Allowed wildcards are "*" and "?"; use "^" to escape wildcard or escape char in actual file name. (For hybrid copy, it requires self-hosted IR version >= 3.6).',FileFilterWarning:'While this property is still supported as-is, you are suggested to use the new filter in above "File" field."',FilePathType:"File path type",AzureBlobFS_DisplayText:"Azure Data Lake Storage Gen2",NewLinkedService_AzureBlobFS:"New Azure Data Lake Storage Gen2 linked service",AzureDataLakeStore_DisplayText:"Azure Data Lake Storage Gen1",AzureDataLakeStore_AccountName_SelectionMethod:"Data Lake Store selection method",AzureDataLakeStore_AccountName_SelectionMethod_Description:"You can select a data lake store account from the list of available accounts in your Azure subscriptions, in which case you don't need to enter account name in free form text fields.",AzureDataLakeStore_Subscription_Description:"You can select an azure subscription to filter the data lake store accounts.",AzureDataLakeStore_AccountName_Select:"Data Lake Store account name",UseSinkSchema:"Use sink schema",CustomizedStagingSchemaDescription:"Uncheck this and specify a schema name under which Data Factory will create a staging table to load upstream data and auto clean up upon completion. Make sure the user has create table permission in the database and alter permission on the schema. If not specified, a temp table will be created under the sink schema as staging.",SqlUseTmpDB:"Use TempDB",SqlUseTmpDBDescription:"If you write large amount of data into SQL database, uncheck this and specify a schema name under which Data Factory will create a staging table to load upstream data and auto clean up upon completion. Make sure the user has create table permission in the database and alter permission on the schema. If not specified, a global temp table is used as staging.",SqlStagingSchemaLabel:"Select user DB schema",SqlStagingSchemaDescription:"Specify a schema name under which {0} will create a staging table to load upstream data and automatically clean them up upon completion. Make sure you have create table permission in the database and alter permission on the schema.",AzureDataLakeStoreCosmosStructuredStream_DisplayText:"Azure Data Lake Storage Gen1 for Cosmos Structured Stream",AzureDataLakeStoreCosmosStructuredStream_AccountName_SelectionMethod:"Data Lake Store for Cosmos Structured Stream selection method",AzureDataLakeAnalytics_DisplayText:"Azure Data Lake Analytics",AzureDataLakeAnalytics_SelectionMode:"Data Lake Analytics selection mode",AzureDataLakeAnalytics_Subscription_Description:"You can select an azure subscription to filter the data lake analytics accounts.",AzureDataLakeAnalytics_AccountName_Select:"Data Lake Analytics account name",AzureDataLakeStoreCosmosStructuredStream_PartitionIndex:"Partition Index",AzureDataLakeStoreCosmosStructuredStream_PartitionIndex_Description:"Specific structured stream partition index to read data",AzureDataLakeStoreGen2CosmosStructuredStream_DisplayText:"Azure Data Lake Storage Gen2 for Cosmos Structured Stream",AzureDataLakeStoreGen2CosmosStructuredStream_Url_SelectionMethod:"Data Lake Store Gen2 for Cosmos Structured Stream selection method",AzureDataLakeStoreGen2CosmosStructuredStream_Url_SelectionMethod_Description:"You can select a data lake store gen2 account from the list of available accounts in your Azure subscriptions, in which case you don't need to enter account name in free form text fields.",AzureMLService_Workspace_SelectionMethod:"Azure Machine Learning workspace selection method",AzureMLService_Workspace_SelectMethodDescription:"Select from a list of available Azure Machine Learning workspace in your Azure subscriptions or enter workspace information manually.",AzureMLService_Workspace_Subscription_Description:"You can select an azure subscription to filter the Azure Machine Learning workspaces.",AzureSynapseArtifacts_Workspace_SelectionMethod:"Azure Synapse Analytics workspace selection method",AzureSynapseArtifacts_Workspace_SelectMethodDescription:"Select from a list of available Azure Synapse Analytics workspace in your Azure subscriptions or enter workspace information manually.",AzureSynapse_Workspace_Subscription_Description:"You can select an Azure subscription to filter the Azure Synapse Analytics workspaces.",AzureDataExplorer_SelectionMethod:"Azure Data Explorer selection method",AzureDataExplorer_SelectMethodDescription:"Select from a list of available Azure Data Explorer in your Azure subscriptions or enter Azure Data Explorer endpoint manually.",AzureDataExplorer_Subscription_Description:"You can select an azure subscription to filter the Azure Data Explorer.",Logging_Perf_Warning:"Reliable mode will log each file name immediately once it is copied to the destination, which may impact copy throughput when copying large amount of files.",AzureBatch_DisplayText:"Azure Batch",AzureBatch_NewLinkedServiceLabel:"New Azure Batch linked service",FTPServer_LinkedService_Host_Placeholder:"Sample: 10.1.1.1 or ftp.xxx.com",LinkedService_EnableSSL:"Enable SSL",LinkedService_DisableSSL:"Disable SSL",SFTPServer_LinkedService_Host_Placeholder:"Sample: 10.1.1.1 or sftp.xxx.com",SFTPServer_LinkedService_SkipHostKeyValidation:"Disable SSH host key Validation",SFTPServer_LinkedService_NotSkipHostKeyValidation:"Enable SSH host key Validation",SFTPServer_LinkedService_PrivateKeyType:"Private key type",SFTPServer_LinkedService_UseKeyContent:"Use key content",SFTPServer_LinkedService_UseKeyPath:"Use key file",SFTPServer_LinkedService_Base64PrivateKeyContent:"Based64 encoded SSH private key content",Host_Placeholder:"Sample: 192.168.222.160",Concur_LinkedService_HttpPath_Placeholder:"Sample: api.concursolutions.com",Hbase_LinkedService_HttpPath_Placeholder:"Sample: /gateway/sandbox/hbase/version",Phoenix_LinkedService_HttpPath_Placeholder:"Sample: /gateway/sandbox/phoenix/version",Square_LinkedService_Host_Placeholder:"Sample: mystore.mysquare.com",Square_LinkedService_RedirectUri_Placeholder:"Sample: http://localhost:2500",Xero_LinkedService_Host_Placeholder:"Sample: api.xero.com",Shopify_LinkedService_Host_Placeholder:"Sample: mystore.myshopify.com",Magento_LinkedService_Host_Placeholder:"Sample: 192.168.222.110/magento3",Authentication_SSH_Public_Key:"SSH public key authentication",Authentication_Multi_Factor_Auth:"Multiple factor authentication",HTTPServer_LinkedService_CertificateLocationTypes:"Certificate location type",HTTPServer_LinkedService_UseEmbeddedCertificateData:"Embedded data",HTTPServer_LinkedService_UseCertificateThumbprint:"Thumbprint",ServiceNow_Endpoint_Placeholder:"Sample: http://<instance>.service-now.com",DynamicsDeploymentTypeOnline:"Online",DynamicsDeploymentTypeOnPremisesWithIfd:"OnPremisesWithIfd",DynamicsAuthTypeOffice365:"Office365",DynamicsAuthTypeIfd:"Ifd",DynamicsServiceUriPlaceholder:"https://<organization-name>. crm[x].dynamics.com",AmazonMWSEndpointPlaceholder:"mws.amazonservices.com",AmazonS3PrefixDescription:"Specify the path to the folder (append a slash '/' at the end) or the prefix with which the keys start.",AmazonS3KeyDescription:"Specify the key name or key filter. Allowed wildcards are '*' and '?', use '^' to escape wildcard or escape char in actual key name. (For hybrid copy, it requires self-hosted IR version >= 3.6).",PayPal_Host_Placeholder:"Sample: api.sandbox.paypal.com",Marketo_Endpoint_Placeholder:"Sample: 123-ABC-321.mktorest.com",Presto_LinkedService_ServerVersion_Placeholder:"Sample: 0.148-t",Jira_Host_Placeholder:"Sample: jira.example.com",Zoho_Endpoint_Placeholder:"Sample: www.zohoapis.com",Eloqua_Endpoint_Description:'To determine your endpoint, login to https://login.eloqua.com with your credential, then copy the base URL portion from the redirected URL with the pattern of "xxx.xxx.eloqua.com".',Eloqua_Username_Placeholder:"site name\\user name",QuickBooks_Endpoint_Placeholder:"Sample: quickbooks.api.intuit.com",RefreshToken_Description:"The refresh token for OAuth 2.0 authentication. Click to view more.",AccessToken_Description:"The access token for OAuth 2.0 authentication. Click to view more.",OracleServiceCloud_Host_Placeholder:"Sample: servicecloud.example.com",UserScope:"User scope",Office365_UserGroupScope_SelectAll_Label:"All users or groups in the Microsoft 365 tenant",Office365_UserScope_SelectAll_Label:"All users in the Microsoft 365 tenant",Office365_GroupScope_SelectAll_Label:"All groups in the Microsoft 365 tenant",Office365_UserScope_SelectGroups_Label:"Select groups from the Microsoft 365 tenant",Office365_UserScope_Filter_Label:"User scope filter",Office365_ScopeFilter_Label:"Scope filter",Office365_UserScope_AllowedGroups_Label:"Allowed groups",Office365_AutoFlatten:"Auto flatten",Office365_AutoFlattenWarning:"When checking this checkbox, the row count would increase exponentially.",Office365_NoFilterColumn:"No date filter available",Office365_DateFilter:"Date filter",Office365_ParameterValueToListColumns:"Please provide actual value of the parameters to list columns",AddUserGroupLabel:"Add user groups",GroupDescription:"Group description",GroupName:"Group name",GroupID:"Group ID",TargetSubResource:"Target sub-resource",TargetSubResourceDescription:"The type of sub-resource for the resource selected above that your private endpoint will be able to access.",GroupNoResult:"No groups selected",ContainedBy:"Contained by {0}",AddTo:"Add to...",UserGroup_SearchPlaceholder:"Search by group name or group ID",UngroupDataflow:"Ungroup",GroupDataflowShortcut:"Ctrl + G",UngroupDataflowShortcut:"Ctrl + Shift + G",LoadMore:"Load more",UserGroup_Candidate_Tooltip:"Group ID:\n{0}",UserGroup_SelectedDataFactories:"Selected groups:",UserAndGroup_AddAdminInfo:"Assign a role to an AD user, group, or service principal. Each role can have up to 10 members.\u200b",UserAndGroup_AddRoleAssignmentDescription:"Grant others access to this workspace by assigning roles to users, groups, and/or service principals.",UserAndGroup_ScopeTypeTooltip:"Select the part of the workspace to which you want to grant access.",UserAndGroup_RoleTooltip:"Select the role definition that has the permissions you want to grant.",UserAndGroup_SearchTooltip:"Select the AAD service principal to which you want the role assigned. Assigning roles to groups is more easily managed than assigning roles to individual users.",UserAndGroup_SearchPlaceholder:"Search by name or email address",UserAndGroup_SearchPlaceholderObjectId:"Enter object ID",UserAndGroup_SearchResult:"Search result",UserAndGroup_Selected:"Selected user(s), group(s), or service principal(s)",UserAndGroup_NoSelected:"No users, groups, or apps selected.",UserAndGroup_NoRolesSelected:"No roles selected.",GuestUser:"{0} (Guest)",DataStoreReadSettings:"Data store read settings",SelectedTableIsNotDelta:"The selected table is not a delta table",CopyWizard_SelectedTableIsNotDelta:"The selected {0} is not a delta table",CopyWizard_SelectedTablesAreNotDeltas:"The selected {0} are not delta tables",Body_Column:"Body column",Dataset_RelativeUrl:"Relative URL",Dataset_RequestMethod:"Request method",Dataset_RequestMethod_Get:"GET",Dataset_RequestMethod_Put:"PUT",Dataset_RequestMethod_Patch:"PATCH",Dataset_RequestMethod_Delete:"DELETE",Dataset_AdditionalHeaders:"Additional headers",Dataset_AdditionalHeadersDesc:"Additional HTTP request headers.",Dataset_RequestBody:"Request body",Dataset_PaginationRules:"Pagination rules",Dataset_RelativeUrlDescription:"A relative URL to the resource that contains the data. When this property isn't specified, only the URL that's specified in the linked service definition is used. The HTTP connector copies data from the combined URL: [URL specified in linked service]/[relative URL specified in dataset].",Dataset_PaginationRulesDescription:"The pagination rules to compose next page requests. Click to check out more details.",Dataset_RequestMethodDescription:"The HTTP method. Allowed values are Get (default) and Post.",CallTransform_RequestMethodDescription:"The HTTP method. Allowed values are Get (default) and Post, Patch, Put, and Delete.",Dataset_RequestBodyDescription:"The request body for the HTTP request.",Dataset_HttpRequestTimeoutDescription:"The timeout (the TimeSpan value) for the HTTP request to get a response. This value is the timeout to get a response, not the timeout to read response data. The default value is 00:01:40.",DataFlow_HttpRequestTimeoutDescription:"The timeout in seconds for the HTTP request to get a response. This value is the timeout to get a response, not the timeout to read response data.",DWU_Usage:"DWU usage",VCore_Allocation:"vCore Allocation",Active_Requests:"Active requests",Ended_Requests:"Ended requests",REST_CopySource_Request_Interval_Description:"The interval time between different requests for multiple pages in milisecond. Request interval value should be a number between [10, 60000].",REST_CopySink_Request_Interval_Description:"The interval time between different requests in milisecond. Request interval value should be a number between [10, 60000].",CopyWizard_Dataset_File_Rest_PageHeader:"Specify REST dataset properties",CopyWizard_Dateset_File_LoadingBehavior:"File loading behavior",CopyWizard_Dateset_File_IncrementalByPartitionLabel:"Incremental load: time-partitioned folder/file names",CopyWizard_Dateset_File_IncrementalByModifiedDateLabel:"Incremental load: LastModifiedDate",CopyWizard_Dateset_File_LoadAll:"Load all files",CopyWizard_TypeConversion_Not_Supported:"Type conversion is not supported for current copy settings",SapCDC_CopySource_ResetActivity_DialogContent:"Are you sure you want your SAP system to stop tracking delta extractions under this subscriber process ({0})?",Resetting:"Resetting...",ResetSuccessMessage:"Reset successful",ResetFailedMessage:"Reset failed",SapCDC_CopySource_Projection_Placeholder:"Select no column to extract all by default",CopySource_AdditionalColumns:"Additional columns",CopySource_AdditionalColumns_Description:"Add additional data columns to store source files' relative path or static value. Expression is supported for the latter.",CopySource_AdditionalColumns_FilePath_Description:"Source files' relative path to the folder path specified in the dataset.",CopySource_AdditionalColumns_Custom_Description:"Add a column with static value.",CopySource_AdditionalColumns_DupColumn_Description:"Duplicate an existing source column.",AdditionalColumns_Not_Allow_Polybase:"Specifying additional columns is not allowed in direct copy by using PolyBase,",AdditionalColumns_Not_Allow_CopyCommand:"Specifying additional columns is not allowed when using copy command.",AdditionalColumns_Not_Allow_EmptyName:"Additional columns' names are not allowed to be empty.",AdditionalColumns_Not_Support_Sink:"Additional columns are not supported for your sink {0} dataset, please create a new dataset to enable additional columns.",AdditionalColumns_Not_Allow_DuplicateName:"Additional columns' names should be different from each other.",AdditionalColumns_Not_Allow_DistCp:"Specifying additional columns are not allowed when using HDFS Distcp setting.",AdditionalColumns_Not_Allow_RedshiftUnload:"Specifying additional columns are not allowed when using unloading data through Amazon S3.",AdditionalColumns_Invalid_Value:"Customized value of additional column cannot start with '$$'",AdditionalColumns_Invalid_DupColumnValue:"Please specify an existing source column in additional columns.",PleaseUpgradedSHIR_Error_Template:"Please upgrade Self-hosted integration runtime (IR) to the latest version to use {0}.",Square_RefreshToken_AccessToken_Hint:"Please specify either refresh token or access token.",Rest_Get_Request_Body_ValidationError:"REST connector doesn't support GET requests with request body.",Rest_Additional_Headers_Warning:'REST connector ignores any "Accept" header specified in additional headers. As REST connector only supports response in JSON, it will auto generate a header of Accept: application/json.',DataFlow_Rest_Additional_Headers_Warning:'Make sure The "Accept" header is compatible with the specified format (JSON/XML).',Rest_ContentType_Header_Warning:'REST connector ignores any "Content-Type" header specified in additional headers when request body is empty.',Rest_Sink_Additional_Headers_Warning:'REST connector ignores any "Accept-Encoding", "Content-Encoding" header specified in additionalHeaders. REST connector sets appropriate headers based on CompressionType parameter.',DataFlow_Rest_Sink_Additional_Headers_Encoding_Warning:'Make sure the "Accept-Encoding", "Content-Encoding" header specified in additionalHeaders are compatible with the specified compression type.',DataFlow_Rest_Sink_Additional_Headers_Content_Warning:'Make sure the "Content-type" header specified in additionalHeaders is compatible with the specified format.',WebTable_Dataset_TableIndex:"Index",PathLabel:"Path",List_Name:"List name",Source_Name:"Source name",Target_Name:"Target name",Query_Parameters:"Query parameters",EntityUsedBy:"This {0} is being used by the following:",ODICopyDestinationFolderPath_Description:"Specify a folder that will contain output files or a specific output file in the destination data store. You can use variables in the folder path to copy data from/to a folder or a file that is determined at runtime. The supported variables are: {year}, {month}, {day}, {hour}, {minute}, and {custom}. Example: inputfolder/{year}/{month}/{day}.",AzureSearch_Subscription_Description:"You can select an azure subscription to filter the services.",AzureSearch_CannotGetKey:"Cannot get admin key",ResourceLoading_GenericErrorMessage:"Could not load resource. Please ensure no mistakes in the JSON and that referenced resources exist.",ResourceLoading_GenericErrorMessageWithName:"Could not load resource '{0}'. Please ensure no mistakes in the JSON and that referenced resources exist.",ResourceLoading_GenericErrorMessageWithNameAndReason:"Could not load resource '{0}'. Please ensure no mistakes in the JSON and that referenced resources exist. Status: {1}, Possible reason: {2}",ResourceLoading_GenericErrorMessageWithReason:"Could not load resource. Please ensure no mistakes in the JSON and that referenced resources exist. Status: {0}, Possible reason: {1}",ResourceLoading_SmallGenericErrorMessageWithReason:"Could not load resource '{0}'. {1}",ResourceLoading_ResourceNotFoundErrorMessageWithNames:"Could not load resource '{0}' because it is referencing a non-existent resource named: '{1}'",ResourceLoading_ResourceNotFoundErrorMessage:"Could not load resource because it is referencing a non-existent resource named: '{0}'",InvalidJsonSyntaxLabel:"Invalid JSON syntax",ResourceLoading_InvalidType:"Could not load resource because one of the types is invalid. Please ensure there are no mistakes in the JSON.",ResourceLoading_SubtypeDoesNotExist:"The type '{0}' does not exist",ResourceLoading_SubtypeDoesNotBelongToType:"The subtype '{0}' does not belong to the type '{1}', it belongs to the type '{2}'",ResourceLoading_PartiallyBadResource:"This resource references resources which could not be loaded",ResourceLoading_ReferencingBadResource:"Error referencing {0}. Bad resource.",ResourceLoading_NameMismatch:"Git file and resource name are different",ResourceLoading_PropertyMismatch:"The {1} of this integration runtime is different from your {0}. Please make sure your Git repository is in-sync with your {0}.",ResourceLoading_MissingSubType:"Could not find a subtype for the resource",ResourceLoading_InvalidJson_Detailed:"{0} has invalid JSON syntax. Fix the resource and refresh the page.",ResourceLoading_NameMismatch_Detailed:"Git file name: {0} and resource name: {1} are different. They should be the same. Fix the resource and refresh the page.",JSON_Editor_NotAllowed_Template:"{0} is not allowed from JSON editor.",ResourceLoading_UpdatingOid:"Updating object id",ResourceLoading_MissingSubType_Detailed:"Could not find a subtype for the resource. Fix the resource and refresh the page.",ResourceLoading_LoadingResourceTypeErrorFormat:'Failed to load resources of type "{0}". Some resources may not be shown.\nError: {1}',ModelName_HdiOnDemand_ClusterType:"Cluster type",ModelName_HdiOnDemand_ClusterSize:"Cluster size",ModelName_HdiOnDemand_TimeToLive:"Time to live",ModelName_HdiOnDemand_HostSubsId:"Host subscription ID",ModelName_HdiOnDemand_ClusterRp:"Cluster resource group",ModelName_HdiOnDemand_AdditionalLinkedServices:"Additional storage linked service",ModelName_HdiOnDemand_HCatalogLinkedService:"Hcatalog linked service",ModelName_HdiOnDemand_ResourceGroupSelectionMethod:"Resource group selection method",HdiOnDemand_FullName:"On-demand HDInsight",HdiOnDemand_HeadNode_DisplayName:"Head",WorkerLabel:"Worker",HdiOnDemand_ZookeeperNode_DisplayName:"Zookeeper",HdiOnDemand_ToolTip_clusterSize:"Number of worker/data nodes in the cluster. The HDInsight cluster is created with 2 head nodes along with the number of worker nodes you specify for this property. The nodes are of size Standard_D3 that has 4 cores, so a 4 worker node cluster takes 24 cores (4*4 = 16 cores for worker nodes, plus 2*4 = 8 cores for head nodes). See Set up clusters in HDInsight with Hadoop, Spark, Kafka, and more for details.",HdiOnDemand_ToolTip_clusterResourceGroup:"\tThe HDInsight cluster is created in this resource group.",HdiOnDemand_ToolTip_timetolive:"The allowed idle time for the on-demand HDInsight cluster. Specifies how long the on-demand HDInsight cluster stays alive after completion of an activity run if there are no other active jobs in the cluster. The minimal allowed value is 5 minutes (00:05:00).",HdiOnDemand_ToolTip_clusterType:'The type of the HDInsight cluster to be created. Allowed values are "hadoop" and "spark". If not specified, default value is hadoop.',HdiOnDemand_ToolTip_version:"Version of the HDInsight cluster. If not specified, it's using the current HDInsight defined default version.",HdiOnDemand_ToolTip_clusterNamePrefix:"The prefix of HDI cluster name, a timestamp will be automatically appended at the end of the cluster name",HdiOnDemand_ToolTip_additionalLinkedServiceNames:"Specifies additional storage accounts for the HDInsight linked service so that the {0} service can register them on your behalf. These storage accounts must be in the same region as the HDInsight cluster, which is created in the same region as the storage account specified by linkedServiceName.",HdiOnDemand_ToolTip_hcatalogLinkedServiceName:"The name of Azure SQL linked service that point to the HCatalog database. The on-demand HDInsight cluster is created by using the Azure SQL database as the metastore.",HdiOnDemand_ToolTip_osType:"Type of operating system. Allowed values are: Linux and Windows (for HDInsight 3.3 only). Default is Linux.",HdiOnDemand_ToolTip_clusterUserName:"The username to access the cluster.",HdiOnDemand_ToolTip_clusterPassword:"The password in type of secure string to access the cluster.",HdiOnDemand_ToolTip_clusterSshUserName:"The username to SSH remotely connect to cluster\u2019s node (for Linux).",HdiOnDemand_ToolTip_clusterSshPassword:"The password in type of secure string to SSH remotely connect cluster\u2019s node (for Linux).",HdiOnDemand_ToolTip_multiScriptActionSupportComingSoon:"Can specify up to 1 script action. Support of multiple script actions coming soon.",HdiOnDemand_ToolTipLink_scriptAction:"https://docs.microsoft.com/azure/hdinsight/hdinsight-hadoop-customize-cluster-linux",HdiOnDemand_ToolTip_scriptName:"The name for the script action. Click to view documentation.",HdiOnDemand_ToolTip_scriptUri:"The URI to the script that is invoked to customize the cluster. Click to view documentation.",HdiOnDemand_ToolTip_scriptNodeTypes:"The nodes (Head, Worker, or ZooKeeper) on which the customization script is run.",HdiOnDemand_ToolTip_scriptParameters:"The script parameters, if required by the script.",HdiOnDemand_ToolTip_servicePrincipalId:"Specify the application's client ID.",HdiOnDemand_ToolTip_servicePrincipalKey:"Specify the application's key.",HdiOnDemand_ToolTip_linkedServiceName:"Azure Storage linked service to be used by the on-demand cluster for storing and processing data. The HDInsight cluster is created in the same region as this Azure Storage account. Azure HDInsight has limitation on the total number of cores you can use in each Azure region it supports. Make sure you have enough core quotas in that Azure region to meet the required clusterSize. For details, refer to Set up clusters in HDInsight with Hadoop, Spark, Kafka, and more",HdiOnDemand_ToolTip_coreConfiguration:"Specifies the core configuration parameters (as in core-site.xml) for the HDInsight cluster to be created.",HdiOnDemand_ToolTip_hBaseConfiguration:"Specifies the HBase configuration parameters (hbase-site.xml) for the HDInsight cluster.",HdiOnDemand_ToolTip_hdfsConfiguration:"Specifies the HDFS configuration parameters (hdfs-site.xml) for the HDInsight cluster.",HdiOnDemand_ToolTip_hiveConfiguration:"Specifies the hive configuration parameters (hive-site.xml) for the HDInsight cluster.",HdiOnDemand_ToolTip_mapReduceConfiguration:"Specifies the MapReduce configuration parameters (mapred-site.xml) for the HDInsight cluster.",HdiOnDemand_ToolTip_oozieConfiguration:"Specifies the Oozie configuration parameters (oozie-site.xml) for the HDInsight cluster.",HdiOnDemand_ToolTip_stormConfiguration:"Specifies the Storm configuration parameters (storm-site.xml) for the HDInsight cluster.",HdiOnDemand_ToolTip_yarnConfiguration:"Specifies the Yarn configuration parameters (yarn-site.xml) for the HDInsight cluster.",HdiOnDemand_ChooseVnetSubnet:"Choose Vnet and Subnet",HdiOnDemand_ToolTip_ChooseVnetSubnet:"Choose the Virtual Network ID and Subnet",HdiOnDemand_NoSubnetSelected:"No Subnet Selected",HdiOnDemand_ToolTip_Vnet:"Format: /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Network/virtualNetworks/<VNetName>",HdiOnDemand_ToolTip_Subnet:"Format: /subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>/providers/Microsoft.Network/virtualNetworks/<SubnetName>",Hdfs_AuthenticationTypeDescription:"Allowed values are: Anonymous, or Windows.\nTo use Kerberos authentication for HDFS connector, refer to https://docs.microsoft.com/azure/data-factory/connector-hdfs#use-kerberos-authentication-for-hdfs-connector to set up your on-premises environment accordingly.",Hdfs_Source:"HDFS source",DistcpSetting:"DistCp setting",Hdi_DisplayName:"Azure HDInsight",Hdi_FullName:"Bring your own HDInsight",Hdi_ToolTip_esp:"Authentication & Authorization with Active Directory and Apache Ranger integration.  This feature requires the latest version (>=3.10) of self-hosted Integration Runtime.",Hdi_FileSystemRequired:"File System is a required property when using ADLS Gen 2",Hdi_StorageOption_AzureBlob:"Blob Storage",Hdi_StorageOption_DataLake:"ADLS Gen 2",Hdi_SelectClusterToListStorageAccounts:"Select a cluster to list storage accounts",Hdi_StorageAccountSectionHeader:"Storage accounts associated with cluster",Hdi_StorageAccountSectionDescription:"Please select a storage linked service using the listed accounts",Hdi_FailedToLoadStorageAccounts:"Failed to load storage accounts, reason: {0}",SqlServer_DisplayText:"SQL server",AzureSql:"Azure SQL",AmazonRdsSqlServer_DisplayText:"Amazon RDS for SQL Server",AmazonRdsSqlServer_AuthenticationTypeDescription:'<a target="_blank" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerWinAuth.html">Learn more</a> about the authentication settings.',AzureSQLManagedInstance_DisplayText:"Azure SQL Database Managed Instance",AlwaysEncrypted:"Always encrypted",AlwaysEncryptedDesc:"Enable to protect sensitive data stored in {0}. This cannot be validated via test connection.",KeyStoreAuthType:"Key store authentication type",FullyServerNameLabel:"Fully qualified domain name",Oracle_DisplayText:"Oracle",Oracle_ResourceTypeLabel:"Connection type",Oracle_SID_Option_DisplayText:"Oracle SID",Oracle_ServiceName_Option_DisplayText:"Oracle service name",Oracle_ResourceTypeDescription:"Select the way to identify your database",AmazonRdsForOracle_DisplayText:"Amazon RDS for Oracle",FileSystem_DisplayText:"File system",FileShare_DisplayText:"File share",FileSystem_HostStartWithHttp_ErrorMessage:"File server does not support host started with 'http:', please consider using http linked service.",FileSystem_HostStartWithFtp_ErrorMessage:"File server does not support host started with 'ftp:', please consider using ftp linked service.",FileSystem_Host_Description:"Specifies the root path of the folder or the Azure File Storage endpoint.",General:"General",OutputLabel:"Output",Binary_Copy_Label:"Binary copy",Binary_Copy_Schema_Agnostic:"Schema agnostic (binary copy)",Binary_Copy_Description:"Files will be treated as binaries, schema definitions will not be enforced. Destination cannot be relational database",Binary_Copy_FileTabular_ErrorMessage:"Binary copy between file-based stores and tabular dataset is not allowed.",Binary_Copy_BinaryToNonBinary_ErrorMessage:"Sink must be binary when source is binary dataset.",Binary_Copy_NonBinaryToBinary_ErrorMessage:"Source must be binary when sink is binary dataset.",Binary_Copy_FolderCopyToFileError:"Binary copy does not support copying from folder to file",Binary_Copy_DeleteFilesAfterCompletionError:"Delete files after completion is only supported in binary copy with source and sink dataset setting None compression type.",Binary_Copy_DeleteFilesAfterCompletionAnonymousError:"Delete files after completion is not supported when source linked service is Azure blob storage with Anonymous authentication.",Binary_Copy_DeleteFilesAfterCompletionHint:"Please note that you have chosen to delete files after completion, so the source files will be deleted once the file copy completes.",DeleteFileAfterCompletion_Description:"The files on source data store will be deleted right after being moved to the destination store. The file deletion is per file, so when copy activity fails, you will see some files have already been copied to the destination and deleted from source while others are still on source store.",SapCdc_Copy_Selection_Option_Between_Error:"Operator BETWEEN (BT) is only allowed if high is filled.",SapCdc_Copy_Selection_Option_Pattern_Error:"Operator LIKE (CP) is only allowed if wildcards ('*' or '+') are used in value.",SapCdc_Copy_Selection_Option_Error:"Field '{0}' can't use operator '{1}' in {2} {3}",SapOdp_IsSSO_Description:"Windows account needs to be input as username/password when 'With single sign-on' is checked",PartitionDiscovery_In_Compatible_With_Direct_Polybase:"Partition discovery is not supported when using direct polybase.",PartitionDiscovery_In_Compatible_With_Copy_Command:"Partition discovery is not supported when using copy command",AEP_CopyToFile_ErrorMessage:"Adobe Experience Platform can only be copied into a folder.",AEP_CopyBehavior_ErrorMessage:'Copy behavior cannot be "Merge files" when source is Adobe Experience Platform.',CopySelfhostedIR_ErrorMessage:"{0} only supports Azure IR and cannot be copied into {1}, which references to a Self-hosted IR.",CopyBehavior_Description:"Defines behaviour when copying files from one file system like storage to the other (e.g. from one blob storage to the other)",CopyBehavior_Option_FlattenHierarchy:"Flatten hierarchy",CopyBehavior_Option_MergeFiles:"Merge files",CopyBehavior_Option_PreserveHierarchy:"Preserve hierarchy",CopyBehavior_Must_Be_None_Error:"Copy behavior must be none when sink compression type is ZipDeflate",CopyBehavior_Must_Be_Mergefiles_Error:'Copy behavior must be "Merge files" when sink is a file and source is a folder or specifies wildcard file name, wildcard folder path or file filter.',CopyBehavior_Must_Be_Mergefiles_Error_Staging:'Copy behavior must be "Merge files" when copy from table to file with staging',CopyBehavior_Must_Be_Mergefiles_Error_Unload:'Copy behavior must be "Merge files" when copy from table to file with unload',CopyBehavior_Cannot_Be_Mergefiles_Error:'Copy behavior cannot be "Merge files" when you want to copy file as-is',CopyBehavior_Cannot_Be_Mergefiles_Error_Table:'Copy behavior "Merge files" is only applicable when copy between two file type data stores',CopyBehavior_Cannot_Be_Mergefiles_When_Use_BinaryCopy_Error:'Copy behavior cannot be "Merge files" when source or sink uses binary copy',CopyBehavior_MergeFiles_HintMessage:"Merges all files from source into one file may downgrade the performance as it can't take advantage of file-level parallelism.",TablePath_Description:"A relative URL to the resource that contains the table",CDMLabel:"CDM",SourceDataset:"Source dataset",SourceDatasetParametersInfo:"Refresh canvas after updating parameter values",SourceStagingParallelismDefault:"No parallelism, run sequentially",SourceStagingParallelismCustom:"Custom Value for the degree of parallelism",SourceStagingParallelismMax:"Maximum Value for the degree of parallelism",SourceLinkedService:"Source linked service",TargetLinkedService:"Target linked service",SinkDataset:"Sink dataset",SinkLinkedService:"Sink linked service",Sink:"Sink",SinkName:"Sink name",Staging:"staging",StagingLabel:"Staging",NewLinkedServiceTitle:"New linked service",NewConnectionTitle:"New connection",EditLinkedServiceTitle:"Edit linked service",SourceConnection:"Source connection",DestinationConnection:"Destination connection",PublishLinkedServiceUnknownError:"Met unknown error when saving the linked service",PublishVNetUnknownError:"Met unknown error when saving virtual network",PublishLinkedServiceNoEncryptedCredentialError:"Please reenter the credential of the linked service and try again",LinkedService_FullyByJsonEditorLabel:"Specify dynamic contents in JSON format",LinkedService_FullyByJsonEditorTooltip:"Specify dynamic contents for the properties in JSON format instead of through the visual experience.",LinkedService_ParameterizedLinkedServiceNotSupportedInCopyWizard_ErrorMessage:"The connection uses parameters. Currently, such connection is not supported in the copy data tool.",LinkedService_ParameterizedLinkedServiceControlTable_ErrorMessage:"The connection is referenced as a control table data store. Currently, such connection is not supported to be parameterized.",LinkedService_ProvideParameter_Desc:"Please provide actual value of the parameters for {0} {1}.",LinkedService_TestConnectionNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, testing connection is not supported for such linked service.",LinkedService_TestConnectionRequireParameterValue_ErrorMessage:"All the linked service's parameters need to be given values",LinkedService_TestConnectionToFilePathLabel:"To file path",LinkedService_TestConnectionToLSLabel:"To linked service",LinkedService_TestConnectionToSecretLabel:"To secret",LinkedService_TestConnectionToFilePathDescTemplate:"If the identity you use to access the data store only has permission to subdirectory instead of the entire account, specify the path to {0}.",LinkedService_NavTableProvideParameter_Table_Title:"Please provide actual value of the parameters to list tables",LinkedService_NavTableProvideParameter_Schema_Title:"Please provide actual value of the parameters to list schemas",LinkedService_NavTableNotSupportedForParameter_Table_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, listing tables is not supported for such linked service.",LinkedService_NavTableProvideParameter_AmazonUnload_Title:"Please provide actual value of the parameters to list bucket names for UNLOAD",LinkedService_NavTableNotSupportedForParameter_AmazonUnload_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, listing bucket names for UNLOAD is not supported for such linked service.",LinkedService_NavTableProvideParameter_File_Title:"Please provide actual value of the parameters to list files",LinkedService_NavTableNotSupportedForParameter_File_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, listing files is not supported for such linked service.",LinkedService_ListStoredProcedureProvideParameter_Title:"Please provide actual value of the parameters to list stored procedures",LinkedService_ListStoredProcedureNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, listing stored procedures is not supported for such linked service.",LinkedService_PreviewProvideParameter_Title:"Please provide actual value of the parameters to preview data",LinkedService_GetWorkSheetListProvideParameter_Title:"Please provide actual value of the parameters to get work sheet list",LinkedService_PreviewNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, previewing data is not supported for such linked service.",LinkedService_DetectFileFormatProvideParameter_Title:"Please provide actual value of the parameters to detect file format",LinkedService_DetectFileFormatNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, detecting file format is not supported for such linked service. Please set the file format by yourself.",LinkedService_DetectFileFormatCancel_ErrorMessage:"The linked service uses parameters. Can't detect the file format because you didn't provide actual value for the parameters.",LinkedService_GetSchemaProvideParameter_Title:"Please provide actual value of the parameters to get schema",LinkedService_GetSchemaNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, getting schema is not supported for such linked service. Please cancel this operation and set the schema by yourself.",LinkedService_GetSchemaInCopyMappingProvideParameter_Source_Title:"Please provide actual value of the parameters to get schema for source dataset",LinkedService_GetSchemaInCopyMappingProvideParameter_Sink_Title:"Please provide actual value of the parameters to get schema for sink dataset",LinkedService_GetSchemaInCopyMappingNotSupportedForParameter_ErrorMessage:'The linked service of dataset "{0}" uses parameters and contains secret. Currently, for visualized authoring experience, getting schema is not supported for such linked service. Please cancel this operation and set the schema by yourself by editing the Schema tab of the dataset.',LinkedService_UploadProvideParameter_Title:'Please provide actual value of the parameters to upload script for activity "{0}" in pipeline "{1}"',LinkedService_UploadNotSupportedForParameter_ErrorMessage:'The linked service uses parameters and contains secret. Currently, uploading script to such linked services is not supported. Please manually upload your script for activity "{0}" in pipeline "{1}", and then save again',LinkedService_UploadCancel_ErrorMessage:'The linked service uses parameters. But you didn\'t provide actual value for the parameters. Please manually upload your script for activity "{0}" in pipeline "{1}", and then save again',LinkedService_FetchScriptProvideParameter_Title:"Please provide actual value of the parameters to fetch script",LinkedService_FetchScriptNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, fetching script from and uploading script to such linked services are not supported. Please manually edit your script and upload the script to the storage",LinkedService_ListSPParameterProvideParameter_Title:"Please provide actual value of the parameters to list stored procedure parameters",LinkedService_ListSPParameterNotSupportedForParameter_ErrorMessage:"The linked service uses parameters and contains secret. Currently, for visualized authoring experience, listing stored procedure parameters from and uploading script to such linked services are not supported. Please manually edit your script and upload the script to the storage",LinkedService_NotSupportSink_ErrorMessage:"The linked service in sink dataset does not support sink.",LinkedService_NotSupportDelete_ErrorMessage:"The linked service in the dataset does not support Delete acticity.",LinkedService_DefaultLsChanged_ErrorMessage:"Default linked services cannot be changed. Please revert changes in the default linked service",User_Cancelled_Parameter_AdhocValue_Input:"User cancelled inputing adhoc values for parameters.",PipelineActivity_AdhocValue_AdditionalExpressions_Title:"Additional expressions",Office365OnlyAllowJsonFormatError:"Microsoft 365 source only supports copying to JSON format (set of objects).",Office365NotSupportSchemaMappingError:"Microsoft 365 source does not support schema mapping.",Office365OnlyAllowCopytoFolder:"Microsoft 365 source only supports copying to folder. Please select another dataset or edit the sink dataset's File path to folder.",Office365OnlyAllowCopytoBinary:"Microsoft 365 source only supports copying to binary format dataset. Please select binary format for sink dataset.",Office365OnlySupportSPError:"Microsoft 365 only supports copying to sink dataset with linked service using Service Principal authentication. Please correct the authentication type in linked service.",Office365RequireDateFilterColumnMessage:"Date filter column is required for table {0}",Office365RequireStartTimeMessage:"Start time is required for table {0}",Office365RequireEndTimeMessage:"End time is required for table {0}",Office365RequiredSuffixMessage:" in {0}",Office365RequireColumnsMessage:"Output columns is required for Microsoft 365 source",Microsoft365MDFAllowSink:"Microsoft 365 dataflow source only supports {0} as sink.",Microsoft365MDFAuthTypeError:"Microsoft 365 dataflow source only supports {0} dataflow sink with linked service using {1} authentication. Please correct the authentication type in linked service.",ColumnNameEmptyMessage:"Coloumn name cannot be empty.",ColumnNameDuplicatedMessage:"Coloumn name cannot be duplicated.",LinkedService_IR_Offline:"Integration runtime {0} is currently unavailable. Please try creating linked service once it is online.",LinkedService_SHIR_With_Credential:"The credentials are stored in the machines of self-hosted integration runtime if you don't choose to store them in Azure Key Vault.",Canvas_ToolboxEmptySearchResult:"No results",NoDataflowSessions:"No active data flow sessions",NoPipelineRuns:"No active pipeline runs",TryRunningPipeline:"Try running a pipeline",ExpressionBuilder_NewParameter:"Add a new parameter to ",ExpressionBuilder_ClearContents:"Clear contents",ExpressionBuilder_SetAsDefaultParameter:"Set expression to a new parameter",ExpressionBuilder_SetAsDefaultParameterDescription:"Creates a default parameter with the same name as the field",FilterPlaceholder:"Filter...",FilterItemsPlaceholder:"Filter items...",FilterExpressionsPlaceholder:"Filter system variables and functions...",FilterByIdOrName:"Filter by run ID or name",FilterByKeyword:"Filter by keyword",FilterByName:"Filter by name",FilterByTable:"Filter by table",SearchByIdOrName:"Search by run ID or name",DebugRunsFilterByKeywordTooltip:"Filter is not supported for debug runs",ExpressionBuilder_ExpressionInsertTooltip:"click to insert expression",CacheSinks:"Cached lookup",Functions:"Functions",DataFlowTypes:"DataFlow Types",CopyFilters:"Copy filters",CopyFiltersTooltip:"Copy the filter details to share or apply again with filter change",ExpressionBuilder_SystemVariablesSection:"System variables",ExpressionBuilder_ActivityOutputsSection:"Activity outputs",ExpressionBuilder_ActivityOutput:"activity output",ExpressionBuilder_LookUpActivityOutputFirstRowDesc:"Data of the first row",ExpressionBuilder_LookUpActivityOutputCountDesc:"Count of the rows",ExpressionBuilder_LookUpActivityOutputValueDesc:"Array of row data",ExpressionBuilder_GetMetaActivityOutputStructureDesc:"Data structure of the file or relational database table",ExpressionBuilder_GetMetaActivityOutputColumnCountDesc:"Number of columns in the file or relational table",ExpressionBuilder_GetMetaActivityOutputExistsDesc:"Whether a file, folder, or table exists",ExpressionBuilder_GetMetaActivityOutputItemNameDesc:"Name of the file or folder",ExpressionBuilder_GetMetaActivityOutputItemTypeDesc:"Type of the file or folder. Returned value is File or Folder",ExpressionBuilder_GetMetaActivityOutputLastModifiedDesc:"Last modified datetime of the file or folder",ExpressionBuilder_GetMetaActivityOutputChildItemsDesc:"List of subfolders and files in the given folder",ExpressionBuilder_GetMetaActivityOutputSizeDesc:"Size of the file, in bytes. Applicable only to files",ExpressionBuilder_GetMetaActivityOutputContentMD5Desc:"MD5 of the file. Applicable only to files",ExpressionBuilder_GetMetaActivityOutputCreatedDesc:"Created datetime of the file or folder",ExpressionBuilder_GetMetaActivityOutputPartitionIndexesDesc:"Partition indexes",ExpressionBuilder_ForeachIterator:"ForEach iterator",ExpressionBuilder_FilterIterator:"Filter iterator",ExpressionBuilder_CurrentItem:"Current item",ExpressionBuilder_ErrorPositionMesage:"Position {0}",ExpressionBuilder_DataPreview:"Expression data preview",ExpressionBuilder_DataPreviewFormatTooltip:"Date and timestamp types are always displayed in format yyyy-dd-MM UTC in data preview",Expression_resolution_validExpression:"Valid",Expression_resolution_invalidExpression:"Invalid",Expression_resolution_invalidType:"Invalid type, expected {0} and provided {1}",Expression_resolution_ExtraTrailingComma:"Extra trailing comma",Expression_resolution_ExtraTrailingSpace:"Expression has an extra trailing space",Expression_resolution_MissingClosingBracket:"Opening bracket has no closing bracket",Expression_resolution_MissingClosingQuote:"String start single quote has no closing quote",Expression_resolution_MissingClosingBrace:"Opening brace has no closing brace",Expression_resolution_MissingComma:"Missing comma between arguments",Expression_resolution_UnrecognizedToken:"Unrecognized token",Expression_resolution_GeneralParsingError:"Unknown parsing error",Expression_resolution_InvalidArgumentCount:"function '{0}' does not accept {1} argument(s)",Expression_resolution_ExtraTrailingDot:"Extra trailing period operator",Expression_resolution_MissingDot:"Missing period",Expression_resolution_MissingEnclosingParenthesis:"Missing enclosing parenthesis",Expression_resolution_ExtraEnclosingParenthesis:"Extra closing parenthesis",Expression_resolution_OpeningParenthesisExpected:"Opening parenthesis expected",Expression_resolution_ExtraClosingBracket:"Extra closing bracket",Expression_resolution_InvalidArrayProperty:"Array property should end with a closing bracket",Expression_resolution_NullSafeOperatorIncomplete:"Null safe operator must be followed by a dot",Expression_resolution_ParameterNotFound:"Parameter {0} was not found under {1}",Expression_resolution_ParameterNotSpecified:"Missing parameter name",Expression_resolution_FunctionNotFound:"'{0}' is not a recognized function",Expression_resolution_SystemVariableNotFound:"'{0}' is not a supported system variable",Expression_resolution_SystemVariableNestedPipelineWarning:"'{0}' is available only when pipeline runs are kicked off by an Execute Pipeline activity. Otherwise, the pipeline will fail with an expression evaluation error.",Expression_resolution_NoMatchingArguments:"'{0}' does not have an overload that supports the arguments given",Expression_resolution_ExpressionContainsWhiteSpace:"The expression has leading spaces, which causes the expression to be treated as a string literal.",Expression_resolution_EmptyExpression:"Please specify an expression",Expression_resolution_SingleStringLiteralError:"Only a single string literal argument is allowed inside the class clause",Expression_resolution_MissingActivityName:"Please specify an activity name inside the activity clause",Expression_resolution_MissingVariableName:"Please specify a variable name inside the variable clause",Expression_resolution_ActivityNotReferencable:"The output of activity '{0}' can't be referenced since it is either not an ancestor to the current activity or does not exist",Expression_resolution_ActivityFoundNoOutput:"The output of activity '{0}' can't be referenced since it has no output.",Expression_resolution_VariableNotReferencable:"The output of variable '{0}' can't be referenced since it is not a variable of the current pipeline.",Expression_resolution_SelfReferencingError:"The expression contains self referencing variable. A variable cannot reference itself in the expression.",Expression_resolution_ItemsSyntaxNotApplicable:"@item() syntax can only be used in the filter activity or child activities of a foreach activity",Expression_resolution_EmptyInterpolationExpression:"@{} expressions must contain an expression",Expression_resolution_PropertyNotFound:"'{0}' is not a property of its parent",Expression_resolution_ExcesssNestedProperties:"'{0}' is a primitive and doesn't support nested properties",Expression_resolution_UseBuiltInArithmetic:"Please use math functions (e.g. 'add') rather than symbol operators (e.g. '+')",Expression_resolution_InvalidSyntax:"Syntax error",Expression_resolution_Unrecognized:"Unrecognized expression",Expression_resolution_UnrecognizedType:"Unrecognized {0} type: '{1}'",Expression_resolution_NotAnArray:"'{0}' is not an array",Expression_resolution_TypeMismatch:"Expression of type: '{0}' does not match the field: '{1}'",Expression_resolution_ExpectedSingleType:"{0}.\nExpected type is '{1}'.",Expression_resolution_ExpectedMultiTypes:"{0}.\nExpected type is '{1}' or '{2}'.",Expression_resolution_Array_Subscript:"Array subscript",Expression_resolution_ExpressionNotApplicable:"This expression can't be applied the given entity.",Expression_resolution_DatasetParameterInaccessible:"Dataset parameters can only be used within their defining dataset",Expression_resolution_PipelineParameterInaccessible:"Pipeline parameters can only be used within their defining pipeline. Define dataset parameters to accept the pipeline parameter values instead.",Expression_resolution_LSPipelineParameterInaccessible:"Pipeline parameters can only be used within their defining pipeline. Define linked service parameters to accept the pipeline parameter values instead.",Expression_resolution_VariablesInaccessible:"Variables can only be used within their defining pipeline.",Expression_Function_ByPosition_ValidationMessage:"The argument provided must be a positive integer between 1 and the number of columns in the stream",Expression_Function_RowMatch_ValidationMessage:"Row matching functions accept a stream index argument, which must be either 1 or 2",SeeMore:"See more",SeeLess:"See less",DynamicContent:"Dynamic content",AddDynamicContent:"Add dynamic content",removeExpression:"remove dynamic content",Parameterization_ValidIntegerErrorMessage:"parameter value should be a valid integer",Parameterization_ValidFloatErrorMessage:"parameter value should be a valid float",Parameterization_ValidBooleanErrorMessage:"parameter value should be a valid boolean",Parameterization_ValidArrayErrorMessage:"parameter value should be a valid array",addDynamicContentAltShiftD:"Add dynamic content [Alt+Shift+D]",addAkv:"Add Azure Key Vault",SelectFactoryTitle:"Select an existing data factory",SelectFactoryDescription:"Microsoft Azure Data Factory is a cloud-based data integration service that automates data movement and transformation.",SelectFactoryNotAccessible:"Azure Data Factory {0} is not accessible",SelectFactory404:"This data factory was not found, it may have been deleted. {0}",SelectFactory403:"You do not have authorization to see this data factory. {0}",SelectFactoryError:"Failed to retrieve this data factory. {0}",FactoryIncludeInArmTemplate:"Include in ARM template",FactoryGlobalParameterInArmDescription:"Including global parameters in the ARM template will not include other factory level settings such as tags, git configuration, and Purview connection. These settings will be removed from the factory when the ARM template is deployed.",FactoryIncludeGlobalParametersInArmTemplate:"Include global parameters in ARM template",FactoryIncludeInArmTemplate_Deprecated:"Include in ARM template (deprecated)",FactoryGlobalParameterInArmDescription_Deprecated:"We have deprecated this option to improve the CICD experience when dealing with global parameters. To include global parameters in the ARM template, please go to 'ARM template' in Manage hub",FactoryGlobalParameterInArmAsEntityDescription:"Global parameters will be included in the ARM template. Please select this if you wish to add global parameters to your CICD deployment",FactoryGlobalParameterValueError:"Global Parameter with the name {0} is missing required property value.",FactoryGlobalParameterSecureStringError:"Secure String is not allowed for Global Parameters",FactoryGlobalParametersImportDescription:"Import global parameters from published data factory",FactoryGlobalParameter_ScopeRestriction:"To use 'DataLakeAnalyticsScope' activity please add the following Global Parameter to your Data Factory: Name: 'ADFScopeSupport' with Type: 'Bool' and value: true. Doing this will allow you to use 'DataLakeAnalyticsScope' activity but <strong> will restrict access to your factory to be from Microsoft IP Addresses only.</strong>",FactoryGlobalParameter_IPRestriction:"Setting 'ADFScopeSupport' to true will cause all access to your factory to be restricted from Microsoft IP Addresses only.",FactoryGlobalParameter_AddScopeSupport:"Click here to add 'ADFScopeSupport' to your factory.",FactoryGlobalParameter_CharacterValidation:'Character "{0}" is not allowed.',FactoryGlobalParamInArm_Exclude_ConfirmTitle:"Exclude global parameters from ARM template?",FactoryGlobalParamInArm_Include_ConfirmTitle:"Include global parameters in ARM template?",FactoryInArmDeprecated_ConfirmMessage:"This option has been deprecated. After it is unchecked, you will not be allowed to select it again. Are you sure you want to continue?",FactoryGlobalParamInArm_RerouteMessage:"We have moved this option to ARM template settings to improve the CICD experience for global parameters. If you wish to import global parameters, please go to ARM template settings.",FactoryInArmDeprecated_Tooltip:"This option has been deprecated",ArmTemplateSettings:"ARM template settings",FactoryCmkInArmDescription:"Including customer managed key configuration in the ARM template will include other factory level settings which will be overridden when you deploy the ARM template.",FactoryCmkInArm_ConfirmTitle:"Exclude customer managed key from ARM template?",ConfigurationLiveModeUnavailable:"Unavailable. This configuration is only available in Git mode.",GlobalParamInArm_ConfirmTitle:"Include global params in ARM?",GlobalParamInArm_ConfirmMessage:"You have selected 'Include in ARM template' in the global parameter/customer-managed key settings, which is deprecated. Selecting the new option will overwrite those settings. You may need to do one-time migration, which involves.",GlobalParamInArm_CmkWarning:"Once selected, customer managed key will no longer be included in ARM template",GlobalParamInArm_ParameterizationUpdate:"Update the parameterization definition file if using a custom definition file.",GlobalParamInArm_OverridesUpdate:"Update the release pipeline if overriding template parameters.",GlobalParamInArm_ConfirmContinue:"Are you sure you want to continue?",GlobalParamInArm_OldTemplateParameter:"Old template parameter",GlobalParamInArm_NewTemplateParameter:"New template parameter",GlobalParamInArm_TemplateParameter:"Template parameter changes",WorkspaceName:"Workspace name",WorkspaceUsers:"Workspace users",SelectWorkspaceTitle:"Select workspace",SelectAnotherWorkspaceLabel:"Select another workspace",WorkspaceNeedPermissionTemplate:"You need permission to access workspace {0}",WorkspaceNeedPermissionDescription:"Contact the workspace owner or administrators for access.",SocWorkspaceNeedPermissionTemplate:"You need to connect to CorpNet.",SocWorkspaceNeedPermissionDescription:"Please reconnect with CorpNet for access.",SelectWorkspaceDescription:"Azure Synapse Analytics is a \u200blimitless cloud data warehouse with unmatched time-to-insight\u200b.",SelectWorkspaceNotAccessible:"Workspace {0} is not accessible",SelectWorkspace404:"This workspace was not found, it may have been deleted. {0}",SelectWorkspace403:"You do not have authorization to see this workspace. {0}",SelectWorkspaceError:"Failed to retrieve this workspace. {0}",WorkspacePackages:"Workspace packages",WorkspacePackagesDescription:"Upload custom packages to your workspace that can be used by your Spark pools.",WorkspaceProvisionInProgress:"Workspace {0} is being provisioned. Please try again later.",WorkspaceProvisionInvalid:"Workspace {0} is not in a valid state ('{1}').",GetSparkHistoryEndpointNotAccessible:"Spark history endpoint can not be accessed for this workspace.",GetSparkHistoryEndpoint401:"You do not have authorization to access the spark history endpoint for this workspace. {0}",TabOptions:"Tab Options",TabCannotBeClosed:"Tab cannot be closed",CanvasEnterToControl:"Canvas component, press enter or space to navigate inside",PropertiesEnterToControl:"Properties component, press enter or space to navigate inside",DetailsEnterToControl:"Details component, press enter or space to navigate inside",PipelineCanvas_ZoomIn:"Zoom in",PipelineCanvas_ZoomOut:"Zoom out",PipelineCanvas_ZoomToFit:"Zoom to fit",PipelineCanvas_ZoomToFitTooltip:"Change the size of your graph nodes to fit inside of the current view pane",PipelineCanvas_ZoomInShortcut:"I",PipelineCanvas_ZoomOutShortcut:"O",PipelineCanvas_ZoomToFitShortcut:"F",PipelineCanvas_NestedActivitiesShortcut:"N",PipelineCanvas_LockCanvas:"Lock canvas",PipelineCanvas_UnlockCanvas:"Unlock canvas",PipelineCanvas_ZoomLevel:"Zoom level",PipelineCanvas_ResetZoomLevel:"Reset zoom level",PipelineCanvas_TurnOffMultiSelect:"Turn off multi select",PipelineCanvas_TurnOnMultiSelect:"Turn on multi select",DataflowCanvas_TurnOffReferenceNodes:"Hide reference nodes",DataflowCanvas_TurnOnReferenceNodes:"Show reference nodes",PipelineCanvas_ShowNestedActivities:"Show nested activities by default",PipelineCanvas_HideNestedActivities:"Hide nested activities by default",PipelineCanvas_AutoAlign:"Auto align",PipelineCanvas_ShowLineage:"Show lineage",PipelineCanvas_ChangeLinkTypeTo:"Change To:",PipelineCanvas_EditResource:"Edit {0}",Canvas_Controls:"Canvas controls",PipelineCanvas_Container_TeachingCallout_Title:"New container design",PipelineCanvas_Container_TeachingCallout_Description_Page1:"You can now add nested activities on the parent activity itself. Click on the nested activity to configure its settings.",PipelineCanvas_Container_TeachingCallout_Description_Page2:"To change the default collapse state of container activities, right click on the canvas or use the button on the bottom righthand side of the canvas.",PipelineCanvas_TeachingCallout_Title:"Add a connecting activity",PipelineCanvas_TeachingCallout_Description:"With the activity menu, you can add a connecting activity which will be executed on success.",Rerun_TeachingCallout_Title:"Rerun updates",Rerun_TeachingCallout_Description:"All rerun options have been condensed into a single button.",PipelineCanvas_ToolboxSearchPlaceholder:"Search activities",PipelineCanvas_ReplaceDataMapping_Confirm_Title:"Are you sure you want to change the referenced data flow from <{0}> to <{1}>?",Pipelines:"Pipelines",Pipeline:"Pipeline",PipelineRun:"Pipeline run",PipelineNotFound:"Pipeline is not found, please publish first",NoParametersFound:"Click new to start adding new parameters",NoAnnotationsFound:"Click new to start adding new annotations",InvalidValue:"Invalid value",DynamicAuthenticationTypeTooltip:"You must use service principal authentication if tenant and user is configured in Azure Active Directory for conditional access and/or Multi-Factor Authentication is required.",Pipeline_ConcurrencyDescription:"The number of simultaneous pipeline runs that are fired",Pipeline_ElapsedTimeMetricChkBoxLabel:"Emit a metric after specified duration",Pipeline_ElapsedTimeMetricDescription:"This will emit a metric in Azure Monitor for long running pipelines beyond the specified time span. Format is in D.HH:MM:SS",Pipeline_ElapsedTimeMetricFormattingError:"Elapsed time metric duration is invalid. The duration must be formatted as D.HH:MM:SS and be greater than 10 minutes",Pipeline_ElapsedTimeMetricLabel:"Elapsed time metric",Pipeline_ElapsedTimeMetricPlaceholder:"0.00:10:00",Pipeline_CancelAfterChkBoxLabel:"Cancel pipeline run after specified duration",Pipeline_CancelAfterDescription:"This will cancel the pipeline after specified time span. Format is in D.HH:MM:SS",Pipeline_CancelAfterFormattingError:"Cancel After duration is invalid. The duration must be formatted as D.HH:MM:SS and be between 10 minutes and 30 days",Pipeline_CancelAfterLabel:"Auto-cancel",Pipeline_CancelAfterPlaceholder:"30.00:00:00",Pipeline_DataflowStarting_Warning:"Data flow activity for this debug run will start as soon as the data flow debug session is ready.",Pipeline_Settings_DIU_Warning:"2 DIU for Copy activity is not supported in managed virtual network.",KeyMustBeUnique:"Key must be unique",InvalidFormat:"Invalid format",PipelineOutput:"Pipeline return value (preview)",PipelineVariables:"Pipeline variable",SetVariableTypeDescription:"Choose which type of variable you wish to set. Pipeline variables are user-defined. Pipeline return value is a system variable that you can set here then use in the parent consuming pipeline.",VariableType:"Variable type",QuoteAll_Description:"Whether all values should always be enclosed with quotes.",Recursive_Copy_Label:"Copy file recursively",Recursive_Label:"Recursively",Ftp_Use_Binary_Transfer_Label:"Use binary transfer",Recursive_Copy_Description:"Copy all files in the input folder and its subfolders recursively or just the ones in the selected folder. This setting is disabled when a single file is selected.",Recursive_Process_Description:"Process all files in the input folder and its subfolders recursively or just the ones in the selected folder. This setting is disabled when a single file is selected.",SFTP_UseTempFileRename_Label:"Upload with temp file",SFTP_UseTempFileRename_Desc:"Upload to temporary file(s) and rename. Disable this option if your SFTP server doesn't support rename operation.",SFTP_DisableChunking_Desc:"The chunking is designed to optimize the performance and happens underneath. This option allows you to disable chunking within each file.",SFTP_PublicKeyPath_AzureIR_Error:"Public key path is not allowed when using Azure integration runtime.",DeleteFilesAfterCompletion_Label:"Delete files after completion",Recursive_Delete_Label:"Delete file recursively",Recursive_Delete_Description:"Delete all files in the input folder and its subfolders recursively or just the ones in the selected folder. This setting is disabled when a single file is selected.",Delete_Activity_Dataset_Description:"Supported dataset type: Azure blob, Amazon S3, Azure Data lake Storage Gen1, Azure Data lake Storage Gen2, File Share associated with FTP, SFTP and File System linked service.",DeleteFromRoot_WarningMessage:"Please note that you are not specifying folderPath and fileName in {0}, all the files under root folder will be deleted.",Trident_DeleteFromRoot_WarningMessage:"Please note that you are not specifying folderPath and fileName, all the files under root folder will be deleted.",DeleteActivityMaxConcurrentConnections_ErrorMessage:"Max concurrent connections value must be a number in 1-10.",QueryTimeout_ErrorMessage:"Query timeout value must be a number in 1-1440.",IntegrationRuntime_NewFeatureAlert:"NEW FEATURE ALERT: You can now create ODBC driver based {0} linked service with the newest Self-hosted integration runtime, ",IntegrationRuntime_UpgradeToTryNewFeature_Suffix:" to try it out!",Please_Make_Sure:"Please make sure the ",IntegrationRuntime_Installed_Suffix:" is installed on the Self-hosted integration runtime (IR) machine. ",SapTable_TableName_Description:"If your SAP table has a large volume of data, such as several billion rows, use partitionOption and partitionSetting in copy source to split the data into smaller partitions. In this case, the data is read per partition, and each data partition is retrieved from your SAP server via a single RFC call.",Click:"Click",To_See_More_Details:" to see more details.",Db2_New_Properties_Required_IR:'To utilize "Package collection" and "Certificate common name", the Self-hosted integration runtime (IR) must be newer than version {0}.',New_Auth_Required_IR:"To utilize the current authentication type, the Self-hosted integration runtime (IR) must be newer than version {0}.",New_ServerVersion_CosmosMongo_IR:"To utilize the new server version, the Self-hosted integration runtime (IR) must be newer than version {0}.",New_ConnectionString_Required_IR:"To utilize connection string, the Self-hosted integration runtime (IR) must be newer than version {0}.",MultiCharDelimiter_Required_IR:"To utilize multiple column delimiter, the Self-hosted integration runtime (IR) must be newer than version {0}.",HDFS_HttpFs_Required_IR:"To utilize HttpFs server type, the Self-hosted integration runtime (IR) must be newer than version {0}.",SelfHostedIRValidationError:"{0} in {1} is not supported with the Self-hosted integration runtime (IR) version lower than {2}. Please ",SHIROutdatedVersionError:"{0} in {1} is not supported with the Self-hosted integration runtime (IR) version lower than {2}. Please upgrade to the latest version.",SapHanaSelfHostedIRValidationWarning:"New features are enabled in SAP HANA linked service, upgrade the Self-hosted integration runtime (IR) to try it out!",ModelName_SapHanaSourcePacketSize:"Packet size (KB)",SapHanaSourcePacketSize_Description:"The network packet size (in Kilobytes) to split request into multiple blocks. The default value is 2048 (2MB).",SapHanaSourcePacketSize_ErrorMessage:"The packet size must be a number from 1024 (1 MB) to 2097152 (2 GB).",TheOpteration_Label:"The operation",Salesforce_SecurityToken_IR_Warning:"Please note that optional security token and api version are only supported when Self-hosted integration runtime version is equal or higher than {0}, ",ModelName_IsolationLevelDescription:"Specifies the transaction locking behavior for the SQL source. The allowed values are: ReadCommitted, ReadUncommitted, RepeatableRead, Serializable, Snapshot.",ModelName_Snapshot:"Snapshot",Snapshot_Description:"Specify snapshot of the file share.",DataFlowCanvas_AddingDataFlowTitle:"Adding data flow",DataFlowCanvas_CreateNewDataFlow:"Create new data flow",DataFlowCanvas_UseExistingDataFlow:"Use existing data flow",DataFlowCanvas_NewDataFlowNameLabel:"Data flow name",DataFlowCanvas_NewDataFlowNamePlaceholder:"Enter data flow name...",DataFlowCanvas_ExistingDataFlowNameLabel:"Existing data flow",DataFlowCanvas_ExistingDataFlowNamePlaceholder:"Select data flow...",Dataflow_Snowflake_Staging_Description:"If your transfer is likely to take 36 hours or more, please enable staging to use external transfer.",DataFlowMonitoring_SinkPanelRowsLabel:"Rows",DataFlowMonitoring_SinkPanelRowsReadLabel:"Rows Read",DataFlowMonitoring_SinkPanelRowsWrittenLabel:"Rows Written",DataFlowMonitoring_SinkPanelStageId:"Stage Id",DataFlowMonitoring_SinkPanelStagesPaneTooltipDescription:"Stage level metrics can assist in giving certain level of information in cases where monitoring stats collection is disabled for performance reasons. Eg. in cases when Logging level is set to None. These are unpolished stats given directly from the underlying execution engine.",SchemaDriftOptions:"Schema drift options",SchemaOptions:"Schema options",DataFlowDatasetNotSupported:"{0} datasets cannot be used in data flows, but you can use the 'Inline' source option to define a data flow {0} source.",DataFlowSourcePanelReadMode:"Read mode",DataFlowSourcePanelStoreType:"Store type",DataFlowSourcePanelStoreTypeDescription:'Select the type of Cosmos DB. Using Analytical store, you can save RUs and there is no performance impact on transactional workloads. <a target="_blank" href="https://learn.microsoft.com/azure/cosmos-db/analytical-store-introduction#column-oriented-analytical-store">Learn more</a>',DataFlowSourcePanelAnalyticalTypeDisableTooltip:"Dataset mode doesn't support analytical store. Switch to inline mode to use analytical store as source.",DataFlowSourcePanelSampling:"Sampling",DataFlowSourcePanelSamplingDescription:"Use Sampling to limit the number of rows from your Source. This is useful when you need just a sample of your source data for testing and debugging purposes.",DataFlowSourcePanelRowsLimit:"Rows limit",DataFlowSourcePanelAllowSchemaDrift:"Allow schema drift",DataFlowSourcePanelUseProjectedSchema:"Use projected schema",DataFlowSourcePanelEmptySchemaSchemaDrift:"No columns exist in the schema, but columns may still be read with schema drift enabled in schema options.",DataFlowSourcePanelDeltaOptionsLabel:"Delta options",DataFlowSourcePanelDeltaTruncateDescription:"Enable truncate table option to clear destination folder before writing new data.",DataFlowSourcePanelDeltaOverwriteDescription:"Enable overwrite table option to overwrite the existing data and schema in the table using the new values",DataFlowSourcePanelDeltaMergeSchemaDescription:"When merge schema option is enabled, any columns that are present in the previous stream but not in the Delta table are automatically added on to the end of the schema.",DataFlowSourcePanelDeltaVacuum:"Vacuum",DataflowSourcePanelDeltaArrayOfValues:"Array of values",DataflowSourcePanelDeltaPartitionPruning:"Partition Pruning",DataflowSourcePanelDeltaPartitionPruningDescription:"Optimize Delta merge query by limiting the number of partitions that are inspected. Only partitions satisfying this condition will be fetched from target store.",DataFlowSourcePanelDeltaVacuumDescription:"Remove files no longer referenced by a Delta table and are older than this retention threshold in hours. Retention threshold will be 30 days if leaving this value as empty or 0.",DataFlowInvalidDeltaVacuum:"Delta vacuum threshold must be an integer",DataFlowSourcePanelDeltaTimeTravel:"Time travel",DataFlowSourcePanelDeltaTimeTravelByTimestamp:"Query by timestamp",DataFlowSourcePanelDeltaTimeTravelByTimestampDescription:"Use timestamp to query old snapshot.",DataFlowSourcePanelDeltaTimeTravelByVersion:"Query by version",DataFlowSourcePanelDeltaTimeTravelByVersionDescription:"Use version number to query old snapshot. The version of initial Delta table is 0.",DataFlowSourcePanelDeltaTimeTravelDescription:"Enable Delta Lake time travel via version or timestamp to query an older snapshot of a Delta table.",DataFlowSourcePanelDeltaDescriptionTimeTravelError:"Time travel value needs to be specified",DataFlowSourcePanelDeltaAutoOptimizeWarning:"You have delta sinks with different 'Auto compact' and 'Optimize write' settings. These settings only apply at the Data Flow job level instead of individual sinks. Please set the same settings for all the delta sinks.",DataFlowSourcePanelDeltaAutoOptimizeWarningFix:"Normalize settings",DataFlowSourcePanelDeltaAutoCompact:"Auto compact",DataFlowSourcePanelDeltaAutoCompactDescription:"After an individual write, checks if files can further be compacted, and runs a quick OPTIMIZE job (with 128 MB file sizes instead of 1GB) to further compact files for partitions that have the most number of small files.",DataFlowSourcePanelDeltaOptimizedWrite:"Optimize write",DataFlowSourcePanelDeltaOptimizedWriteDescription:"Dynamically optimize partition sizes based on the actual data by attempting to write out 128 MB files for each table partition. This is an approximate size and can vary depending on dataset characteristics.",DataFlowSourcePanelAllowSchemaDriftDescription:"Select Allow Schema Drift if the source columns will change often. This setting will allow all incoming fields from your source to flow through the transformations to the Sink.",DataFlowSourcePanelUseSchemaDescription:"Use the schema defined in the projection to read the source files",DataFlowSourcePanelUseSchemaDisabledTooltip:"Projected schema needs to be specified before enabling 'Use projected schema'",DataFlowSourceUseSchemaViolation:"Use projected schema cannot be used in conjunction with 'Allow schema drift', 'Validate schema', or 'Infer drifted column types'",DataFlowSourceUseSchemaEmptyViolation:"Use projected schema cannot be used when schema is empty",DataFlowSourcePanelAdobeCampaign:"Adobe Campaign",DataFlowSourcePanelAdobeIOEvents:"Adobe I/O Events",DataFlowSourcePanelAdobePrivacyService:"Adobe Privacy Service",DataFlowSourcePanelCommonDataModel:"Common Data Model",DataFlowSourcePanelCommonDataModelDescription:"Common data model description",DataFlowSourcePanelInferDriftedColumnTypes:"Infer drifted column types",DataFlowSourcePanelInferDriftedColumnTypesDescription:"Allow auto-detection of drifted column types",DataFlowSourcePanelValidateSchema:"Validate schema",DataFlowSourcePanelValidateSchemaDescription:"Setting this option will cause data flow to fail if any column and type defined in the projection does not match the discovered schema of the source data.",DataFlowSourcePanelValidateSchemaConstraint:"Should not be selected with 'Allow schema drift'",DataFlowSourcePanelValidateSchemaWarning:"Should not be selected with 'Enable staging'",DataFlowSourcePanelEmptyQuery:"You must specify a query for your data. Otherwise, use the 'Table' option",DataFlowCosmosChangeFeedError:"Enabling both schema drift and inferring drifted column types is not supported when Cosmos change feed is enabled",DataFlowSinkPanelCache:"Cache",DataFlowSinkPanelCacheOutputLabel:"Write to activity output",DataFlowSinkPanelCacheOutputDescription:"Specify if output is expected for this sink",DataflowXsdShouldWorkWithNamespace:"Namespace should be enabled when validation mode is 'xsd'.",DataFlowSourceSettingsMultiLineRow:"Multiline rows",DataFlowSourceSettingsMultiLineRowDescription:"The source file contains rows that span multiple lines (multiline values must be in quotes)",DataFlowSourceSettingsMaximumColumnsDescription:"Specify the maximum number of columns. Default is 20480.",DataFlowSourceSettingsListOfFiles:"List of files",DataFlowSourceSettingsModifiedAfter:"Modified After",DataFlowSourceSettingsModifiedBefore:"Modified Before",DataFlowSourceSettingsListOfFilesDescription:"Point to a text file that lists each file (relative to the container's root path) that you wish to process.",DataFlowSourceSettingsJsonAccordionHeader:"JSON settings",DataFlowSourceSettingsDelimitedTextAccordionHeader:"Delimited text settings",DataFlowSourceSettingsXmlAccordionHeader:"XML settings",DataFlowSourceSettingsCorpusAccordionHeader:"Corpus settings",DataFlowSourceSettingsEntityReferenceAccordionHeader:"Entity reference",DataFlowSourceSettingsEntityReferenceType:"Entity reference type",DataFlowSourceSettingsCdmFileAccordionHeader:"File settings",DataFlowSourceSettingsOtherSettingsAccordionHeader:"Other settings",DataFlowSourceSettingsDocumentFormLabel:"Document form",DataFlowSourceSettingsDocumentFormDescription:"Choose whether your files contain single JSON, multiple JSON in separate lines, or in array format.",DataFlowSourceSettingsDocumentFormSingleLabel:"Single document",DataFlowSourceSettingsDocumentFormPerLineLabel:"Document per line",DataFlowSourceSettingsDocumentFormArrayLabel:"Array of documents",DataFlowSourceSettingsUnquotedColumnNames:"Unquoted column names",DataFlowSourceSettingsUnquotedColumnNamesDescription:"Column names in the JSON documents are not surrounded by quotes.",DataFlowSourceSettingsHasComments:"Has comments",DataFlowSourceSettingsHasCommentsDescription:"The JSON document has C/C++ style commenting.",DataFlowSourceSettingsSingleQuoted:"Single quoted",DataFlowSourceSettingsSingleQuotedDescription:"JSON fields and values are single quoted.",DataFlowSourceSettingsBackslashEscaped:"Backslash escaped",DataFlowSourceSettingsBackslashEscapedDescription:"A backslash is used to escape characters in the JSON.",DataFlow_DatasetFileNameOptionDefaultDescription:"Allow Spark to name files based on PART defaults.",DataFlow_DatasetFileNameOptionDescription:"Enter a file name matching pattern. [n] is optional and will be replaced with the partition number.",DataFlow_DatasetFileNameOptionPerPartitionDescription:"Enter one file name per partition.",DataFlowSourceSettingsValidationMode:"Validation mode",DataFlowSourceSettingsValidationModeDescription:"Specifies whether to validate the XML schema.",DataFlowSourceSettingsNamepsacePrefixes:"Namespace prefix pairs",DataFlowSourceSettingsNamepsacePrefixesDescription:"Namespace URI to prefix mapping, which is used to name fields when parsing the xml file.",DataFlowSourceSettingsNamespaces:"Namespaces",DataFlowSourceSettingsNamespacesDescription:"Whether to enable namespace when parsing the XML files.",DataFlowSourceSettingsAfterCompletion:"After completion",DataFlowSourceSettingsNoAction:"No action",DataFlowSourceSettingsDeleteSourceFiles:"Delete source files",DataFlowSourceSettingsAllowNoFilesFound:"Allow no files found",DataFlowSourceSettingsAllowNoFilesFoundDescription:"Do not throw an error if no source files match the configurations set by fields such as referencing specific file names, wildcard paths, or modified date filters.",DataIntegrationUnitLabel:"Data integration unit",DataIntegrationUnitLabel_Maximum:"Maximum data integration unit",DataIntegrationUnitPriceCharged:"You will be charged",DataIntegrationUnitPriceDetails:" # of used DIUs * copy duration * $0.25/DIU-hour. ",DataIntegrationUnitPriceComplement:"Local currency and separate discounting may apply per subscription type. ",DataflowPanelDataVisibility:"Transaction Commit",DataflowPanelDataVisibilityDescription:"Choose whether the data flow will be written in a single transaction or in batches ",DataflowPanelDataVisibilityComplete:"Single",DataFlowPanelFaultToleranceSettingsLabel:"Fault tolerance",DataflowPanelEnableErrorRowHandlingLabel:"Error row handling",DataflowPanelEnableErrorRowHandlingDescription:"When selecting this option, you can ignore some errors occurred in the middle of writing to this sink.",DataflowPanelEnableErrorRowHandlingOptionStopOnFirst:"Stop on first error (default)",DataflowPanelEnableErrorRowHandlingOptionHandleAllErrors:"Continue on error",DataflowPanelEnableErrorRowHandlingReportSuccessOnError:"Report success on error",DataflowPanelEnableErrorRowHandlingReportSuccessOnErrorDescription:"Mark the data flow as successful regardless of whether error rows are present.",DataflowPanelEnableErrorRowHandlingRows:"SQL error rows",DataflowPanelEnableErrorRowHandlingRowsDescription:"Settings for when a row causes an error while being written to a SQL database.",DataFlowSqlErrorStoragePath:"SQL error storage path",DataFlowAssertFailureStoragePath:"Assert failure storage path",DataflowPanelAssertFailureRows:"Assert failure rows",DataflowPanelAssertFailureRowsDescription:"Settings for failed assert rows.",DataflowPanelOutputAssertFailedRowsToAltStorage:"Output to separate file",DataflowPanelOutputAssertFailedRowsToAltStorageDescription:"Store the failed assertion rows to a file in a linked service.",DataflowPanelOutputAssertFailedRowsToSink:"Output to sink",DataflowPanelOutputAssertFailedRowsToSinkDescription:"Ignore rows that failed validation in an assert transformation when writing to this sink.",DataflowPanelEnableErrorRowHandlingLinkedServiceHeaderDescription:"Store the error rows to a file in a linked service.",DataflowPanelEnableErrorRowHandlingLinkedServiceDescription:"The Azure Storage linked service to store the log that contains the skipped rows.",DataFlowPanelIncompatibleRowLoggingLinkedServiceLabel:"Storage connection name",DataFlow_Suggestions:"Suggestions",DataFlow_ViewAllError:"View all errors in data flow validation output",DataFlow_SkipIncompatibleRows:"Skip incompatible rows",DataFlow_SkipAndLogIncompatibleRows:"Skip and log incompatible rows",DataFlow_AbortActivityOnFirstIncompatibleRow:"Abort activity on first incompatible row",DataFlow_ShouldBeIntegerLiteral:"Should be integer literal",DataFlow_ShouldReturnBoolean:"Conditional expression should return boolean type value",DataFlow_ShouldReturnSameType:"Expression should return the same type '{0}' as previous expressions.",DataFlow_ArgumentsReturnSameType:"Expression arguments should be the same type.",DataFlow_AtFunctionArgumentsTypeViolation:"At function takes an array or a map for the first parameter.",DataFlow_AtFunctionSecondParameterViolationArray:"Second parameter of at function must take an integer if the first parameter is an array.",DataFlow_AtFunctionSecondParameterViolationMap:"Second parameter of at function must take the map key type if the first parameter is a map.",DataFlow_AssociateFunctionParameterCountViolation:"Associate function must take a non-zero even number of parameters.",DataFlow_AssociateFunctionParameterTypeMatchViolation:"Type of keys and type of values must respectively match for associate function.",DataFlow_FlattenFunctionParameterTypeViolation:"Flatten must take at least one array as an argument, with an optional boolean as the last argument.",DataFlowProjectionPanel_Projection:"Projection",DataFlowTransientColumnDescription:"Locals are temporary columns that only exist within the current transformation. They can be used in expressions for normal columns or other locals, and they will not appear in your output schema. Locals are evaluated in order and before normal columns.",CastTransformationErrorsLabel:"Assert type check",CastTransformationErrorsDescription:"Mark rows where type casting fails with type check error",DataFlowWorkspaceDatabases:"Workspace DB",DataFlowDeltaUpdateOptimizeSettings:"Update optimize settings",DataFlowDeltaUpdateOptimizeSettingsDescription:"Optimize settings apply at the Data Flow job level instead of individual sinks. This change will be applied to all your delta sink settings. Do you want to proceed?",DataFlowSuggestion_ChooseDifferentColumn:"Choose different column from incoming transformations with the same data type: ",DataFlowSuggestion_ChangeDataType:"Change the data type in the incoming transformations: ",DataFlowSuggestion_CheckSpelling:"Check the spelling for function and column name.",CopyActitivy_Details:"Copy data details",CopyActivity_FaultTolerance_StorageFolderPath:"Incompatible rows logging path",CopyActivity_FaultTolerance_EnableSkipRows:"Enable skipping incompatible rows",CopyActivity_FaultTolerance_EnableSkipInconsistentFiles:"Enable skipping inconsistent files",CopyActivity_FaultTolerance_EnableSkipMissingFiles:"Enable skipping missing files",CopyActivity_FaultTolerance_EnableSkipForbiddenFiles:"Enable skipping forbidden files",CopyActivity_FaultTolerance_EnableSkipFilesWithInvalidNames:"Enable skipping files with invalid names",CopyActivity_FaultTolerance_SkipMissingFilesLabel:"Skip missing files",CopyActivity_FaultTolerance_SkipForbiddenFilesLabel:"Skip forbidden files",CopyActivity_FaultTolerance_SkipFilesWithInvalidNamesLabel:"Skip files with invalid names",CopyActivity_FaultTolerance_SkipRowsDescription:"Skip the incompatible rows between source and target store. e.g. type and field mismatch or PK violation.",CopyActivity_FaultTolerance_SkipMissingFilesDescription:"Skip the files if it is being deleted from source store during the data movement.",CopyActivity_FaultTolerance_SkipForbiddenFilesDescription:"Skip the files which ADF does not have permission to access.",CopyActivity_FaultTolerance_LinkedServiceDescription:"The linked service of Azure Blob Storage or Azure Data Lake Storage Gen1 to store the log that contains the skipped rows.",CopyActivity_FaultTolerance_StorageFolderPathDescription:"The path of the log file that contains the skipped rows.",CopyActivity_FaultTolerance_Description:"When selecting this option, you can ignore some errors occurred in the middle of copy process. E.g. incompatible rows between source and destination store, file being deleted during data movement etc.",CopyActivity_Log_Path:"Copy activity logging path",CopyActivity_LoggingSettings_EnableDescription:"When selecting this option, you can log copied files, skipped files and rows",CopyActivity_LoggingSettings_LinkedServiceDescription:"The linked service of Azure Blob Storage or Azure Data Lake Storage Gen2 to store the log of copy activity jobs. Please note that blob storage accounts with hierarchical namespaces enabled are not supported in logging settings.",CopyActivity_LoggingSettings_StorageFolderPathDescription:"The path of the log files.",CopyActivity_LoggingSettings_IRVerisonError:"Please upgrade your Integration Runtime to the latest version to use logging in copy activity.",CopyWizard_LoggingSettings_IRVerisonError:"Please upgrade your Integration Runtime to the latest version to use logging.",CopyData_Introduction_Text:"Build your data ingestion task to move objects from a data source to a data destination.",DataFlow_MoveFilesAfterCompletion_WarningMessage:"Moving files after completion will overwrite existing files and folders in your target location that have the same name as your source files and folders",Inconsistent_Data_Found:"Inconsistent data found",Inconsistent_Data_Skipped:"Inconsistent data skipped",Data_Consistent_And_Copied:"100% of data copied was verified to be consistent",Consistency_Verified:"Consistency verified",Data_Consistency_Enable_Label:"Data consistency verification",Verification_Result:"Verification result",Data_Consistency_EnableDescription:"When selecting this option, copy activity will do additional data consistency verification between source and destination store after data movement. The verification includes file size check and checksum verification for binary files, and row count verification for tabular data.",Fail_When_Data_Inconsistency:"Abort on failure",Fail_When_Data_Inconsistency_Description:"Copy activity will stop with error message when encountering any inconsistent objects copied from source to target during data validation check.",Skip_When_Data_Inconsistency:"Ignore and continue",Skip_When_Data_Inconsistency_Description:"Copy activity will continue to copy the rest of the data by skipping the inconsistent objects and logging if you also enable logging in copy activity.",CopyActivity_DataConsistency_IRVerisonError:"Please upgrade your Integration Runtime to the latest version to enable data consistency check in copy activity.",CopyActivity_TypeConversion_IRVerisonError:"Please upgrade your Integration Runtime to the latest version to enable type conversion check in copy activity.",Xml_ValidationMode_InCompatible_With_MaxConcurrentConnection_Error:"Validation mode must be 'none' when specifying max concurrent connection.",ReferenceErrorMessage:"The following artifacts referenced by the pipeline cannot be found.",CopyWizard_DataConsistency_IRVerisonError:"Please upgrade your Integration Runtime to the latest version to enable data consistency.",CopyWizard_DataConsistency_StagingError:"Data consistency verification cannot be enabled together with staging",CopyWizard_DataConsistency_UnloadError:"Data consistency verification cannot be enabled together with Redshift unload",CopyWizard_DataConsistency_PolybaseError:"Data consistency verification cannot be enabled together with ploybase",CopyWizard_DataConsistency_CopyCommandError:"Data consistency verification cannot be enabled together with copy command",enablePartitionDiscoveryDescription:"Specify whether to parse the partitions from the file path and add them as additional source columns.",partitionRootPathDescription:"When partition discovery is enabled, specify the absolute partition root path in order to read partitioned folders as data columns.",DataConsistency_NotSupport_Datasets:"Data consistency verification is not supproted when source/sink dataset is Snowflake or Microsoft 365",DataConsistency_NotSupport_Datasets_Delta:" or Azure Databricks Delta Lake",DataConsistency_NotSupport_ExternalCopy:"Data consistency verification is not supported when {0} is enabled",DataConsistency_NotSupport_BinaryCompression:"Data consistency verification is not supported when source/sink binary dataset has compression",DataConsistency_OnlySupport_TheseSourceFileTypeLs:"Data consistency verification is not supported when source dataset is of file type and source linked service is not Azure Blob Storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2, Amazon S3, HDFS, File System or Azure File Storage.",DataConsistency_OnlySupport_TheseSourceFileTypeLs_1:"Data consistency verification is not supported when source dataset is of file type and source linked service is not Azure Blob Storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2, Amazon S3, HDFS, File System, Azure File Storage or SharePoint Online File.",DataConsistency_OnlySupport_TheseSinkFileTypeLs:"Data consistency verification is not supported when sink dataset is of file type and sink linked service is not Azure Blob Storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2, File System or Azure File Storage.",DataConsistency_OnlySupport_Binary_PreserveHierarchy:"Data consistency verification is not supported when sink is binary dataset and copy behavior is not none or preserve hierarchy.",DataConsistency_OnlySupport_File_PreserveHierarchy_And_MergeFiles:"Data consistency verification is not supported when sink dataset is of file type and copy behavior is not none, preserve hierarchy or merge files.",DataConsistency_NotSupport_CollectionReference:"Data consistency verification is not supported when collection reference is enabled.",DataConsistency_Not_Support_SourceV1:"Data consistency verification is not supported for your source {0} dataset, please create a new dataset to enable data consistency verification.",DataConsistency_Not_Support_SinkV1:"Data consistency verification is not supported for your sink {0} dataset, please create a new dataset to enable data consistency verification.",Dataset_Creation_Title:"New dataset",Dataset_Creation_TitleSynapse:"New integration dataset",ADFDataMapperPreview_Title:"New CDC (preview)",ADFDataMapper_Advanced_Mapping_Tooltip:"Click on the corresponding Source column to build advanced column expression",ADFDataMapper_Source_Selection_Label:"Choose Your Sources",ADFDataMapper_Target_Selection_Label:"Choose Your Targets",ADFDataMapper_MicroBatch_Title:"Micro-batch",ADFDataMapper_RealTime_Description:"Enables stream data integration",ADFDataMapper_MicroBatch_Description:"Enables micro-batch of changes to be retrieved and applied",Edit_ADFDataMapper_Source_Selection_Label:"Edit Your Sources",Edit_ADFDataMapper_Target_Selection_Label:"Edit Your Targets",Violation_NoTableMappingExists:"No table mapping provided for the cdc",Violation_NoColumnMappingProvided:"No column mapping provided for source table '{0}' and target table '{1}' mapping for connection '{1}'",Violation_NoTableMappingProvided:"No table mapping provided for one or more tables for connection '{0}'",Violation_InvalidTableMapping:"No source and target table selected for table mapping for connection '{0}'",Violation_SourceNotSelectedInTableMapping:"No source table selected for table mapping with target table '{0}' for connection '{1}'",Violation_TargetNotSelectedInTableMapping:"No target table selected for table mapping with source table '{0}' for connection '{1}'",Violation_InvalidMappingType:"No mapping type specified while mapping '{0}' column for target table '{1}'",Violation_InvalidSourceColumn:"No source column specified while mapping '{0}' column for target table '{1}'",Violation_InvalidTargetColumn:"No target column specified while mapping for target table '{0}'",Violation_InvalidMappingCombination:"Invalid mappings for target table '{0}': cannot use both 'derived' and 'aggregate' transformations together. Please use any one of them.",Violation_EmptyConnectionsInfo:"{0} connections should have atleast one connection",Violation_EmptyTablesInfo:"{0} connection should have atleast one table",MapColumns:"Map Columns",SourceInSmall:"source",TargetInSmall:"target",StartStopCDCMapping:"Click to {0} the CDC",StartStopBeforePublishCDCMapping:"Please publish your changes to enable the start button.",Violation_UpdateWhileRunning:"Changes cannot be pulished in running state, please stop and try again.",Violation_InvalidTableDatasetType:"'{0}' table for {1} connection has invalid dataset type",Violation_InvalidTableDslConnectorProperties:"'{0}' table for {1} connection has invalid connector properties",Violation_InvalidConnectionDetails:"Invalid connection info present for {0} connection",Violation_InvalidPolicyDetails:"Invalid policy details specified",Violation_InvalidMapperName:"Invalid name for cdc",Violation_DuplicateSourceTableMapping:"Duplicate Mappings: source table '{0}' has been mapped with multiple target tables: '{1}' with linked service: '{2}' and '{3}' with linked service: '{4}'. A single source table cannot be mapped with multiple target tables",Violation_DuplicateTargetTableMapping:"Duplicate Mappings: target table '{0}' of linked service: '{1}' has been mapped with multiple source tables: '{2}' and '{3}'. A single target table cannot be mapped with multiple source tables",Violation_DuplicateTargetColumnMapping:"Duplicate Mappings: target column '{0}' of target table: '{1}' has been mapped with multiple source columns. A single target column cannot be mapped with multiple source columns",Violation_OrphanColumn:"Invalid column: {0} column '{1}' could not be found in the schema of {2} table: '{3}'. Please choose a valid column for mapping",Violation_InvalidColumnDataType:"Invalid data type: function '{0}' can be applied on any of these: [{1}] data type(s) but column: '{2}' for {3} table: '{4}' has '{5}' data type",Violation_InvalidComplexColumnMapping:"Invalid mapping: a column with 'complex' data type can only be mapped to another 'complex' data type column having exactly same signature. Here, source column: '{0}' has data type signature: '{1}' but target column: '{2}' has data type signature: '{3}'",Violation_InvalidTargetForComplexDataType:"Invalid mapping: '{0}' column of target table: '{1}' (type: '{2}') has a complex structure which can only be written to any of these: [{3}]. Please remove the mapping or choose a target belong to mentioned types",Violation_InvalidMethodForContinupusMapper:"Unsupported mapping method specified while mapping '{0}' column for target table '{1}': cannot use 'aggregate' transformations for real-time latency. Please update the mapping.",Attributemapping:"Attribute mapping",Attributemappings:"Attribute mappings",MappingTargetConnectionInfo:"Mapping target connection info",MappingTargetConnectionsInfo:"Mapping target connections info",ADFDataFlowChangeDataCaptureLabel:"Change data capture",ADFDataMapperChangeDataCapturePreviewLabel:"Change Data Capture (preview)",ADFDataMapperChangeDataCaptureLabel:"Change Data Capture",ADFDataMapperCDCLabel:"CDC",ADFCDC_PageReportTemplate:"Showing {0} - {1} of {2} tables ({3} selected)",ADFCDC_PageReportNewEntitiesTemplate:"Showing {0} - {1} of {2} tables (new:{3}, total:{4} tables selected)",ADFCDC_PageReportExistingEntitiesTemplate:"Showing {0} - {1} of {2} tables (existing:{3}, total:{4} tables selected)",ADFCDCPreviewWarning:'This preview feature is licensed to you as part of your <a target="_blank" href="https://azure.microsoft.com/support/legal/">Azure subscription</a>. By continuing you agree to the <a target="_blank" href="https://azure.microsoft.com/support/legal/preview-supplemental-terms/">Preview Terms</a> and <a target="_blank" href="https://privacy.microsoft.com/privacystatement">Privacy Statement.</a>',InvalidMappingsSpecified:"Invalid mappings specified",ADFDataMapperEntitiesFetchFailure:"Failed to fetch entities. Please try again",DataMapper_SelectSourceStore_Title:"Select data mapper source store type",DataMapper_SelectTargetStore_Title:"Select data mapper target store type",DataMapper_SelectSourceAndTargetStore_Title:"Select source and target stores",DataMapper_SelectSourceDataStores:"Select Source Datastores",DataMapper_SelectTargetDataStores:"Select Target Datastores",DataMapper_SKAlternateKeys_Warning:"Please note that all target table keys cannot be selected as alternate keys.",AddTable:"Add Table",SetLatency:"Set Latency",Latency:"Latency",ADF_DataMapper_Changes_Read:"Changes Read",ADF_DataMapper_Changes_Written:"Changes Written",ADF_DataMapper_Changes_Written_Cap:"Changes written",ADF_DataMapper_Realtime:"Real-time",ADF_DataMapper_EveryMinuteSchedule:"1 minute",ADF_DataMapper_QuaterHourSchedule:"15 minute",ADF_DataMapper_HalfHourSchedule:"30 minute",ADF_DataMapper_TwoHourSchedule:"2 hours",ADF_DataMapper_TwentyFourHourSchedule:"24 hours",ADF_DataMapper_CdcId:"CDC Id",SourceTable:"Source Table",TargetTable:"Target Table",ColumnsMapped:"Columns mapped",FilterByTargetTableName:"Filter by target table name",NumTotalUnmappedColumns:"{0} mappings: {1} unmapped",SelectDataSourceLabel:"Select a data store",SelectDataSource_SetPropertiesLabel:"Set properties",SelectDataSource_SelectFormatDescription:"Choose the format type of your data",ADFDataMapper_Continous_Start_Failed:"Previous Start has failed. Please retry after sometime",ViewJSONCode:"View JSON code",CreateNewDataset:"Create a new dataset",EditSelectSourceDataset:"Open select dataset",DatasetAddColumn:"Add new column",DatasetAddNewPartition:"Add new partition",DatasetRemovePartition:"Remove partition",DatasetAddNewExpression:"Add new expression",DatasetRemoveExpression:"Remove expression",DatasetAddNewWildcardExpression:"Add new wildcard path expression",DatasetRemoveWildcardExpression:"Remove wildcard path expression",Dataset_Table_UseQueryLabel:"Use query",Dataset_FileOrContainer_SelectLabel:"Select a container or file",ReadBehaviorLabel:"Read behavior",ReadBehaviorQueryLabel:"query",ReadBehaviorQueryAllLabel:"queryAll (include deleted objects)",Dataset_Sink_AutoTableCreationWith_SP:"Auto table creation is not supported when copy sink is associated with a stored procedure.",ModelName_Preferred_Regions_Placeholder:"Example: West US",ModelName_Preferred_Regions:"Preferred regions",ModelName_CaptureIntermediateUpdates:"Capture intermediate updates",ModelName_CaptureDeletes:"Capture Deletes",ModelName_CaptureTransactionalStoreTTLs:"Capture Transactional store TTLs",ModelName_Preferred_Regions_Description:"In order to take advantage of global distribution, the ordered preference list of regions will be used to perform document operations. The most optimal endpoint will be chosen by the SQL SDK to perform write and read operations.",ModelName_Partition_Names:"Partition names",ModelName_Partition_Option:"Partition option",ModelName_Partition_ColumnName:"Partition column name",ModelName_Partition_UpperBound:"Partition upper bound",ModelName_Partition_LowerBound:"Partition lower bound",ModelName_MaxPartitionsNumber:"Max partitions number",Sql_Partition_Info:"Please preview data to validate the partition settings are correct before you trigger a run or publish the pipeline.",PartitionReadViaLabel:"Partition read via",PartitionOptionType_DynamicRange:"Dynamic range",HashLabel:"Hash",PartitionOptionType_PhysicalPartitionOfTable:"Physical partitions of table",PartitionOptionType_DataSlice:"Data slice",PartitionOptionType_SapPartitionOnInt:"On Int",PartitionOptionType_SapPartitionOnCalendarYear:"On calendar year",PartitionOptionType_SapPartitionOnCalendarDate:"On calendar date",PartitionOptionType_SapPartitionOnTime:"On time",PartitionOptionType_SapPartitionOnCalendarMonth:"On calendar month",PartitionVersionValidationErrorMessage:"Partition is only supported in the lastest Integration Runtime. Please upgrade your Integration Runtime to the lastest version to use this feature.",PartitionODBCErrorMessage:"Please upgrade your Integration Runtime to the latest version and create a new linked service to use partition feature.",TeradataQueryRequiredMessage:'Query is required for old version Integration Runtime. Please upgrade your Integration Runtime to the latest version or just populate query by adding "select * " before your table name',PartitionValidation_Query_DynamicRange_ErrorMessage_Template:"Range partition parameters ({0}) should be included in queries that will be executed in parallel.",PartitionValidation_Query_SAPHANADynamicRange_ErrorMessage:"Range partition parameter (?AdfHanaDynamicRangePartitionCondition) is needed for queries that will be executed in parallel.",PartitionValidation_Query_Physical_ErrorMessage:"Physical partition parameter (?AdfTabularPartitionName) is needed for queries that will be executed in parallel.",PartitionValidation_Query_Hash_ErrorMessage:"Hash partition parameter (?AdfPartitionWhereClause) is needed for queries that will be executed in parallel.",TeradataV1ReferenceNewLinkedServiceErrorMessage:"Relational dataset can't reference the new Teradata linked service, please use a Teradata dataset.",TeradataV2ReferenceOldLinkedServiceErrorMessage:"This linked service is outdated and can't be used in Teradata dataset, please create a new Teradata linked service.",PartitionValidation_Query_DataSlice_ErrorMessage:"Partition parameter (?AdfPartitionCount, ?AdfDataSliceCondition) are needed for queries that will be executed in parallel.",ParallelCopyNotSupportsOldNetezzaDataset_ErrorMessage:"Parallel copy with partition settings is not supported for your Netezza dataset, please create a new dataset to enable parallel copy.",WriteDataIntoSingleFileWarning_MessageTemplate:"Please note that writing data into a single file from {0} may be slower than writing into a folder as multiple files.",WithPartitionMessage:" with partition option enabled",PartitionValidation_StoredProcedure_ErrorMessage:"Parallel copy with partition settings is not supported when copy source specifies Stored Procedure settings.",SQLPartitionUpperBound_Description:"The maximum value of the partition column for partition range splitting. This value is used to decide the partition stride, not for filtering the rows in table. All rows in the table or query result will be partitioned and copied.",SQLPartitionLowerBound_Description:"The minimum value of the partition column for partition range splitting. This value is used to decide the partition stride, not for filtering the rows in table. All rows in the table or query result will be partitioned and copied.",Partition_DynamicRange_Tooltip_Template_s:"When using query with parallel enabled, range partition parameters{0} are needed. Sample query: {1}",Partition_DynamicRange_Tooltip_Template:"When using query with parallel enabled, range partition parameter{0} is needed. Sample query: {1}",Partition_Physical_Tooltip_Template:"When using query with parallel enabled, physical partition parameter (?AdfTabularPartitionName) are needed. Sample query: {0}",Partition_Hash_Tooltip_Template:"When using query with parallel enabled, hash partition parameter (?AdfHashPartitionCondition) are needed. Sample query: {0}",Partition_DataSlice_Tooltip_Template:"You can get the total data slice number by running query: {0}. When using query with parallel enabled, data slice partition parameter (?AdfPartitionCount, ?AdfDataSliceCondition) are needed. Sample query: {1}",Partition_SapSql_Physical_Tooltip:"When using physical partition, ADF will auto determine the partition column and mechanism based on your physical table definition.",CopyWizard_ConnectionNameRequired:"Connection name is required.",Dataset_GeneratedFromActivityDescription:"If this dataset was generated from an activity (through script compilation), certain fields will be uneditable.",Dataset_CompressionLevel_Option_Fastest:"Fastest",Dataset_CompressionLevel_Option_Optimal:"Optimal",Dataset_CompressionType_Option_BZip2:"BZip2",Dataset_CompressionType_Option_Deflate:"Deflate",Dataset_CompressionType_Option_GZip:"GZip",Dataset_CompressionType_Option_ZipDeflate:"ZipDeflate",Dataset_CompressionType_Option_TarGZip:"TarGZip",Dataset_CompressionType_Avro_Warning:'The compression type "snappy" will be ignored when using Avro dataset as copy destination.',Dataset_FileFormat_Detecting:"Detecting...",Dataset_FileFormat_ColumnDelimiterComma:"Comma (,)",Dataset_FileFormat_ColumnDelimiterPipe:"Pipe (|)",Dataset_FileFormat_ColumnDelimiterSemicolon:"Semicolon (;)",Dataset_FileFormat_ColumnDelimiterStartOfHeading:"Start of heading (\\u0001)",NoDelimiterLabel:"No delimiter",Dataset_FileFormat_ColumnDelimiterTab:"Tab (\\t)",Dataset_FileFormat_ColumnDelimiterDescription:"The character used to separate columns in a file. You can consider to use a rare unprintable character that may not exist in your data. For example, specify \"\\u0001\", which represents Start of Heading (SOH). Only one character is allowed. The default value is comma (','). ",Dataset_FileFormat_ColumnRowDelimiterDescriptionDynamic:"Characters typed into textboxes are escaped. Ensure that you are passing in the intended value using the code editor. In order to type '\\t' into a parameter, you could copy and paste a tab from a text editor.",Dataset_FileFormat_FileFormatFieldDescription:"Format of the file.",Dataset_FileFormat_FileFormatFieldLabel:"File format",Dataset_FileFormat_FileFormatStructuredStream:"Structured stream",Dataset_FileFormat_FieldTerminatorFieldLabel:"Field terminator",Dataset_FileFormat_FieldTerminatorFieldDescription:"The column delimiter which can be multi-character or specified in hexadecimal notation\u200b.",Dataset_FileFormat_AutoDetectFormatText:"Detect text format",Dataset_FileFormat_CompressionWarning:"The compression type in previous page will be ignored for this format",Dataset_FileFormat_AutoDetectFormatDescription:"Click to detect format in case you edit the folder/file selection.",Dataset_FileFormat_JreWarning_Part1:"In addition to an Integration Runtime, JRE 8 (Java Runtime Environment) is also required on the Integration Runtime machine. Please download the appropriate version at",Dataset_FileFormat_JreWarning_Part2:"Java downloads",FileFormat_Label:"File format settings",Dataset_FileFormat_JSONPathGroupLabel:"JSON path settings",Dataset_FileFormat_QuoteCharDouble:'Double quote (")',Dataset_FileFormat_QuoteCharSingle:"Single quote (')",Dataset_FileFormat_QuoteCharNone:"No quote character",Dataset_FileFormat_EscapeCharBacklash:"Backslash (\\)",Dataset_FileFormat_EscapeCharSlash:"Slash (/)",Dataset_FileFormat_EscapeCharNone:"No escape character",Dataset_FileFormat_QuoteCharShouldBeNoneError:'Quote character should be "No quote character" when "No escape character" is set.',Dataset_FileFormat_DelimiterCanOnlyBeOneErrorRef:"{0} cannot be empty string or multi-character string when dataset is referenced in {1}.",Dataset_FileFormat_DelimiterCannotBeNoneError:"{0} {1} cannot be empty string.",Dataset_FileFormat_DelimiterCannotBeNoneErrorRef:"{0} cannot be empty string when dataset is referenced in {1}.",FileFormat_DelimiterCannotBeNoneErrorRef:"{0} cannot be empty string when it is used in {1}.",Dataset_FileFormat_DupCharWithColDelimiter:'Character "{0}" is used in {1}, hence cannot be reused in column delimiter.',Dataset_FileFormat_ColDelimiterNotSameAsNullValue:"Column delimiter cannot equal to null value.",Dataset_FileFormat_CopyCompressionNotSupport:'Compression {0} is not supported in activity "{1}"',Dataset_FileFormat_DataflowCompressionNotSupport:"Compression {0} is not supported in data flow",Dataset_FileFormat_ColumnDelimiterShouldBeNoneError:'Column delimiter must be "No delimiter" when row delimiter is "No delimiter"',Dataset_FileFormat_QuoteCharShouldNotBeNoneError:"Quote character cannot be 'No quote character' in data flow",Dataset_FileFormat_EscapeCharShouldNotBeNoneError:"Escape character cannot be 'No escape character' in data flow",Dataset_FileFormat_ColumnDelimiterShouldNotBeNoneError:"Column delimiter with 'No delimiter' in data flow is only allowed when encoding is UTF-8",Dataset_FileFormat_UTF7IsNotSupported:"UTF-7 encoding is not supported for data flow",Dataset_FileFormat_UTF8WithoutBOMIsNotSupported:"UTF-8 without BOM encoding is not supported for data flow",Dataset_FileFormat_GZipFilenameError:'Gzip compression in data flow only supports files with ".gz" extension. Please update the file name configuration or choose another compression type.',Dataset_FileFormat_RowDelimiterAutoDetect:"Default (\\r,\\n, or \\r\\n)",Dataset_FileFormat_RowDelimiterBoth:"Carriage return + line feed (\\r\\n)",Dataset_FileFormat_RowDelimiterCarriageReturn:"Carriage return (\\r)",Dataset_FileFormat_RowDelimiterFieldLabel:"Row delimiter",Dataset_FileFormat_RowTerminatorFieldLabel:"Row terminator",Dataset_FileFormat_RowTerminatorFieldDescription:"The row terminator can be multi-character or specified in hexadecimal notation.",Dataset_FileFormat_RowDelimiterLineFeed:"Line feed (\\n)",Dataset_FileFormat_RowDelimiterDescription:'The character used to separate rows in a file. Only one character is allowed. The default value is any of the following values on read: ["\\r\\n", "\\r", "\\n"] and "\\r\\n" on write. <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2185155">Learn more</a>',Dataset_FileFormat_EncodingDescription:"The character used to used to read/write test files. Note mapping data flow doesn't support UTF-7 encoding.",Dataset_FileFormat_EscapeCharDescription:"The character used to escape quotes inside a quoted value. The default value is backslash .When escapeChar is defined as empty string, the quoteChar must be set as empty string as well, in which case make sure all column values don't contain delimiters.",Dataset_FileFormat_quoteCharDescription:'The character used to quote column values if it contains column delimiter. The default value is double quotes ". When quoteChar is defined as empty string, it means there is no quote char and column value is not quoted, and escapeChar is used to escape the column delimiter and itself.',Dataset_FileFormat_firstRowAsHeaderDescription:'Specifies whether to treat the first row as a header line. <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2185155">Learn more</a>',Dataset_FileFormat_nullValueDescription:"The character used to specify the string representation of null value. The default value is empty string.",Dataset_FileFormat_UseCustomDelimiterMessage:"Use custom delimiter",Dataset_FileFormat_UseCustomEscapeCharMessage:"Use custom escape character",Dataset_FileFormat_UseCustomQuoteCharMessage:"Use custom quote character",Dataset_FileFormat_SkipHeaderLineCountFieldDescription:"Number of lines to be skipped at the top of the file",Dataset_FileFormat_UseFirstLineAsHeaderMessage:"Column names in the first row",Dataset_FileFormat_AddHeaderToFile:"Add header to file",Dataset_FileFormat_TreatEmptyAsNullLabel:"Treat empty column value as null",Dataset_FileFormat_EncodingNameFieldDescription:"Name of the preferred encoding",Dataset_FileFormat_EncodingNameFieldLabel:"Encoding name",Dataset_FileFormat_EscapeCharFieldDescription:"The character used to escape any special character in the file content. This property is optional. Only one character is allowed. No default value.",Dataset_FileFormat_EscapeCharFieldLabel:"Escape character",Dataset_FileFormat_QuoteCharFieldDescription:"The character used for quoting. This property is optional. Only one character is allowed. No default value.",Dataset_FileFormat_DateFormatFieldDescription:"Specifies how the load interprets the incoming input date field. The default is the dateformat specified at the session level.\u200b",Dataset_FileFormat_FirstRowFieldDescription:"The row number that is read first in all files for the COPY. If the value is set to two, the first row in every file (header row) is skipped during the load.\u200b",Dataset_FileFormat_FirstRowFieldPlaceHolder:"Row number",Dataset_FileFormat_FieldQuoteFieldLabel:"Field quote",Dataset_FileFormat_FieldQuoteFieldDescription:"A single character that will be used as the string delimiter.\u200b",Dataset_FileFormat_ExecuteScriptOnOpenCheckboxDescription:"Checking this box will automatically load into the SQL pool table when opening the script.",Dataset_FileFormat_QuoteCharFieldLabel:"Quote character",Dataset_FileFormat_NullValueFieldDescription:"Specifies the string representation of null value. The default value is empty string.",Dataset_FileFormat_FormatFieldDescription:"Select the format of the embedded string",NullValueFieldLabel:"Null value",MaxColumns:"Maximum columns",Dataset_FileFormat_NestingSeparatorFieldDescription:"Character that is used to separate nesting levels. The default value is . (dot)",NestingSeparatorFieldLabel:"Nesting separator",PageSizeLabel:"Page size",pageBtnLabel:"Page {0}",firstPageLabel:"First page",prevPageLabel:"Previous page",nextPageLabel:"Next page",lastPageLabel:"Last page",PageSizeErrorMessage:"Page size can only be -1 or positive value",DetectDateTimeLabel:"Detect datetime",BatchSizeErrorMessage:"Batch size must be a valid number",SystemColumnsLabel:"Include system columns",ThroughputLabel:"Throughput",ThroughputDescription:"400 - 1000,000 RUs",ThroughputErrorMessage:"Throughput must be multiple of 100 and between 400 and 1000,000 RUs",WriteThroughPutBudgetDescription:"An integer string that represents the number of total RUs that you want to allocate to the bulk ingestion Spark job, out of the total throughput allocated to the collection",WriteThroughPutBudgetErrorMessage:"Write throughput budget must be > 0",WriteThroughPutBudgetLabel:"Write throughput budget",CaptureIntermediateUpdateDescription:"Capture the history of changes to items including intermediate changes between CDC reads.",CaptureDeletesDescription:"Capture User Delete records.",CaptureTransactionalStoreTTLsDescription:"Capture Cosmos DB Transactional store TTL records.",CosmosASAdditionConfigsDescription:"Additional Cosmos DB analytical store configs.",CosmosASQueryDescription:"Apply filters, projections and transformations on the Change feed via the source query",ADX_Timeout_Description:"Timeout (sec)",Oracle_Timeout_Description:"Timeout (minutes)",PartitionKeyLabel:"Partition key",PartitionKeyPlaceholder:"e.g. /address/zipCode",PartitionKeyDescription:"The Partition Key is used to automatically partition data among multiple servers for scalability. Choose a JSON property name that has a wide range of values and is likely to have evenly distributed access patterns.",Dataset_vertiParquet:"Optimized with Verti-Parquet technology.",Dataset_enableVertiParquet:"Enable Verti-Parquet",Dataset_FileFormat_ErrorMessageEscChaQuotCha:"Specifying both escapeChar and quoteChar is not supported. Specify one of them instead.",Dataset_FileFormat_ErrorMessageColDelEscChar:"Cannot use the same character for columnDelimiter and escapeChar.",Dataset_FileFormat_ErrorMessageColDelQuoCha:"Cannot use the same character for columnDelimiter and quoteChar.",Dataset_FileFormat_ErrorMessageCompression:"Compression is not needed for this format",Dataset_FileFormat_SetOfObjects:"Set of objects",Dataset_FileFormat_ArrayOfObjects:"Array of objects",Dataset_FileFormat_FilePatternLabel:"File pattern",Dataset_FileFormat_JSONNodeReferenceLabel:"Cross-apply nested JSON array",Dataset_FileFormat_JSONNodeReferenceDescription:"Select or specify the JSONPath of a nested JSON array for cross-apply.",Dataset_FileFormat_ParseJsonPathButtonLabel:"Parse JSON Path",Dataset_FileFormat_JsonPathExpressionLabel:"JSONPath expression",Dataset_FileFormat_AddJsonDefinitionLabel:"Add column definition",Dataset_FileFormat_OneCharacterValidationErrorMessage:"Only one character or unicode is allowed.",Dataset_FileFormat_OneCharacterValidationErrorMessageForPolybase:"Multiple characters will only be honored when using direct PolyBase to load into Azure Synapse Analytics or SQL Pool.",FilePath_Label:"File path",FolderFilePath_Label:"Folder / file path",InputFileOrFolderPath_Label:"Input file or folder",WildcardPath_Label:"Wildcard file path",PropertyRequired_InActivity_Template:"{0} is required for {1}.",PropertyRequired_InActivity_WildcardFilename:'"File path" of Dataset {0} is a folder, the wildcard file name is required for {1}',PropertyRequired_InActivity_WildcardFilenameTrident:'"File path" is a folder, the wildcard file name is required for {1}',WildcardFileNameUnsupportedForDistCp:"Wildcard file name is not supported while using HDFS DistCp",WildcardFolderPathUnsupportedForDistCp:"Wildcard folder path is not supported while using HDFS DistCp",LastModifiedTimeStartUnsupportedForDistCp:"Last modified time start is not supported while using HDFS DistCp",LastModifiedTimeEndUnsupportedForDistCp:"Last modified time end is not supported while using HDFS DistCp",RecursiveRequiredForDistCp:"Recursive must be true while using DistCp",Dateset_CannotUseWildCardPathForSink:"Cannot use wildcard path for sink dataset",Dataset_CollectionSelect_Label:"Collection",Dataset_TableSelect_EditDescription:"Check to enter the name manually. Uncheck to get the dropdown with selection.",Dataset_PreScript_Label:"Pre-copy script",Dataset_PreCopyScript_Description:"Specify a script for Copy Activity to execute before writing data into sink table in each run. You can use this property to clean up the pre-loaded data.",Dataset_ImportSchema_Label:"Import schema",FromConnectOrStoreLabel:"From connection/store",FromLocalFileLabel:"From sample file",FromLocalSchemaFileLabel:"From schema file",FromXsdFileLabel:"From xsd file",FromSampleInputLabel:"From manual input",FromSampleInputHelpText:"Replace with a JSON sample",Dataset_ImportSchema_FromLocal_SelectFile:"Select file",Dataset_TableOptions_Select_Option:"Select from existing table",Dataset_TableOptions_Create_Option:"Create new table",Dataset_TableOptions_Create_Label:"Schema and table name",Dataset_TableOptions_Staging_Schema:"Staging schema",Dataset_TableOptions_Create_Schema_Placeholder:"Schema name",Dataset_TableOptions_Staging_Database:"Staging database",Dataset_ImportSchema_JsonRecord:"From JSON record",Dataset_ImportSchema_FromLocal_SizeLimit:"File size exceeded the limit: {0} Kb.",Dataset_ImportSchema_FailedError:"Schema import failed: ",Dataset_GoAdvancedEdit:"Next->Advanced",Dataset_GoAdvancedTooltip:"For more advanced configuration with parameterization, etc",Dataset_GoDynamicExpressionHint:"Add parameters by clicking 'OK' at the bottom",Dataset_ReaderStoredProcedure:"Source stored procedure",Dataset_ReaderStoredProcedureName:"Source stored procedure name",Dataset_ReaderStoredProcedureParameters:"Source stored procedure parameters",Dataset_WriterStoredProcedure:"Sink stored procedure name",Dataset_WriterStoredProcedure_SqlWriterType:"Sink table type",Dataset_WriterStoredProcedure_TableTypeParameterName:"Sink table type parameter name",Dataset_WriterStoredProcedure_SqlWriterParameter:"Sink stored procedure parameters",Dataset_WriterStoredProcedure_MissingTableType:"Sink stored procedure table type is required",Dataset_WriterStoredProcedure_MissingTableTypeParameterName:"Sink stored procedure table type parameter name is required",Dataset_WriterTableOption:"Table option",Dataset_LoadSettings:"Load Settings",Dataset_LoadToNewTable:"Load to new table",Dataset_LoadToExistingTable:"Load to existing table",Dataset_DestinationTableName:"Destination table name",Dataset_WriterTableOptionAutoCreateTable:"Auto create table",Dataset_WriterTableOptionReminder:"Configure table option in sink side as 'autoCreate' to enable automatic table creation.",Dataset_WriterTableOptionDatasetTypeValidationErrorTemplate:"Auto table creation in sink side is not applicable when source dataset type is {0}.",Dataset_WriterTableOptionDatasetV2ValidationErrorTemplate:"Auto table creation in sink side is not applicable for your {0} dataset {1}. Please create a new one.",Dataset_WriterAutoTableCreationDesc:"It automatically creates sink table (if nonexistent) in source schema, which won't be supported when a stored procedure is specified (on the sink side) or when staging is enabled.",Dataset_AutoTableCreation:"auto table creation",Dataset_ImportParameter:"Import parameter",Dataset_Parquet_FilenameTooltip:"Filename doesn't support wildcard in dataset",Dataset_UseBulkInsertTableLock:"Bulk insert table lock",Dataset_UseBulkInsertTableLock_info:"Use this to improve copy performance during bulk insert operation on table with no index from multiple clients. ",Dataset_OpenDataset:"Open this dataset",Dataset_MoreConfig:"for more advanced configuration with parameterization.",Dataset_ParameterValidationError:"No value provided for parameter {0}",Dataset_ParameterValidationMismatchError:"Please update parameters in {0} activity",DataFlow_ParameterTypeValidationError:"Value provided for parameter {0} does not match parameter type '{1}'",DataflowActivity_SourceStagingConcurrencyError:"Provide -1 for maximum parallelism. Otherwise provide a number greater than 0",Named_Keyword_ValidationError:"{0} cannot be used as a name",Named_StartsWithNumber_ValidationError:"{0} cannot start with a number",Data_Preview_Button_Label:"Preview data",AzureEventHubs_Coming_Soon_PreviewText:"Coming soon for Event Hubs",Data_ImportFromSource_Button_Label:"Import schema from source",Script_Button_Label:"Preview script",Data_Preview_No_Data_Text:"No preview data to show",ExtendedResource:"Extended",ExecutePipelineActivityConfigurationInvokedPipeline:"Invoked pipeline",ExecutePipelineActivityConfigurationWaitOnCompletion:"Wait on completion",Feedback_Form_Icon_Title:"Are you satisfied with your experience?",Feedback_Form_Comments_Placeholder:"Give as much detail as you can. Do not include any personal data.",Feedback_Form_Upload_Screenshot_Size_Error_Message:"Maximun size for screenshot picture is 4MB.",Feedback_Form_Dialog_Message:"Feedback is sent successfully. Thank you for your feedback!",Feedback_Form_Feedback_Collected_By_MSFT:"Microsoft can email me about this feedback.",Feedback_Form_Icon_Smile:"Smile",Feedback_Form_Icon_Frown:"Frown",Feedback_Form_Subtitle:"Tell Microsoft how to improve {0}.",Feedback_Form_Header:"Give feedback to Microsoft",QA:"Q&A",ColumnProjectionDialog_ApplyColumns:"Apply columns",CreateUserManagedDatabaseInfopart1:"Please go to ",CreateUserManagedDatabaseInfopart2:" to create a new dedicated SQL pool.",ItemLabel:"Item",ItemsLabel:"Items",FilterActivityConfigurationItemsToolTip:"Input array on which filter should be applied.",FilterActivityConfigurationConditionToolTip:"Condition to be used for filtering the input.",ForEachActivityConfigurationSequential:"Sequential",ForEachActivityConfigurationBatchCount:"Batch count",ForEachActivityConfigurationBatchCountToolTip:"Maximum number of parallel runs of inner activities",EditActivities:"Edit activities",AddActivities:"Add activities",NoActivities:"No activities",ForEachTestRunSequentialNotification:"All the activities inside the foreach loop will be executed sequentially and each execute pipeline activity will wait on completion for debugging purposes. For triggered runs, the foreach loop will use the defined \u2018Batch count\u2019 for parallel executions.",ForEachTestRunSequentialDesc:"Starting pipeline {0}\xa0({1}).\xa0All the activities inside the foreach loop will be run sequentially and each run pipeline activity will wait on completion for debugging purposes. For triggered runs, the foreach loop will use the defined \u2018Batch count\u2019 for parallel runs.",GetMetadataActivityConfigurationFieldList:"Field list",SizeFieldLabel:"Size",LastModifiedFieldLabel:"Last modified",GetMetadataActivityConfigurationFieldPartitionsLabel:"Partition Indexes",GetMetadataActivityConfigurationFieldStructureLabel:"Structure",GetMetadataActivityConfigurationFieldColumnCountLabel:"Column count",ExistsLabel:"Exists",GetMetadataActivityConfigurationFieldItemNameLabel:"Item name",ItemTypeLabel:"Item type",GetMetadataActivityConfigurationFieldContentMD5Label:"Content MD5",GetMetadataActivityDatasetWildcardValidationError:'GetMetadata activity doesn\'t support wildcard "*" or "?" in dataset folderPath or fileName.',GetMetadataActivityDatasetWildcardValidationFolderPathError:'GetMetadata activity doesn\'t support wildcard "*" or "?" in dataset folderPath.',GetMetadataActivityDatasetWildcardValidationFileNameError:'GetMetadata activity doesn\'t support wildcard "*" or "?" in dataset fileName.',GetMetadataActivityDatasetNotSupportLinkedServiceError:"GetMetadata activity doesn't support {0}.",GetMetadataActivityFieldListEmpty:"Field list in GetMetadata activity cannot be empty.",LinkedService_DataStoreLabel:"Data store",PowerBIBannerInfo:"Build interactive reports with integrated Power BI capabilities.",PowerBIConnectedDescription:"You've already connected a Power BI workspace. Go to Develop to view your Power BI datasets and reports.",PowerBIConnectWorkspaceDescription:"Connect a Power BI workspace to create reports and datasets from data in your workspace.",PowerBIButtonLabel:"Connect to Power BI",PowerBILoadingWorkspaceFailed:"Failed to load workspace",PowerBILoadingArtifactsFailed:"Failed to load artifacts",GoToDevelop:"Go to Develop",GoToAzurePortal:"Go to Azure portal",HDInsight_LinkedServiceLabel:"HDInsight linked service",HDInsight_NewLinkedServiceLabel:"New HDI linked service",HDInsight_ClusterAccount:"Hdi Cluster",AML_LinkedServiceLabel:"Azure Machine Learning Studio (classic) linked service",AMLService_LinkedServiceLabel:"Azure Machine Learning linked service",Databricks_LinkedServiceLabel:"Databricks linked service",Databricks_NewLinkedServiceLabel:"New Databricks linked service",Databricks_InitScripts:"Databricks init scripts",Databricks_ScriptPath:"script path",Databricks_DockerContainerLabel:"Docker configuration",Databricks_DockerDescription:'Specify a Docker image to use when creating a cluster. <a target="_blank" href="https://docs.microsoft.com/azure/databricks/clusters/custom-containers">Learn more</a>',Databricks_DockerUsernameRequired:"Username is required",Databricks_DockerContainerUrlRequired:"Container url is required",Databricks_DockerPasswordRequried:"Password is required",Databricks_DockerImageUrl:"Docker image URL",Synapse_LinkedServiceLabel:"Azure Synapse Analytics (Artifacts) linked service",Synapse_NewLinkedServiceLabel:"New Azure Synapse Analytics (Artifacts) linked service",Synapse_Invalid_Workspace_Or_Token_For_Query:"Invalid Synapse Analytics workspace or token",Kusto_LinkedServiceLabel:"Kusto linked service",KustoPools:"Data Explorer pools (preview)",IfConditionActivityExpressionToolTip:"Expression of If Condition activity, the value must evaluate to true or false",IfConditionActivityTrueActivitiesNaviLabel:"True activities",IfConditionActivityFalseActivitiesNaviLabel:"False activities",IfConditionActivityTrueActivitiesLabel:"If True Activities ({0})",IfConditionActivityFalseActivitiesLabel:"If False Activities ({0})",IfConditionActivityAddTrueActivities:"Add If True Activity",IfConditionActivityAddFalseActivities:"Add If False Activity",IfConditionActivityEditTrueActivities:"Edit If True Activities",IfConditionActivityEditFalseActivities:"Edit If False Activities",IfConditionActivitiesUniqueError:"Activities with the name '{0}' exist in both the If and Else branches of '{1}'. These activities must have unique names.",IfConditionActivityUnallowedActvitiyError:"{0} ('{1}') is not allowed under an If Condition Activity.",UntilActivityUnallowedActivityError:"{0} ('{1}') is not allowed under an Until Activity.",Case:"Case",Cases:"Cases",AddCase:"Add case",SwitchActivityCasesToolTip:"Specify the expected value of the expression. If no case matches, the condition evaluates to Default.",SwitchActivityExpressionToolTip:"Expression of Switch activity. The evaluated case sensitive string value will be matched with each Case.",SwitchActivityUnallowedActvitiyError:"{0} ('{1}') is not allowed under a Switch Activity.",SwitchActivityCasesUniqueError:"Multiple Switch cases share the value '{0}' in {1}. These cases must have unique values.",SwitchActivityCasesUniqueErrorShort:"This value is already used.",SwitchActivityEmptyCaseError:"Switch cases can not have empty values.",SwitchActivityEmptyCaseErrorShort:"Case can not be empty.",SwitchActivityTooManyCasesError:"Switch activity has a maxiumum case limit of 25.",SwitchActivityEmptyExpressionError:"Switch activity expression must be defined.",MapReduceDisplayText:"Map Reduce",Notebook_DisplayText:"Databricks Notebook",Jar_DisplayText:"Databricks Jar",Python_DisplayText:"Databricks Python",SchemaMapping_CollectionReference:"Collection reference",SchemaMapping_NotSupportedWarning:"Mapping is not supported when both source and sink are hierarchical data.",SchemaMapping_NotSupportedWarning_Cosmos:"Mapping is not supported between Azure Cosmos DB and hierarchical data source.",BinaryMapping_NotSupportedWarning:"Mapping is not supported for binary copy.",ColumnMapping_NotSupportedWarning:"Mapping is not supported when source is Adobe Experience Platform.",ColumnMapping_NotExistedSourceColumnTemplate:"Column {0} may not exist in the source data store",ColumnMapping_EmptyStateTitle:"No column mapping exists",ColumnMapping_Suggestion:"Use the <b>New Mapping</b> button to map source data fields in the input file to target table columns.",SchemaMapping_NotIncludeColumn:"-- Not Include --",SchemaMapping_CopyFromArrayNotSupported:"Copy from array is unsupported",SchemaMapping_CopyToArrayNotSupported:"Copy to array is unsupported",SchemaMapping_MapComplexValuesToString:"Map complex values to string",MappingV2_MapIndexHeader:"Column number",MappingV2_AddMappingLabel:"New mapping",MappingV2_UploadMappingExcel:"Upload Mapping Excel",MappingV2_AutofillFromExcel:"Autofill from file:",MappingV2_UploadMappingExcelSidenavHeader:"Import mappings from file",MappingV2_ExcelImportOverwriteConfiguration:"Overwrite existing mappings",MappingV2_ExcelImportIgnoreConfiguration:"Keep existing mappings",MappingV2_ExcelImportConfigurationHeading:"Choose if you want to overwrite the existing mappings for a target table/column?",MappingV2_RulesBasedMappingSidenavHeader:"Add mapping rules",MappingV2_ColumnPatternOnSchemaSidenavHeader:"Set rules to transform source schema columns for a target table",MappingV2_ColumnEditablePlaceholder:"Select or edit column",MappingV2_ColumnInputPlaceholder:"Edit column",MappingV2_ColumnJsonPathPlaceholder:"Edit JSON path",ColumnTypeNotSupportedMessage:"Type of column '{0}' is not supported.",ColumnTypeWithSizeRequiresUserEdit:"Please specify a valid value for 'n' for this column type.",ColumnTypeWithPrecisionAndScaleRequiresUserEdit:"Please specify valid values for 'p' and 's' for this column type.",ColumnTypeWithPrecisionRequiresUserEdit:"Please specify a valid value for 'p' for this column type.",ColumnTypeWithScaleRequiresUserEdit:"Please specify a valid value for 's' for this column type.",MappingV2_SourceColumnIsEmptyMessage:"Mapping source is empty.",MappingV2_SinkColumnIsEmptyMessage:"Mapping sink is empty.",MappingV2_HierarchicalView:"Advanced editor",MappingV2_Hierarchical_Add_Array:"Add array",MappingV2_Hierarchical_Add_Object:"Add object",MappingV2_Hierarchical_Add_Node:"Add node",MappingV2_Hierarchical_Add_ChidrenNode_Tooltip:"Add new node to children",MappingV2_Hierarchical_Add_Node_Tooltip:"Add new node",MappingV2_ResetMapping_Tooltip:"Reset mapping from source to sink",MappingV2_ClearMapping_Tooltip:"Clear all mapping settings",IR_AuthKey:"Authentication key",IR_Key1:"Key1",IR_Key2:"Key2",VersionLabel:"Version",IR_NodeName:"Node name",IR_DotNetVersion:".Net version",IR_RunningRegisteredNode:"Running / Registered node(s)",IR_RunningRequestedNode:"Running / Requested node(s)",NodeSizeLabel:"Node size",IR_SSISDBEndpoint:"SSISDB server endpoint",IR_SSISDataProxyStaging:"Proxy / Staging",IR_SSISPublicIp:"Static public IP addresses",IR_SSISFirstPublicIp:"First static public IP address",IR_SSISSecondPublicIp:"Second static public IP address",IR_SSISDualStandbyPairTile:"DUAL STANDBY PAIR / ROLE",IR_SSISDualStandbyPairPrimaryDescription:"This Azure-SSIS Integration Runtime is in a primary role that can access SSISDB to execute packages deployed there, while it can also execute packages deployed somewhere else, for example in Azure Files",IR_SSISDualStandbyPairSecondaryDescription:"This Azure-SSIS Integration Runtime is in a secondary role that cannot access SSISDB to execute packages deployed there, but it can execute packages deployed somewhere else, for example in Azure Files",IR_HAEnabled:"High availability enabled",IR_ActionableError:"Error(s)",IR_NodeDetails:"Node Details",IR_AvailableMemory:"Available memory",IR_CpuUtilization:"CPU utilization",IR_ConcurrentJobsLabel:"Concurrent jobs",IR_SSIS_ConcurrentJobs:"Concurrent jobs (running/limit)",IR_ConcurrentJobs:"Concurrent jobs (running/available/limit)",IR_ConcurrentJobsDescription:"Running: worker number that are executing jobs.\nAvailable: healthy worker number that are capable for executing jobs.\nLimit: user specified value for max concurrent job.",IR_Network:"Network (In/Out)",IR_CredentialStatus:"Credential status",ResourceIDLabel:"Resource ID",TargetResourceIDLabel:"Target resource ID",ManagedPrivateEndpointPrivateIPLabel:"Managed private endpoint private IP",ManagedPrivateEndpointResourceIDLabel:"Managed private endpoint resource ID",IR_IPAddress:"IP address",IR_LimitConcurrentJobs:"Limit concurrent jobs",IR_GetIPAddress:"Get IP address",CopyToClipboard:"Copy to clipboard",RegenerateAuthKey:"Regenerate the authentication key",IR_SharedIR:"Shared integration runtime(s)",IR_ComputeNodes:"Compute nodes",IR_ParallelExecutionPerNode:"Parallel execution per node",IR_Incident:"Manually restarting the Self-hosted IR nodes may help mitigate the \u2018Unavailable\u2019 IR status.\xa0More details",IR_Incident_Link:"here.",IR_Incident_Title:"MESSAGE",IR_Default_Multiselect:"All integration runtimes",IR_PackageStores:"Package stores",IR_PackageStores_Monitoring:"Package store/linked service names",IR_SSISVNetSubnet:"Validate VNet / Subnet",IR_SSISVNetId:"VNet resource ID",IR_SSISDiagnoseConnectivity:"Diagnose connectivity",IR_ValidatingVNet:"Validating VNet\u2026",IR_SSISTarget:"Server endpoint/IP address",IR_SSISTargetDescription:"The server endpoint/IP address to test connection from your Azure-SSIS IR",IR_SSISTargetPlaceHolder:"FQDN or IP address",IR_SSISPortDescription:"The port number to test connection from your Azure-SSIS IR",IR_SSISTarget_empty:"The server endpoint/IP address cannot be empty.",IR_SSIS_SqlPort1433:"SQL Server/Azure SQL (1433)",IR_SSIS_MI3342:"Azure SQL Managed Instance \u2013 Public Endpoint (3342)",IR_SSIS_HTTP80:"HTTP (80)",IR_SSIS_AzureFileShare445:"Azure Files (445)",IR_SSIS_FTP21:"FTP (21)",IR_SSIS_HTTPS443:"Azure Storage/HTTPS (443)",IR_SSIS_DNS53:"DNS (53)",IR_SSIS_OtherPort:"Other (input below)",IR_SSISPort_Invalid:"The port number must be between 0 and 65535.",IR_Failed_Register:"Failed to register",IF_Failed_Detect:"Failed to detect",IR_Feedback_Title_Stop:"Thank you for using our product, could you tell us why you want to stop your running Azure-SSIS IR?",IR_Feedback_Stop_Reason_2:"Everything\u2019s good, it\u2019s part of a regular routine.",IR_Feedback_Stop_Reason_3:"The required features are not available.",IR_Feedback_Stop_Reason_4:"The product performance is poor.",IR_Feedback_Stop_Reason_5:"The product is overpriced.",IR_Feedback_Other_Reasons:"Other reasons",IR_Feedback_Email_Text:"Would you like to provide us with your email address, so we can follow up with more information?",IR_Feedback_Can_Email:"I understood that Microsoft can email me to follow up on my feedback.",IR_Feedback_Title_Cancel_Page1:"You\u2019ve just started to provision your Azure-SSIS IR, could you tell us why you want to stop now?",IR_Feedback_Cancel_Page1_Reason_1:"Everything\u2019s good, I\u2019m just looking around for now.",IR_Feedback_Cancel_Page1_Reason_2:"The region I need is not available.",IR_Feedback_Cancel_Page1_Reason_3:"I don\u2019t know which node size/how many nodes I need.",IR_Feedback_Cancel_Page1_Reason_4:"I don\u2019t know which Azure-SSIS IR edition/SQL Server license I need.",IR_Feedback_Cancel_Page1_Reason_5:"I don\u2019t know if I\u2019m qualified for Azure Hybrid Benefit.",IR_Feedback_Title_Cancel_Page2:"You\u2019ve completed most of the steps to provision your Azure-SSIS IR, could you tell us why you want to stop now?",IR_Feedback_Cancel_Page2_Reason_2:"I can\u2019t find/connect to my Azure SQL Database server or managed instance to host SSISDB.",IR_Feedback_Cancel_Page2_Reason_3:"I can\u2019t create Azure-SSIS IR package stores to manage my packages in file system/Azure Files/MSDB.",IR_Feedback_Cancel_Page3_Reason_2:"I don\u2019t know how many parallel package executions I need for each node.",IR_Feedback_Cancel_Page3_Reason_3:"I don\u2019t know how to customize my Azure-SSIS IR with standard/express custom setups.",IR_Feedback_Cancel_Page3_Reason_4:"I don\u2019t know how to join my Azure-SSIS IR to a VNet/bring my own static public IP addresses.",IR_Feedback_Cancel_Page3_Reason_5:"I don\u2019t know how to set up Self-Hosted IR as a proxy for my Azure-SSIS IR to access data on premises.",IR_Feedback_Title_Cancel_Page4:"You\u2019ve completed all steps to provision your Azure-SSIS IR, could you tell us why you want to stop now?",IR_Feedback_Cancel_Page4_Reason_2:"I\u2019m still unsure about the general/deployment/advanced settings for my Azure-SSIS IR.",IR_Feedback_Cancel_Page4_Reason_3:"I\u2019m still unsure about deploying packages to run on my Azure-SSIS IR.",IR_Feedback_Cancel_Page4_Reason_4:"I\u2019m still unsure about the cost of running my Azure-SSIS IR.",IR_UpgradeDotNet_Banner_Message:"Update .NET Framework runtime version to 4.7.2 or above to allow upgrade of self-hosted integration runtime by 01 December 2020.",IR_UpgradeDotNet_Detail_Message:"You have a self-hosted integration runtime (IR) installed on a machine with .NET Framework runtime lower than 4.7.2. For the update to newer version of self-hosted integration runtime, we require .NET Framework runtime 4.7.2 or above. Please update the .NET Framework runtime version on the self-hosted IR node(s) to 4.7.2 or above by 01 December 2020. In case you do not upgrade the .Net version by 01 December 2020, you will not be upgraded to the latest version. Your current version will continue to work as is but you will not get the latest updates (features, bug fixes, security enhancements). We recommend using the latest self-hosted IR versions.",IR_UpgradeDotNet_Detail_Action:"Upgrade the .NET Framework runtime to 4.7.2 ({0}) on the below integration runtime node(s):",IR_UpgradeDotNet_Detail_Label:"Upgrade .NET Framework runtime on the self-hosted IR node(s)",IR_DotNetFramework:".Net Framework {0}",IR_DotNetFrameworkLater:".Net Framework {0} or later",IR_DotNetFrameworkLower:".Net Framework {0} or lower",IR_SendLog_ConfirmDialog_Title:"Share the Self-hosted Integration Runtime (IR) logs with Microsoft",IR_SendLog_ConfirmDialog_Content:'You can send the logs of this activity run or this self-hosted integration runtime (IR) to Microsoft to facilitate troubleshooting of your issues. Please note that you still need to create a Microsoft support ticket to solve your issues.<br/><br/>Microsoft respects the <a target="_blank" href="https://go.microsoft.com/fwlink/p/?LinkId=512132">privacy</a> of your personal information.',IR_SendLog_ConfirmDialog_Content_Linked:'You can send the logs of this activity run to Microsoft to facilitate troubleshooting of your issues. Please note that you still need to create a Microsoft support ticket to solve your issues.<br/><br/>Microsoft respects the <a target="_blank" href="https://go.microsoft.com/fwlink/p/?LinkId=512132">privacy</a> of your personal information.',IR_SendLog_ConfirmDialog_Ack:"Acknowledge Microsoft privacy statement",IR_SendLog_ConfirmDialog_SendLogs:"Send logs",IR_SendLog_ConfirmDialog_SendActivityLogs:"Activity specific logs",IR_SendLog_ConfirmDialog_SendActivityLogs_Description:"This will send activity specific logs on self-hosted IR to Microsoft for troubleshooting",IR_SendLog_ConfirmDialog_SendIRLogs:"All logs",IR_SendLog_ConfirmDialog_SendIRLogs_Description:"This will send entire self-hosted IR logs to Microsoft for troubleshooting",IR_SendLog_ConfirmDialog_SendActivityLogs_Progress:"Sending activity logs",IR_SendLog_ConfirmDialog_SendIRLogs_Progress:"Sending all logs",IR_SendLog_ResultDialog_Title:"Your integration runtime logs are uploading...",IR_SendLog_ResultDialog_TitleFailed:"Failed to upload your integration runtime logs",IR_SendLog_ResultDialog_Content:"Please keep your Report ID and contact Microsoft support to solve the issue.",IR_SendLog_ResultDialog_ReportId:"Report ID",IR_SendLog_ResultDialog_EntriesUploaded:"Entries uploaded",IR_ActivityLog_Title:"Activity logs on self-hosted IR",IR_ActivityLog_Description:"Self-hosted integration runtime logs",IR_ActivityLog_GetLogConfirm:"To display these logs, we would be accessing your event logs located on the machine(s) where the self-hosted integration runtime is setup.",SelfHostedLabel:"Self-Hosted",IRType_Linked:"Self-Hosted (Linked)",IRType_AzureSSIS:"Azure-SSIS",IRType_AzureSSIS_Linked:"Azure-SSIS (Linked)",IRType_AzurePublic:"Azure (Public)",IRType_AzureVNet:"Azure (VNet)",IRType_ManagedVNet:"Managed Virtual Network",Using_PE_Header:"Using private endpoint",CreatePrivateEndpoint:"Create private endpoint",PrivateEndpoint:"Private endpoint",VNetConfiguration:"Virtual network configuration",EnableVNetMessage_synapse:"This may happen if {0} only allows secured connections. If that's the case, please create a new workspace with virtual network option enabled.",EnableVNetMessage_ADF:"This may happen if {0} only allows secured connections. If that's the case, please use a VNet integration runtime.",VNetConfigurationTooltip_Enabled:"This integration runtime uses the default virtual network of this {0}.",VNetConfigurationTooltip_Synapse_Disabled:"This workspace is not enabled for virtual network, please create a new one with virtual network option enabled.",VNetConfigurationTooltip:'Enabling Managed Virtual Network ensures that the Azure Integration Runtime compute is provisioned within it, and can access data securely using Private Endpoints. <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/managed-virtual-network-private-endpoint">Learn more</a>',PE_NotExists_Part1:"If {0} only allows secured connections, you need to create a private connection. Click ",PE_NotExists_Part2:" to create one.",PE_NotApprovedMessage_Part1:"If {0} only allows secured connections, you need to have a private connection in {1} state. And the approval state may be stale by a minute. The private connection: ",PE_NotApprovedMessage_Part2:' is in "{0}" state.',PE_ApprovedMessage:" is currently Approved.",PE_LoadingStatus:"Loading private endpoint status...",PE_datasource:"your data source",PE_referencedAKV:"your referenced Azure Key Vault",ManagedResources:"Managed resources",IRStatus_Unavailable:"Unavailable",IRStatus_Unauthorized:"Access denied",IRStatus_Limited:"Running (Limited)",IRStatus_Inactive:"Inactive",Initializing:"Initializing",Initial:"Initial",IRStatus_Provisioning:"Provisioning",IRStatus_ProvisionFailed:"Provision failed",IRStatus_ProvisionSucceeded:"Provision succeeded",IRStatus_InitializeFailed:"Initialize failed",Status_Stopped:"Stopped",IRStatus_StoppedWithError:"Stopped with error(s)",IRStatus_FailedToStart:"Failed to start",Status_Starting:"Starting",Status_Restarting:"Restarting",Status_Started:"Started",Status_Scaling:"Scaling",IRStatus_Recycling:"Recycling",OnLabel:"On",OffLabel:"Off",TroubleShooting:"Trouble shooting",Snapshotting:"Snapshotting",Replicating:"Replicating",VirtualNetWork:"Virtual network",IRStatus_Tooltip_NeedRegistration:"Needs registration",IRStatus_Tooltip_LimitedWorker:"Limited functionality (Some worker node unhealthy)",IRStatus_Tooltip_Limited:"Limited functionality",IRStatus_Tooltip_Offline:"Offline",IRStatus_Tooltip_Error:"Please check integration runtime errors or node-specific errors",IRCreation_VNET_RadioGroup_Tooltip:"Update virtual network configuration is not supported.",IRCreation_CopyTTL_RadioGroup_Tooltip:"Enable copy activity TTL is not supported for AutoResolveIntegrationRuntime.",IRCreation_PipelineAndExternalTTL_Tooltip:"Change pipeline and external activity TTL is not supported for AutoResolveIntegrationRuntime.",IRCreation_Title:"Integration runtime setup",IRAirflowCreation_Title:"Airflow environment setup",IRExpressCreation_Title:"SSIS Integration Runtime express setup",IRCreation_IRName_Description:"Integration runtime name must be unique within one {0}.",IRCreation_IRName_Placeholder:"Enter name here...",IRCreation_IRName_Validating:"Validating the integration runtime name...",IRCreation_Sharing_New:"Grant permission to another {0}",IRCreation_Sharing_ResourceID_Description:"This resource ID needs to be used while creating a linked IR.",IRCreation_QuickReuse_Warning:"The quick re-use option will be removed soon as all data flows with a TTL will automatically use quick reuse",IRCreation_CustomSetupContainerSasUri_Validating:"Validating the container SAS URI...",IRCreation_CustomSetupContainerSasUri_ValidationFailedPlainText:"Validation failed! Message: {0}",IRCreation_CustomSetupContainerSasUri_ValidationFailed:"Validation failed!",IRCreation_CustomSetupContainerSasUri_ValidationFailedVNetDesc:"Test connection failed. If your Azure Storage connectivity is isolated with VNet service endpoint or private endpoint, you can ignore this error because test connection request is from public network so it is expected to fail. ",IRCreation_CustomSetupContainerSasUri_ValidationErrorMsg:"{Code: {0}, Message: {1}}",IRCreation_CustomSetupContainerSasUri_ValidationError:"Validation error!",IRCreation_IRDescription_Placeholder:"Enter description here...",IRCreation_IRAuthKey_Error_Pattern:"The authentication key is incorrect.",IRCreation_Error_FailedToPublish:"Failed to save integration runtime.",IRCreation_Copy_Selfhosted_Properties_Description:"Private network support is realized by installing integration runtime to machines in the same on-premises network/VNET as the resource the integration runtime is connecting to. Follow below steps to register and install integration runtime on your self-hosted machines.",IRCreation_Copy_Selfhosted_Properties_Shared_Description:"Use an existing self-hosted integration runtime infrastructure in another {0}. This will create a logical link to an existing self-hosted integration runtime.",IRCreation_Copy_Selfhosted_Properties_Shared_Description_Tooltip:"Please make sure to grant access to {0} from the shared self-hosted integration runtime (Author -> Connections -> Integration runtimes -> Edit the self-hosted integration to be shared -> Sharing)",IRCreation_Copy_Selfhosted_Properties_Shared_SelectFromDF_InvalidDF:"Cannot find the {0} with this service principal application ID.",IRCreation_Copy_Selfhosted_Properties_Shared_EditNotEnabled:"The edit feature of linked self-hosted integration runtime is still in development. Cuerrently you may delete and re-create a linked self-hosted integration runtime to change the authentication key.",IRCreation_Copy_Selfhosted_Properties_Shared_ManuallyInput:"Input resource id",IRCreation_Copy_Selfhosted_Properties_Shared_ManuallyInput_ResourceId_Description:"Input the resource ID of the shared integration runtime. You can get it from the shared integration runtime -> edit -> sharing tab.",IRCreation_Copy_Selfhosted_Properties_Shared_ManuallyInput_ResourceId_Placeholder:"Enter the resource ID of the shared integration runtime here...",IRCreation_Copy_SharedTopLevelIR_ManuallyInput_ResourceId_Description:"Input the resource ID of the data management runtime connect. You can get it from Azure portal",IRCreation_Copy_SharedTopLevelIR_ManuallyInput_ResourceId_Placeholder:"Enter the resource ID of the data management runtime connect here...",IRCreation_Shared_ManuallyInput_ResourceId_Error:'The resource ID is incorrect. It should be some string like "{0}"',IRCreation_Copy_Selfhosted_Properties_AuthKey_Tooltip:"Input the authentication key of the existing integration runtime",IRCreation_Copy_Selfhosted_Properties_AuthKey_Placeholder:"Enter authentication key here...",IRCreation_Copy_Selfhosted_Properties_AuthKey_ShowAuthKey:"Show authentication key",IRCreation_Copy_Selfhosted_IRNodes_Description:"Install integration runtime on Windows machine or add further nodes using the Authentication Key.",IRCreation_Copy_Selfhosted_IRNodes_Option1_Title:"Option 1: Express setup",IRCreation_Copy_Selfhosted_IRNodes_Option1_Desc:"Click here to launch the express setup for this computer",IRCreation_Copy_Selfhosted_IRNodes_Option2_Title:"Option 2: Manual setup",IRCreation_Copy_Selfhosted_IRNodes_Option2_Step1_Title:"Step 1: ",IRCreation_Copy_Selfhosted_IRNodes_Option2_Step1_Desc:"Download and install integration runtime",IRCreation_Copy_Selfhosted_IRNodes_Option2_Step2:"Step 2: Use this key to register your integration runtime",IRCreation_Copy_Selfhosted_IRNodes_Express_Setup_Link_Creating:"Creating Express Setup Link",IRCreation_Copy_Selfhosted_IRNodes_Fetch_AuthKey:"Fetching Authentication Key",IRCreation_Copy_Selfhosted_IRNodes_Regenerate_AuthKey:"Regenerating Authentication Key",IRCreation_Copy_Selfhosted_IRNodes_Key_Copied:"Copied to your Clipboard",IRCreation_Copy_Selfhosted_IRNodes_Key_Refreshing:"Regenerating...",IRCreation_Copy_Selfhosted_IRNodes_Key_Refreshed:"Regenerated",IRCreation_Copy_Selfhosted_IRNodes_Key_Regenerate_Description:"Are you sure you want to regenerate the authentication key ({1}) of this integration runtime: {0} ?",IRCreation_Copy_Selfhosted_IRNodes_Linked_ConcurrentJob:"You cannot modify the concurrent job value from the linked integration runtime",IRCreation_Copy_Selfhosted_IRNodes_Linked_Delete:"You cannot delete node from the linked integration runtime",IRCreation_Copy_Selfhosted_IRNodes_Linked_Update:"You cannot modify auto update settings from the linked integration runtime",IRCreation_Copy_Selfhosted_IRNodes_AutoUpgrade_Fetching:"Fetching Auto Update Settings...",IRCreation_Copy_Selfhosted_IRNodes_ViewServiceUrls:"View Service URLs",IRCreation_Copy_Selfhosted_IRNodes_ServiceUrls:"Service URLs",IRCreation_Copy_Selfhosted_IRNodes_ServiceUrls_Description:"The Service URLs are used by the self-hosted integration runtime for communicating with this Azure Service. In case you need to allow the outbound URLs in your on-premises firewall, you may use this list. This list does not contain the URLs of the cloud data stores that you may be using, so you may need to allow them as well.",IRCreation_Copy_Selfhosted_IRNodes_ServiceUrls_Failed:"Loading failed. Please close the window and try again.",IRCreation_Copy_Selfhosted_IRNodes_IPDescription:"IP addresses of the nodes can be helpful for firewall / allowlisting purposes in your data stores.",IRCreation_Copy_Selfhosted_IRNodes_GetIPNotSupport:"Get IP address is not supported for nodes version lower than 3.2",IRCreation_Copy_Selfhosted_IRNodes_ChangeScheduleTimeInProgress:"Changing the schedule upgrade time.",IRCreation_Copy_Selfhosted_IRNodes_ChangeScheduleTimeSucceeded:"Schedule upgrade time is updated.",IRCreation_Copy_Selfhosted_IRNodes_StartUpgrade:"Upgrading the integration runtime nodes.",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Description_1:"You can share your self-hosted integration runtime (IR) with another {0}.",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Description_2:"To enable sharing:",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Description_3:"Grant permission to the {0} in which you would like to reference this IR (shared).",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Description_4:"Copy the below 'Resource ID' and use it while creating a new linked self-hosted IR in the other {0}.",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Description_5:"Click here to check out our document",IRCreation_Copy_Selfhosted_IRNodes_Sharing_NoResult:"---",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Error:"Error occurred when getting the result. Please refresh the table.",IRCreation_Copy_Selfhosted_IRNodes_Sharing_NoIR:"No linked integration runtime created.",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Edit_Title:"Change in resource ID",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Edit_Description:"We have detected a change in resouce ID. Are you sure you want to save the updated resource ID?\nIf you intend to change this IR to reference another shared IR, you might have to reset your existing linked services referencing this IR.",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Inherited_Description:'The "{0}" has inherited permissions from a parent resource (Resource Group, Subscription, etc.) of this integration runtime "{1}".',IRCreation_Copy_Selfhosted_IRNodes_Sharing_KeyAuth_Description:'The linked integration runtimes(IR) in "{0}" are created with IR authentication key. If you want to remove it, please delete all the linked IRs from the linked resource "{0}".\nWe have released IR sharing with RBAC authentication as GA, please go to "{0}" and edit the linked IR with key authentication to upgrade it.',IRCreation_Copy_Selfhosted_IRNodes_Sharing_KeyAuth_Tag:"Preview - Authentication Key",IRCreation_Copy_Selfhosted_IRNodes_Sharing_Edit_LimitConcurrentJobs_Description:'We have detected an unsaved value in "Limit concurrent jobs" field. If you continue, your changes will be lost.',IRCreation_Copy_Azure_Properties_Description:"The {0} manages the integration runtime in Azure to connect to required data source/destination or external compute in public network. The compute resource is elastic allocated based on performance requirement of activities.",IRCreation_Copy_Azure_Properties_VNet_Description:"The {0} manages the integration runtime in Azure to connect to required data source/destination or external compute in Azure Virtual Network. The compute resource is elastic allocated based on performance requirement of activities.",IRCreation_Copy_Managed_Properties_VNet_Subscription_Description:"Azure subscription(s) in which to select a VNet for your Azure Integration Runtime to join",IRCreation_Copy_Managed_Properties_VNet_Type:"Virtual Network",IRCreation_Copy_Managed_Properties_VNet_EditVNet:"Edit Virtual Network",IRCreation_Copy_Managed_Properties_VNet_Name_Description:"The name of VNet for your Azure Integration Runtime to join",IRCreation_Copy_Managed_Properties_VNet_Subnet_Name_Description:"The name of subnet for your Azure Integration Runtime to join",VNetNameLabel:"VNet name",InteractiveAuthoringTooltip:"Interactive authoring capability is used during authoring for functionalities like Test connection / Browse and Preview data / Import parameter / Import schema inside managed Virtual Network.",InteractiveAuthoringTooltipDescription:"It is used for testing connection, browsing folder/table list, importing schema/parameter, and previewing data inside managed virtual network. {0} in integration runtime",InteractiveAuthoringTooltipAction:"Edit interactive authoring",IRCreation_Copy_Managed_Properties_VNet_Notfound:"No VNet can be found.",IRCreation_Copy_Managed_Properties_VNet_Subnet_Notfound:"No subnet can be found.",IRCreation_Copy_Managed_Properties_VNet_SelectSubscriptionFirst:"Please select a subscription first.",IRCreation_Copy_Managed_Properties_VNet_SelectVNetFirst:"Please select an VNet first.",IRCreation_Copy_Managed_Properties_VNet_RegionNotSupported:"(Region not supported)",IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Title:"Virtual network subnet selection method",IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Description:"You can select a virtual network subnet from the list of available subnets in your Azure subscriptions, in which case you don\u2019t need to enter the resource ID in free form text fields.",IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Manually_VNetResourceId:"Virtual network resource ID",IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Manually_VNetResourceId_Description:"Input the resource ID of the virtual network resource ID.",IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Manually_VNetResourceId_Placeholder:'Enter the resource ID here... It should be some string like "/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}"',IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Manually_VNetResourceId_Error:'The resource ID is incorrect. It should be some string like "/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/Microsoft.Network/virtualNetworks/{vnetName}"',IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Manually_SubnetName_Description:"Input the subnet name.",IRCreation_Copy_Managed_Properties_VNet_SubnetSelectMethod_Manually_SubnetName_Placeholder:"Enter the subnet name here...",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_Enabled:"Interactive authoring enabled",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_Enabling:"Interactive authoring enabling",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_Disabled:"Interactive authoring disabled",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_EnableAfterCreation:"Enable interactive authoring capability after creation",IRCreation_Copy_Managed_Properties_VNet_EnableCopycomputeScale:"Enable Copy compute scale",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_EnableAfterCreation_Description:"Enable interactive authoring capability (test connection / browse folder/ browse table / preview data / import parameter / import schema) on this VNet integration runtime.",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_AutoTermimation_Message:"The interactive authoring capability for this VNet integration runtime will be automatically disabled after 60 minutes of inactivity (i.e., no interactive commands).",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_AutoTermimation:"Auto termination",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_AutoTermimation_TerminateAfter:"Terminate after",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_AutoTermimation_MinutesOfInactivity:"minutes of inactivity",IRCreation_Copy_Managed_Properties_VNet_InteractiveQuery_AutoTermimation_InvalidInterval:"Interval must be between 10 and 10000 minutes",IRCreation_Copy_Managed_Properties_VNet_PreventDataExfiltration_Label:"Block outbound data traffic",IRCreation_Copy_Managed_Properties_VNet_PreventDataExfiltration_Description:"When enabled, the outbound data traffic will be blocked for this VNet integration runtime.",IRCreation_Copy_Managed_Properties_VNet_PreventDataExfiltration_Banner:"Dataflow functionality will not be available for this integration runtime when block outbound data traffic option is enabled.",IRCreation_Copy_Managed_Properties_VNet_CreateLonger:"We are still working on creating the integration runtime. It may take up to 2 minutes.",IRCreation_Copy_Managed_Properties_VNet_InteractiveAuthoringTTL_Description:"The allowed idle time for interactive authoring. Specifies how long it stays after completion of an interactive authoring run if there are no other active actions.",IRCreation_Copy_Managed_Properties_VNet_EnableCopyComputeScale_Description:"When enabled, the compute will terminate after the specified time interval of inactivity.",IRCreation_Copy_Managed_Properties_VNet_CopyComputeScaleDIU_Description:"Specify the powerfulness of the copy executor.",IRCreation_Copy_Managed_Properties_VNet_CopyComputeScaleTTL_Description:"The allowed idle time for copy activity. Specifies how long it stays after completion of a copy activity run if there are no other active jobs.",IRCreation_Copy_Managed_Properties_VNet_PipelineAndExternalScale_Description:"The compute will terminate after the specified time interval of inactivity.",IRCreation_Copy_Managed_Properties_VNet_PipelineAndExternalScaleTTL_Description:"The allowed idle time for pipeline and external activity. Specifies how long it stays after completion of a pipeline or external activity run if there are no other active jobs.",IRCreation_Copy_Azure_Properties_VNet_Interactive_Description:"Enable or disable interactive authoring capability (test connection / browse folder/ browse table / preview data / import parameter / import schema) on this VNet integration runtime.",IRCreation_SSIS_Node_Number_Label:"Node number",IRCreation_SSIS_Sku_Label:"Edition/license",Standard:"Standard",StandardMemoryOptimized:"Standard (Memory Optimized)",BasicGeneralPurpose:"Basic (General Purpose)",IRCreation_SSIS_Enterprise:"Enterprise",IRCreation_SSIS_Join_Vnet_Description:"Select a VNet for your Azure-SSIS Integration Runtime to join, allow {0} to create certain network resources, and optionally bring your own static public IP addresses",IRCreation_SSIS_Join_Vnet_Description_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network#resource-group">here</a>)',IRCreation_SSIS_Bring_Ip_Description:"Bring static public IP addresses for your Azure-SSIS Integration Runtime",IRCreation_SSIS_Bring_Ip_Description_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network#publicIP">here</a>)',IRCreation_SSIS_SelfhostedIR_Blob_Description:"Set up Self-Hosted Integration Runtime as a proxy for your Azure-SSIS Integration Runtime",IRCreation_SSIS_SelfhostedIR_Blob_Description_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/self-hosted-integration-runtime-proxy-ssis">here</a>)',IRCreation_SSIS_SelfhostedIR_Blob_Description_Tooltip:"Use a proxy to access data stores on premises/within VNet/configured with VNet service endpoints.",IRCreation_SSIS_SelfhostedIR_Label:"Self-Hosted Integration Runtime",IRCreation_SSIS_SelfhostedIR_Tooltip:"Select your existing Self-Hosted Integration Runtime as a proxy.",IRCreation_SSIS_Sharing_Name_Tooltip:"The name of your linked Azure-SSIS Integration Runtime",IRCreation_SSIS_Sharing_Description:"Share your Azure-SSIS Integration Runtime with another Data Factory.",IRCreation_SSIS_Sharing_ManuallyInput_ResourceId_Error:'The resource ID is incorrect. It should be some string like "/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{dataFactoryName}/integrationruntimes/{integrationRuntimeName}"',IRCreation_SSIS_Sharing_Description_Tooltip:"Follow these steps to share your Azure-SSIS Integration Runtime with another Data Factory:\n1. Grant permission to another Data Factory, with which your Azure-SSIS Integration Runtime will be shared \n2. Copy the resource ID of your shared Azure-SSIS Integration Runtime \u2013 Use it to create your linked Azure-SSIS Integration Runtime in another Data Factory",IRCreation_Copy_SSIS_Sharing_ManuallyInput_ResourceId_Placeholder:"Enter the resource ID of the shared ssis integration runtime here...",IRCreation_SSIS_Sharing_Description_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/share-azure-ssis-integration-runtime">here</a>)',IRCreation_SSIS_Sharing_ResourceID_Description:"The resource ID of your shared Azure-SSIS Integration Runtime to copy and use when creating your linked Azure-SSIS Integration Runtime in another Data Factory",IRCreation_SSIS_BlobLinkedService_Label:"Staging storage linked service",IRCreation_SSIS_BlobLinkedService_Tooltip:"Select your existing Azure Blob Storage linked service or create a new one that references the staging data store to be used when moving data between self-hosted and Azure-SSIS integration runtimes. For this purpose, the connectVia property of staging storage linked service must be set to AutoResolveIntegrationRuntime.",IRCreation_SSIS_BlobLinkedService_NotSupportIrType:"To use self-hosted integration runtime as a proxy for your Azure-SSIS integration runtime, the connectVia property of staging storage linked service must be set to AutoResolveIntegrationRuntime.",IRCreation_SSIS_StagingPath_Label:"Staging path",IRCreation_SSIS_StagingPath_PlaceHolder:"Your staging path, leave it blank to use a default container",IRCreation_SSIS_StagingPath_Tooltip:"Specify the path in your staging data store to be used when moving data between Self-Hosted and Azure-SSIS Integration Runtimes, a default container will be used if unspecified.",IRCreation_SSIS_StagingPath_Error:"Staging path may only contain lowercase letters, numbers, hyphens, and must begin with a letter or number. Each hyphen must be preceded and followed by a non-hyphen character. The path must be between 3 and 63 characters long.",IRCreation_SSIS_CustomSetup_Label:"Customize your Azure-SSIS Integration Runtime with additional system configurations/component installations",IRCreation_SSIS_CustomSetup_Description:"Only required if you want to customize your Azure-SSIS Integration Runtime with additional system configurations/component installations via a custom setup script/express custom setups \u2013 V2-series VMs/node sizes are no longer supported with custom setups, please use v3-series ones instead",IRCreation_SSIS_CustomSetup_Description_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup">here</a>)',IRCreation_SSIS_CustomSetup_NoCustomScript_Label:"Express custom setup",IRCreation_SSIS_CustomSetup_NoCustomScript_Tooltip:"Add/edit/delete express custom setups without a custom setup script",NotSupportedType:"Not supported type",IRCreation_SSIS_CustomSetup_Edit_Title:"Edit express custom setup",IRCreation_SSIS_CustomSetup_Add_Title:"Add express custom setup",IRCreation_SSIS_CustomSetup_Type_Label:"Express custom setup type",IRCreation_SSIS_CustomSetup_Type_Description:"Select the type of express custom setup to add",IRCreation_SSIS_CustomSetup_Name_Shortcut_Suffix:"...",IRCreation_SSIS_CustomSetup_Validation_Error_HasWhitespace:"Whitespace cannot be entered.",IRCreation_SSIS_CustomSetup_Validation_Error_licenseKey:"The license key is invalid.",IRCreation_SSIS_CustomSetup_CmdKey:"Run cmdkey command",IRCreation_SSIS_CustomSetup_CmdKey_Format:"{0},{1}",IRCreation_SSIS_CustomSetup_CmdKey_Add_Label:"/Add:",IRCreation_SSIS_CustomSetup_CmdKey_Add_PlaceHolder:"Your targeted computer/domain name",IRCreation_SSIS_CustomSetup_CmdKey_Add_Description:"Enter the name of targeted computer/domain, for which access credentials will be persisted on your Azure-SSIS Integration Runtime \u2013 For file shares or Azure Files, enter YourFileShareServerName or YourAzureStorageAccountName.file.core.windows.net, respectively",IRCreation_SSIS_CustomSetup_CmdKey_Add_Error_Duplicated:"The target '{0}' already exists.",IRCreation_SSIS_CustomSetup_CmdKey_User_Label:"/User:",IRCreation_SSIS_CustomSetup_CmdKey_User_PlaceHolder:"Your account name/username",IRCreation_SSIS_CustomSetup_CmdKey_User_Description:"Enter the account name/username to be persisted on your Azure-SSIS Integration Runtime \u2013 For file shares or Azure Files, enter YourDomainName\\YourUsername or azure\\YourAzureStorageAccountName, respectively",IRCreation_SSIS_CustomSetup_CmdKey_Pass_Label:"/Pass:",IRcreation_SSIS_CustomSetup_CmdKey_Pass_PlaceHolder:"Your account key/password",IRCreation_SSIS_CustomSetup_CmdKey_Pass_Description:"Enter the account key/password to be persisted on your Azure-SSIS Integration Runtime \u2013 For file shares or Azure Files, enter YourPassword or YourAccountKey, respectively",IRCreation_SSIS_CustomSetup_SqlServerAlias:"Add SQL Server alias",IRCreation_SSIS_CustomSetup_SqlServerAlias_Format:"SqlServerAlias: {0}/{1}:{2}",IRCreation_SSIS_CustomSetup_SqlServerAlias_AliasName_Label:"Alias name:",IRCreation_SSIS_CustomSetup_SqlServerAlias_AliasName_PlaceHolder:"Put your alias Name here",IRCreation_SSIS_CustomSetup_SqlServerAlias_AliasName_Description:"Specifies the alias name here",IRCreation_SSIS_CustomSetup_SqlServerAlias_AliasName_Error_Duplicated:"The alias name '{0}' does already exist.",IRCreation_SSIS_CustomSetup_SqlServerAlias_ServerName_Label:"Server name:",IRCreation_SSIS_CustomSetup_SqlServerAlias_ServerName_PlaceHolder:"Put your server name here",IRCreation_SSIS_CustomSetup_SqlServerAlias_ServerName_Description:"Specifies the server name here",IRCreation_SSIS_CustomSetup_SqlServerAlias_Port_Label:"Port:",IRCreation_SSIS_CustomSetup_SqlServerAlias_Port_PlaceHolder:"Put your port number here",IRCreation_SSIS_CustomSetup_SqlServerAlias_Port_Description:"Specifies the server port here",IRCreation_SSIS_CustomSetup_Install:"Install licensed component",IRCreation_SSIS_CustomSetup_Install_ComponentName_Label:"Component name",IRCreation_SSIS_CustomSetup_Install_ComponentName_Description:"Select the name of component to install",IRCreation_SSIS_CustomSetup_Install_ComponentName_IntegrationToolkit:"KingswaySoft's SSIS Integration Toolkit",IRCreation_SSIS_CustomSetup_Install_ComponentName_ProductivityPack:"KingswaySoft's Productivity Pack",IRCreation_SSIS_CustomSetup_Install_ComponentName_TaskFactory:"SentryOne's Task Factory",IRCreation_SSIS_CustomSetup_Install_ComponentName_HeddaIO:"oh22's HEDDA.IO",IRCreation_SSIS_CustomSetup_Install_ComponentName_SQLPhoneticsNet:"oh22's SQLPhonetics.NET",IRCreation_SSIS_CustomSetup_Install_ComponentName_XtractIS:"Theobald Software's Xtract IS",IRCreation_SSIS_CustomSetup_Install_ComponentName_CDataStandard:"CData's SSIS Standard Package",IRCreation_SSIS_CustomSetup_Install_ComponentName_CDataExtended:"CData's SSIS Extended Package",IRCreation_SSIS_CustomSetup_Install_ComponentName_AsIntegrationService:"AecorSoft's Integration Service",IRCreation_SSIS_CustomSetup_Install_ComponentName_Unknown:"Unknown component",IRCreation_SSIS_CustomSetup_Install_ComponentName_Error_Duplicated:"The component '{0}' already exists.",IRCreation_SSIS_CustomSetup_Install_ComponentLicense_Label:"License key",IRCreation_SSIS_CustomSetup_Install_ComponentLicense_PlaceHolder:"Your component license key",IRCreation_SSIS_CustomSetup_Install_ComponentLicense_Description:"Enter the license key for additional component to be installed on your Azure-SSIS Integration Runtime",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_Label:"License file",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_PlaceHolder:"Drag & drop your license file here",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_Description:"Upload the license file for additional component to be installed on your Azure-SSIS Integration Runtime",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_Uploaded:"License file is uploaded, drag & drop a new one here",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_UploadMessage:"The license file {0} is uploaded successfully at {1}",IRCreation_SSIS_CustomSetup_Install_ComponentLicenseFile_ErrorMessage:"License file must be provided",IRCreation_SSIS_CustomSetup_AzPowerShell:"Install Azure PowerShell",IRCreation_SSIS_CustomSetup_AzPowerShell_Error_Duplicated:"Azure PowerShell already exists.",IRCreation_SSIS_CustomSetup_AzPowerShell_Format:"Azure PowerShell {0}",IRCreation_SSIS_CustomSetup_AzPowerShell_Version_Description:"Enter the version of Azure PowerShell to install, e.g. 4.4.0",IRCreation_SSIS_CustomSetup_AzPowerShell_Version_Description_Link:'(See a list of supported versions <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2148919">here</a>)',IRCreation_SSIS_CustomSetup_AzPowerShell_Version_PlaceHolder:"Your Azure PowerShell version number (x.y.z)",IRCreation_SSIS_CustomSetup_Msdtc:"Configure MSDTC",IRCreation_SSIS_CustomSetup_MsdtcEnabled:"Enable MSDTC",IRCreation_SSIS_CustomSetup_Msdtc_DtcAccess:"Network DTC access",IRCreation_SSIS_CustomSetup_Msdtc_ClientAndAdmin:"Client and administration",IRCreation_SSIS_CustomSetup_Msdtc_Allow_RemoteClients:"Allow remote clients",IRCreation_SSIS_CustomSetup_Msdtc_Allow_RemoteAdmin:"Allow remote administration",IRCreation_SSIS_CustomSetup_Msdtc_TransactionManager:"Transaction manager communication",IRCreation_SSIS_CustomSetup_Msdtc_Allow_Inbound:"Allow inbound",IRCreation_SSIS_CustomSetup_Msdtc_Allow_Outbound:"Allow outbound",IRCreation_SSIS_CustomSetup_Msdtc_Authentication_Matual:"Mutual authentication required",IRCreation_SSIS_CustomSetup_Msdtc_Authentication_Incoming:"Incoming caller authentication required",IRCreation_SSIS_CustomSetup_Msdtc_Authentication_NoAuth:"No authentication required",IRCreation_SSIS_CustomSetup_Msdtc_XATransaction:"Enable XA transactions",IRCreation_SSIS_CustomSetup_Msdtc_LUTransaction:"Enable SNA LU 6.2 transactions",IRCreation_SSIS_CustomSetup_EnvrionmentVariable:"Add environment variable",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Format:"{0}={1}",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Name_Label:"Variable name",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Name_Placeholder:"Your environment variable name",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Name_Description:"Enter the name of environment variable to be added on your Azure-SSIS Integration Runtime",IRCreation_SSIS_CustomSetup_EnvironmentVariable_Name_Error_Duplicated:"The variable '{0}' already exists.",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Value_Label:"Variable value",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Value_Placeholder:"Your environment variable value",IRCreation_SSIS_CustomSetup_EnvrionmentVariable_Value_Description:"Enter the value of environment variable to be added on your Azure-SSIS Integration Runtime",IRCreation_SSIS_PackageStore_Add_Title:"Add package store",IRCreation_SSIS_PackageStore_Edit_Title:"Edit package store",IRCreation_SSIS_PackageStore_Name_PlaceHolder:"Your package store name",IRCreation_SSIS_PackageStore_Name_Error:"The name of the package store '{0}' already exists.",IRCreation_SSIS_PackageStore_LinkedService_Label:"Package store linked service",IRCreation_SSIS_PackageStore_LinkedService_Description:"Select the linked service for file system/Azure Files/Azure SQL Managed Instance where your packages are stored or create a new one of your package store",IRCreation_SSIS_PackageStore_TYPE_AzureFileStorage:"Azure Files",IRCreation_SSIS_PackageStore_Type_Unsupported:"Unsupported Type",IRCreation_SSIS_Server_Label:"Catalog database server endpoint",IRCreation_SSIS_Username_Label:"Admin username",IRCreation_SSIS_Password_Label:"Admin password",IRCreation_SSIS_Tier_Label:"Catalog database service tier",IRCreation_SSIS_Executions_Per_Node_Label:"Maximum parallel executions per node",IRCreation_SSIS_Custom_Setup_Container_SAS_URI_Label:"Custom setup container SAS URI",IRCreation_SSIS_Service_Access:"Allow Azure services to access",IRCreation_SSIS_Error_Subscription_Not_Chosen:"You should choose a subscription for catalog database server.",IRCreation_SSIS_Error_Database_Not_Chosen:"You should choose a database.",IRCreation_SSIS_Error_Admin_Username_empty:"Admin username is empty",IRCreation_SSIS_Error_Admin_Password_empty:"Admin password is empty",IRCreation_SSIS_Error_Dual_Standby_Pair_Name_empty:"Dual standby pair name is empty",IRCreation_SSIS_Error_Azure_Batch_Not_Registered:"Fail to register to Azure Batch resource provider.",IRCreation_SSIS_Warning_Azure_Batch_Not_Registered:"We are unable to register Azure Batch as a resource provider for subscription containing the selected VNet, which is required for the provisioning of your IR. Please ensure that the subscription owner has completed this action, {0} ({1}).",IRCreation_SSIS_Error_Cannot_Find_Vnet:"Fail to find VNet",IRCreation_SSIS_Error_Vnet_Deprecation:"Classic VNet will not be supported for Azure-SSIS IR to join in the near future, so please use Azure Resource Manager (ARM) VNet instead.",IRCreation_SSIS_Error_Sharing_NotSupported:"To share your Azure-SSIS Integration Runtime, its node size must have at least 8 cores.",IRCreation_SSIS_Name_Description:"The name of your Azure-SSIS Integration Runtime",IRCrestion_SSIS_Description_Description:"The description of your Azure-SSIS Integration Runtime",IRCreation_SSIS_Location_Description:"The location of your Azure-SSIS Integration Runtime \u2013 Recommended to be the same as the location of your Azure SQL Database/Managed Instance server to host SSISDB and or VNet connected to your on-premises network to minimize cross-location traffics",IRCreation_SSIS_Node_Size_Description:"The node size of your Azure-SSIS Integration Runtime \u2013 Scale up to run compute/memory \u2013intensive packages",IRCreation_SSIS_Node_Number_Description:"The cluster size of your Azure-SSIS Integration Runtime \u2013 Scale out to run packages in parallel across multiple nodes",IRCreation_SSIS_Sku_Description:"SQL edition/license of your Azure-SSIS Integration Runtime components \u2013 Select Enterprise edition/license to use advanced components",IRCreation_SSIS_SQL_Setting_Subscription_Description:"Azure subscription(s) in which to select your Azure SQL Database/Managed Instance server to host SSISDB",IRCreation_SSIS_SQL_Setting_Location_Description:"The location of your Azure SQL Database/Managed Instance server to host SSISDB \u2013 Recommended to be the same as the location of your Azure-SSIS Integration Runtime to minimize cross-location traffics",IRCreation_SSIS_Managed_Instances_Public_Endpoint_Description:"Since your Azure SQL Managed Instance has a public endpoint enabled with limited access via port 3342, it is optional for your Azure-SSIS Integration Runtime to join a VNet.",IRCreation_SSIS_Endpoint_Description:"The endpoint of your Azure SQL Database/Managed Instance server to host SSISDB \u2013 Use it to connect SSDT/SSMS",IRCreation_SSIS_Username_Description:"The admin username of your Azure SQL Database/Managed Instance server \u2013 Required for us to prepare/manage SSISDB on your behalf",IRCreation_SSIS_Password_Description:"The admin password of your Azure SQL Database/Managed Instance server \u2013 Required for us to prepare/manage SSISDB on your behalf",IRCreation_SSIS_Tier_Description:"The service tier of your Azure SQL Database server \u2013 Use it to scale SSISDB performance, not applicable for Azure SQL Managed Instance \u2013 This setting reflects the selected service tier at provisioning time, visit Azure SQL Database portal for the current setting",IRCreation_SSIS_Allow_Service_Description:"Required for us to prepare/manage SSISDB on your behalf \u2013 Not applicable for Azure SQL Database with Vnet Service endpoint/Managed Instance",IRCreation_SSIS_Executions_Per_Node_Description:"The concurrency level of your Azure-SSIS Integration Runtime nodes \u2013 Run a single package using more than one cores or run one/more packages in a single core",IRCreation_SSIS_Custom_Setup_Container_SAS_URI_Description:"Shared Access Signature (SAS) Uniform Resource Identifier (URI) of Azure Storage blob container where your custom setup script and associated files are stored to be downloaded and executed on your Azure-SSIS Integration Runtime. V2-series node sizes are no longer supported with custom setup. Please use v3-series ones instead.",IRCreation_SSIS_Vnet_Description:"Only required if you use Azure SQL Database with VNet service endpoints/Managed Instance with a private endpoint to host SSISDB, need on-premises data access without configuring Self-Hosted Integration Runtime, or need static public IP addresses for your Azure-SSIS Integration Runtime",IRCreation_SSIS_Vnet_Subscription_Description:"Azure subscription(s) in which to select a VNet for your Azure-SSIS Integration Runtime to join",IRCreation_SSIS_Vnet_Location_Description:"The location of VNet for your Azure-SSIS Integration Runtime to join \u2013 Required to be the same as the location of your Azure-SSIS Integration Runtime",IRCreation_SSIS_Vnet_Type_Description:"The type of VNet for your Azure-SSIS Integration Runtime to join",IRCreation_SSIS_Vnet_Name_Description:"The name of VNet for your Azure-SSIS Integration Runtime to join \u2013 If you don't have any, create a new one on Azure portal and click the Refresh button to select it afterwards",IRCreation_SSIS_Subnet_Name_Description:"The name of subnet for your Azure-SSIS Integration Runtime to join",IRCreation_SSIS_Vnet_ModeType:"VNet injection method",IRCreation_SSIS_Vnet_ModeType_Express:"Express",IRCreation_SSIS_Vnet_ModeType_Description:"The method to inject your Azure-SSIS Integration Runtime into a VNet \u2013 The express method starts your Azure-SSIS Integration Runtime faster and has less inbound/outbound traffic requirements, but it has some limitations \u2013 See our documentation for more details",IRCreation_SSIS_Ip_Description:"Check to assign static public IP addresses to your Azure-SSIS Integration Runtime that can be allowed on the firewall of your data stores - Uncheck to assign dynamic public IP addresses to your Azure-SSIS Integration Runtime",IRCreation_SSIS_Ip_Value_Description:"Two unused standard static public IP addresses with DNS name are required for your Azure-SSIS Integration Runtime \u2013 If you don't have any, create new ones/modify existing ones on Azure portal to meet the selection requirements and click the Refresh button to select them afterwards",IRCreation_SSIS_IpOptionIncorrectFormat:"The Ip name or Ip id is null when updating static public Ip options.",IRCreation_SSIS_Save_Money_Label:"Save money",IRCreation_SSIS_Save_Money_Detail_Label:"Save with a license you already own. Already have a SQL Server license?",IRCreation_SSIS_Save_Money_Description:'By selecting "yes", I confirm I have a SQL Server license with Software Assurance\u200b to apply this ',IRCreation_SSIS_Ahub:"Azure Hybrid Benefit for SQL Server",IRCreation_SSIS_SAMI_Label:"Use AAD authentication with the system managed identity for {0}",IRCreation_SSIS_UAMI_Label:"Use AAD authentication with a user-assigned managed identity for {0}",IRCreation_SSIS_Use_Dual_Standby_Pair:"Use dual standby Azure-SSIS Integration Runtime pair with SSISDB failover",IRCreation_SSIS_Use_Dual_Standby_Pair_Description:"This allows you to have a pair of running Azure-SSIS Integration Runtimes to support SSISDB failover \u2013 At any given time, only one Azure-SSIS Integration Runtime can access SSISDB to execute packages deployed there (primary role), while the other can execute packages deployed somewhere else, for example in Azure Files (secondary role) \u2013 When SSISDB failover happens, the primary and secondary Azure-SSIS Integration Runtimes will swap roles",IRCreation_SSIS_Use_Dual_Standby_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/configure-bcdr-azure-ssis-integration-runtime">here</a>)',IRCreation_SSIS_Dual_Standby_Pair_Name:"Dual standby pair name",IRCreation_SSIS_Dual_Standby_Pair_Name_Description:"Please enter the same pair name when provisioning the primary and secondary Azure-SSIS Integration Runtimes.",IRCreation_SSIS_SAMI_Description:"This allows you to use Azure Active Directory (AAD) authentication with the system managed identity for {0} \u2013 Please add the managed identity into an AAD group with access permissions to your catalog database server.",IRCreation_SSIS_UAMI_Description:"This allows you to use Azure Active Directory (AAD) authentication with a user-assigned managed identity for {0} \u2013 Please assign the managed identity you created to {1} and grant it access permissions to your catalog database server.",IRCreation_SSIS_Credential_Name:"Credential name",IRCreation_SSIS_IR_Price_Label:"Please be aware that the cost estimate for running your Azure-SSIS Integration Runtime is",See:"see",IRCreation_SSIS_IR_Price_Message:"for current prices.",IRCreation_SSIS_Link_Html:'(See how to enable it <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/enable-aad-authentication-azure-ssis-ir">here</a>)',IRCreation_SSIS_Managed_Instances_Same_region_Description:"Since your Azure SQL Managed Instance is joined to a VNet, your Azure-SSIS Integration Runtime must join the same VNet, but in a different subnet in the next step.",IRCreation_SSIS_Managed_Instances_Different_region_Description:"Since your Azure SQL Managed Instance is joined to a VNet, but your Azure-SSIS Integration Runtime is in a different region, it must join another VNet that is connected/peered to that VNet in the next step.",IRCreation_SSIS_SQL_With_Vnet_Same_Region_Description:"Since your Azure SQL Database connectivity is isolated with VNet service endpoints, your Azure-SSIS Integration Runtime must join the same VNet and subnet in the next step.",IRCreation_SSIS_SQL_With_Vnet_Different_Region_Description:"Since your Azure SQL Database connectivity is isolated with VNet service endpoints, your Azure-SSIS Integration Runtime must be in the same region to join the same VNet and subnet.",IRCreation_SSIS_Summary_Description:"Your Azure-SSIS Integration Runtime (IR) is created with the following settings: ",IRCreation_SSIS_Azure_Data_Factory_Settings:"Azure {0} Settings",IRCreation_SSIS_Rescource_Group_Text:"Resource group: ",IRCreation_SSIS_Location_Text:"Location: ",GeneralSettingsLabel:"General settings",IRCreation_SSIS_Description_Text:"Description: ",IRCreation_SSIS_Node_Size_Text:"Node size: ",IRCreation_SSIS_Node_Number_Text:"Node number: ",IRCreation_SSIS_Edition_Text:"Edition: ",IRCreation_SSIS_Azure_Hybrid_benifit_Text:"Azure Hybrid Benefit: ",SQLSettingsLabel:"SQL settings",DeploymentSettingsLabel:"Deployment settings",IRCreation_SSIS_Catalog_Endpoint_Text:"Catalog database server endpoint: ",IRCreation_SSIS_Catalog_Location_Text:"Catalog database server location: ",IRCreation_SSIS_Catalog_Service_Tier_Text:"Catalog database service tier: ",IRCreation_SSIS_Parallel_Per_Node_Text:"Maximum parallel executions per node: ",IRCreation_SSIS_Custom_Setup_URI_Text:"Custom setup container SAS URI: ",IRCreation_SSIS_Custom_Setup_Without_Script_Text:"Express custom setup:",IRCreation_SSIS_Custom_Setups_Without_Script_Text:"Express custom setups:",IRCreation_SSIS_Custom_Setups_Without_Script_Summary_Format:"{0}: {1}",IRCreation_SSIS_VNet_Name_Text:"VNet name: ",IRCreation_SSIS_Subnet_Name_Text:"Subnet name: ",IRCreation_SSIS_VNet_ModeType_Text:"VNet injection method: ",IRCreation_SSIS_SelfhostedIR_Text:"Self-Hosted Integration Runtime: ",IRCreation_SSIS_BlobLinkedService_Text:"Staging storage linked service: ",IRCreation_SSIS_StoragePath_Text:"Staging path: ",IRCreation_SSIS_SharingDatafactory_Text:"Shared with other Data Factories: ",IRCreation_SSIS_StaticIp_Text:"Static public ip addresses: ",IRCreation_SSIS_Catalog_In_Vnet_Warning_Inner_Html:"If you are using Azure SQL Database w/ VNet service endpoints or Managed Instance, join your Azure-SSIS IR to the appropriate VNet, click <strong>{0}</strong> to do so.",IRCreation_SSIS_Catalog_MI_Subnet_Warning_Inner_Html:"Select a different subnet than the one used for your Managed Instance, click <strong>{0}</strong> to do so.",IRCreation_SSIS_Catalog_Without_Vnet_Warning_Inner_Html:"If you need to access data on premises, join your Azure-SSIS IR to a VNet connected to your on-premises network, click <strong>{0}</strong> to do so.",IRCreation_SSIS_Catalog_Without_Vnet_Warning:"Join your Azure-SSIS IR to a VNet connected to your on-premises network OR",IRCreation_SSIS_Catalog_Without_Vnet_and_SelfHostedIr_Warning_Inner_Html:"If you need to access data on premises, click <strong>{0}</strong> to do any of the followings:",IRCreation_SSIS_Catalog_Without_SelfHostedIr_Warning_Inner_Html:"If you need to set up Self-Hosted Integration Runtime as a proxy, click <strong>{0}</strong> to do so.",IRCreation_SSIS_Change_Setting_Description_Inner_Html:"If you want to change any of the above settings, click <strong>{0}</strong> to do so.",IRCreation_SSIS_Running_Package_Description_Inner_Html:'Once your Azure-SSIS IR is running, you can execute your packages on it after <a target="_blank" href="https://docs.microsoft.com/sql/integration-services/lift-shift/ssis-azure-deploy-run-monitor-tutorial?view=sql-server-2017#deploy-a-project-with-the-deployment-wizard">deploying</a> them into SSISDB hosted by ',IRCreation_SSIS_Running_Package_Description_Inner_Html_Support_File:'Once your Azure-SSIS IR is running, you can execute your packages on it after <a target="_blank" href="https://docs.microsoft.com/sql/integration-services/lift-shift/ssis-azure-deploy-run-monitor-tutorial?view=sql-server-2017#deploy-a-project-with-the-deployment-wizard">deploying</a> them into your file system/Azure Files/SSISDB hosted by ',IRCreation_SSIS_Running_Package_Description_Inner_Html_NoDb:'Once your Azure-SSIS IR is running, you can execute your packages on it after <a target="_blank" href="https://docs.microsoft.com/sql/integration-services/lift-shift/ssis-azure-deploy-run-monitor-tutorial?view=sql-server-2017#deploy-a-project-with-the-deployment-wizard">deploying</a> them into your file system/Azure Files.',IRCreation_SSIS_Manage_Cost_Description_Inner_Html:'To manage the running cost of your Azure-SSIS IR, you can <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/manage-azure-ssis-integration-runtime">stop & restart</a> it whenever convenient or <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime">schedule</a> it just in time.',IRCreation_SSIS_Link_SSISDB_Label:"Create SSIS catalog (SSISDB) hosted by Azure SQL Database server/Managed Instance to store your projects/packages/environments/execution logs",IRCreation_SSIS_Link_SSISDB_Description:"Check if you use SSIS project deployment model that stores your packages in SSIS catalog (SSISDB), uncheck if you use SSIS package deployment model that stores your packages in file system/Azure Files/SQL Server database (MSDB) \u2013 Regardless of your deployment model, check if you want to use SQL Server Agent hosted by Azure SQL Managed Instance to orchestrate/schedule your package executions",IRCreation_SSIS_Link_SSISDB_Description_Link:'(See more info <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/create-azure-ssis-integration-runtime">here</a>)',IRCreation_SSIS_PackageStore_Label:"Create package stores to manage your packages that are deployed into file system/Azure Files/SQL Server database (MSDB) hosted by Azure SQL Managed Instance",IRCreation_SSIS_PackageStore_Description:'Check if you use SSIS package deployment model and want to manage your packages via package stores \u2013 Check the above "Create SSIS catalog\u2026" check box too, if you want to use SQL Server Agent hosted by Azure SQL Managed Instance to orchestrate/schedule your package executions',IRCreation_SSIS_PackageStore_Summary_Text:"Package stores:",IRCreation_SSIS_PackageStore_ItemList_Text:"{0}({1})",IRCreation_SSIS_VnetUnchecked_Warning:'If access to your Azure SQL Database server is disabled from other Azure services/resources, please select a VNet for your Azure-SSIS Integration Runtime to join, so it can access SSISDB \u2013 Alternatively, please configure the "Firewall and virtual networks" settings of your Azure SQL Database server on Azure portal to enable the "Allow Azure services and resources to access this server" property.',IRCreation_SelectScenario_DataMovement_Description:"Perform data flows, data movement and dispatch activities to external compute.",IRCreation_SSIS_Link_SSISDB_In_Server_Label:"Create SSISDB hosted by {0}",IRCreation_SSIS_Azure_Sql_Require_Join_IR_To_Vnet:"Since your Azure SQL database server is joined to a VNet ({0}), your Azure-SSIS Integration Runtime must join the same VNet, but in a different subnet for you to select.",IRCreation_SSIS_MI_Required_Join_IR_To_Vnet:"Since your Azure SQL Managed Instance is joined to a VNet ({0}), your Azure-SSIS Integration Runtime must join the same VNet, but in a different subnet for you to select.",IRCreation_SSIS_Modify_Settings:"Modify settings",IRCreation_SelectScenario_Description:"Integration Runtime is the native compute used to execute or dispatch activities. Choose what integration runtime to create based on required capabilities.",IRCreation_SelectScenario_SSIS_Description:"Lift-and-shift existing SSIS packages to execute in Azure.",IRCreation_SelectType_SSIS_Description:"Create a new Azure-SSIS Integration Runtime.",IRCreation_SelectType_Linked_SSIS_Description:"Link to a shared Azure-SSIS Integration Runtime in another Data Factory.",IRCreation_SSIS_LinkedNode_Description:"Use your shared Azure-SSIS Integration Runtime in another Data Factory \u2013 It will appear as your linked Azure-SSIS Integration Runtime in this Data Factory",IRCreation_SSIS_LinkedNode_Description_Tooltip:"Please grant permission to {0} when configuring your shared Azure-SSIS Integration Runtime in another Data Factory",IRCreation_SSIS_LinkedNode_ManuallyInput_ResourceId_Description:"Paste the copied resource ID of your shared Azure-SSIS Integration Runtime in another Data Factory",IRCreation_SelectNetwork_NetworkEnvironment:"Network environment:",IRCreation_SelectNetwork_NetworkEnvironment_Description:"Choose the network environment of the data source / destination or external compute to which the integration runtime will connect to for data flows, data movement or dispatch activities:",IRCreation_SelectNetwork_SelfhostedIR_ShortDescription:"Use this for running activities in an on-premises / private network",IRCreation_SelectScenario_DataMovement_Title:"Azure, Self-Hosted",IRCreation_SelectNetwork_SelfhostedIR_LongDescription:"Use this for running data movement, external and pipeline activities in an on-premises / private network by installing the integration runtime. \n\nNote: Data flows are only supported on Azure integration runtime. You can use self-hosted integration runtime to stage the data on cloud storage and then use data flows to transform it. ",IRCreation_SelectNetwork_Description:"Choose the network environment of the data source/destination or external compute to which the integration runtime will connect to for data movement or dispatch activities:",IRCreation_SelectNetwork_ExternalResource:"External Resources:",IRCreation_SelectNetwork_ExternalResource_Description:"You can use an existing self-hosted integration runtime that exists in another resource. This way you can reuse your existing infrastructure where self-hosted integration runtime is setup.",IRCreation_SelectNetwork_AzureIR_Description:"Use this for running data flows, data movement, external and pipeline activities in a fully managed, serverless compute in Azure.",IRCreation_SelectNetwork_SelfhostedIR_Description:"Choose this if you are accessing services in private network like your On-Premises environment, or in Virtual Network environments like Azure Virtual Network.",IRCreation_SelectNetwork_LinkedIR_Title:"Linked Self-Hosted",IRCreation_SelectNetwork_TopLevelLinkedIR_Title:"Linked Data Management Runtime",IRCreation_SelectNetwork_AzureVnetIR_Title:"Azure VNet",IRCreation_SelectNetwork_AzureVnetIR_Description:"Use this for running activities in Azure Virtual Network.",IRCreation_SelectNetwork_Airflow_Description:"Use this for running your existing DAGs",IRCreation_Airflow_Blade_Description:"Use this interface to setup and create your Airflow integration runtime environment",IRCreation_Airflow_Auth_Type:"Airflow auth type",IRCreation_Airflow_Azure_AD_Label:"Azure AD authentication",IRCreation_Airflow_Edit_Credentials_Label:"Edit airflow credentials",IRCreation_Airflow_Username_Label:"Airflow username",IRCreation_Airflow_Password_Label:"Airflow password",IRCreation_Airflow_Credentials_Cant_Be_Empty_Description:"Username and password can't be empty",IRCreation_Airflow_Credentials_Cant_Match_Description:"Username and password can't match",IRCreation_Airflow_Region_Description:"Airflow IR region defaults to the Data Factory region. To create an Airflow IR in a different region, create a new datafactory in the required region",IRCreation_Compute_Size:"The size of the compute node you want your Airflow environment to run on. You initially get 3 nodes",IRCreation_Airflow_Autoscale:"Enable autoscale (Coming Soon)",IRCreation_Select_Extra_Nodes:"Extra nodes",IRCreation_Extra_Nodes:"Each additional node will add 3 workers",IRCreation_Select_Airflow_Version:"Airflow version",IRCreation_Airflow_Version:"The version of Airflow you wish to use",IRCreation_Select_Airflow_Configuration_Overrides:"Airflow configuration overrides",IRCreation_Airflow_Configuration_Overrides:"Airflow configuration overrides you wish to use. E.g. name: AIRFLOW__VAR__FOO, value: BAR",IRCreation_Airflow_Configuration_Overrides_Name_Validation:"Variable 'name' must follow the following format 'AIRFLOW__{SECTION}__{KEY}'",IRCreation_Airflow_Configuration_Overrides_Value_Validation:"Variable 'value' can't be empty",IRCreation_Select_Environment_Variables:"Environment variables",IRCreation_Environment_Variables:"Environment variables you wish to use. E.g. name: hello, value: world",IRCreation_Environment_Variables_Validation:"Variable 'name' and 'value' can't be empty",IRCreation_Select_Airflow_Requirements:"Airflow requirements",IRCreation_Airflow_Requirements:'Python libraries you wish to use. E.g. "flask-bcrypy=0.7.1". Can be a comma delimited list',IRCreation_Airflow_Kubernetes_Secret_Label:"Kubernetes secrets",IRCreation_Airflow_Secret_Description:"Custom secrets you wish to add to your airflow aks environment",IRCreation_linkedIR_TopLevelConnect:"Data Management Runtime connect",IRCreation_linkedIR_TopLevel:"Data Management Runtime",ConnectSelectionMethod:"Connect selection method",IRCreation_SelectType_LinkedIR_Title:"Linked SSIS",IRCreation_DataFlow_Pricing:"Billing for data flows is based upon the type of compute you select and the number of cores selected per hour. If you set a TTL, then the minimum billing time will be that amount of time. Otherwise, the time billed will be based on the execution time of your data flows and the time of your debug sessions. Note that debug sessions will incur a minimum of 60 minutes of billing time unless you switch off the debug session manually. For further details, please click ",IRCreation_DataFlow_Pricing_A365:"Billing for data flows is based upon the type of compute you select and the number of cores selected per hour. Debug sessions will incur a minimum of 60 minutes of billing time unless you switch off the debug session manually. For further details, please click ",IRCreation_DataFlow_Pricing_End:"for the pricing page.",IRDelete_Confirm_Description_DataMovement_Temp:"Are you sure you want to delete {0}: {1}?",IRDelete_ErrorTemplate:"Failed to delete {0}.\nDetail: {1}",IRDelete_Error_Model:"Cannot get IR model",IRSharing_RevokePermission_Title:"Remove permission",IRSharing_RevokePermission_Description:'Are you sure you want to remove the permission to "{0}" for the integration runtime "{1}"?\n\nPlease note this action will not impact linked IRs that have already been created. If you want to keep existing linked IRs from accessing "{0}", please go to "{2}" tab and click "{3}" button.',IRSharing_RevokePermission_Inherited_Description:'The permission for "{0}" cannot be removed here as it is inherited from the parent resource of the integration runtime "{1}". If you want to remove this permission, please go to <a target="_blank" href="{2}">Azure Portal</a> and remove it.',IRSharing_RevokePermission_Error:"Failed to remove permission.\nDetail: {0}",IRSharing_RemoveLinkedIR_Title:"Remove linked integration runtimes",IRSharing_Unlink_Description:'Are you sure you want to remove the linked integration runtimes from accessing this integration runtime "{1}"?\n\nIf you do so, you will no longer be able to use any of the linked integration runtimes in "{0}".',IRSharing_SelectedDataFactories:"Selected {0}:",IRSharing_AddPermission:"Assign permissions",IRSharing_SearchPlaceholder:"Search by name or service identity application ID",IRSharing_Candidate_Tooltip:"Service identity application ID:\n{0}",IRSharing_SearchNoResult:"We didn't find {0}. Try another name or service identity.",IRSharing_SearchNoResult2:"Please make sure the above searched resource exists or it has the service identity attached.",IRSharing_Search_RequestDenied:'The search functionality currently does not work for Active Directory Guest users. If you are an Active Directory Guest User, select "Enter Manually" to assign the permission.',IRSharing_Search_OtherError:"Failed to list the resources.\nDetail: {0}",IRSharing_Manual_ServiceIdentityId:"Service Identity ID",IRSharing_Manual_ServiceIdentityId_Placeholder:'Please enter the "SERVICE IDENTITY ID" of the {0} to grant permission',IRSharing_Manual_ServiceIdentityId_Error:'Please enter a valid "SERVICE IDENTITY ID"',IRSharing_Manual_ServiceIdentityId_ErrorWhenAdd:'The input "{0}" is not a valid SERVICE IDENTITY ID. Please enter the correct SERVICE IDENTITY ID',IRSharing_GrantPermissionError:"Error occurred when grant permission to {0}. \nError: {1}",IRSharing_GrantPermissionError_1:"Error occurred when grant permission to {0}.",IRSharing_GetRoleError:"Failed to get current {0}'s Contributor role.",IRSharing_UpgradeFromKeyAuth_Description_1:"Note: The feature of sharing self-hosted integration runtime is changed to use role-based access control (RBAC). Please follow",IRSharing_UpgradeFromKeyAuth_Description_2:"this documentation",IRSharing_UpgradeFromKeyAuth_Description_3:"to upgrade this linked integration runtime.",IRSharing_AccessDenied_Description_1:"This linked integration runtime(IR) doesn't have the permission to access the shared IR '{0}' in '{1}'. Please consider one of the following actions to make it available.",IRSharing_AccessDenied_Description_2:"Grant access to '{0}' from the shared IR '{1}'. Go to the shared IR -> edit -> sharing tab -> grant access.",IRSharing_AccessDenied_Description_3:"You can modify the resource ID accordingly to link it to another shared IR to which '{0}' has access to. In this approch, you might have to reset your existing linked services referencing this IR.",IRSharing_AccessDenied_Description_4:'After completing one of the above actions, please click "Update".',IRNodeDelete_Confirm_Title:"Delete integration runtime node",IRNodeDelete_Confirm_Description_DataMovement:"Are you sure you want to delete this integration runtime node: {0}?",IntegrationRuntimeDetailMain:"Integration runtime item main",IntegrationRuntimeNameInsertLabel:"Integration runtime: {0}",IntegrationRuntimeRepairConfirm:"Do you want to repair this integration runtime?",IREdit_Nodes_Title:"Nodes",IREdit_AutoUpdate_Title:"Auto update",IREdit_Sharing_Title:"Sharing",IREdit_Links_Title:"Links",IREdit_InteractiveQuery_Title:"Interactive authoring",IREdit_CopyComputeScale_Title:"Copy compute scale",IREdit_PipelineAndExternalComputeScale_Title:"Pipeline and external compute scale",IRStop_Confirm_Title:"Are you sure you want to stop integration runtime {0}?",IRStop_Confirm_Description:"Stopping integration runtime will immediately cancel all package executions and release all allocated nodes.",External:"External",IRMonitor_SharedCount:"Linked count",IRMonitor_SSISSourceDF:"Sharing {0}",IRMonitor_SourceDF:"Shared {0}",IRMonitor_SourceIR:"Shared integration runtime",IRMonitor_SharedDf:"Linked {0}",IRMonitor_SharedIR:"Linked integration runtime",IRMonitor_SharedIRs:"Linked integration runtimes",IRMonitor_CreateTime:"Create time",IRMonitor_Node_Role_Dispatcher:"Dispatcher/worker",IRMonitor_Node_Credential_InSync:"In sync",IRMonitor_Node_Credential_NotInSync:"Not in sync",IRMonitor_QueueLength:"Queue length",IRMonitor_QueueDuration:"Average queue duration",IRError_SSIS_LastOperation:"Last operation '{0}' get the status '{1}'.",IRError_SSIS_AzureSqlConnectionFailure:"Failed to connect to Azure SQL DB server due to sql error '{0}', message: {1}",IRError_SSIS_CatalogDbAlreadyExist:'To provision new Integration Runtime, Azure SQL DB server should not have existing SSISDB. If this SSISDB was once associated with an SSIS IR, you can re-associate it with another SSIS IR (for more info, see <a target="_blank" href="https://aka.ms/ssis-attachanexistingssisir">here</a>).',IRError_SSIS_InputCannotBeNullOrEmpty:"The required property '{0}' in payload is null or empty.",IRError_SSIS_IntegrationRuntimeCannotModify:"Integration Runtime can only be updated/deleted when it is in 'Initial' or 'Stopped' state. Current state: '{0}'.",IRError_SSIS_IntegrationRuntimeCannotStart:"Integration Runtime can only be started when it is in 'Initial' or 'Stopped' state. Current state: '{0}'.",IRError_SSIS_IntegrationRuntimeCannotStop:"Integration Runtime can only be stopped when it is in 'Started' state. Current state: '{0}'.",IRError_SSIS_InvalidCatalogDbVersion:"SSISDB version is invalid, please delete it and start Integration Runtime again.",IRError_SSIS_InvalidIntegrationRuntimeNodeNumber:"Invalid node number. The node number for each Integration Runtime should be between 1 and {0}.",IRError_SSIS_InvalidVnetConfiguration:"VNet configuration is invalid. Code : {0}, Message: {1}. Please see https://docs.microsoft.com/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network to configure your VNet.",IRError_SSIS_InvalidPublicIPSpecified:"Invalid public IP address is specified. Code : {0}, Message: {1}. Please see https://docs.microsoft.com/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network to setup the static public IP addresses for your IR.",IRError_SSIS_InvalidEdition:"The specified edition for Azure-SSIS IR is invalid, please specify a valid edition which could be {0}.",IRError_SSIS_InvalidLicenseType:"The specified license type for Azure-SSIS IR is invalid, please specify a valid license type which could be {0}.",IRError_SSIS_PropertyValueNotEnabled:"The specified {0} option for Azure-SSIS IR is not enabled.",IRError_SSIS_IRAlreadyExists:"The Integration Runtime '{0}' already exists.",IRError_SSIS_IRNotSupportEncryption:"Encryption is not supported for Azure-SSIS Integration Runtime.",IRError_SSIS_ManagedIRAuthKeyOperationNotSupported:"Auth key operation is not supported for Azure-SSIS Integration Runtime.",IRError_SSIS_OperationNotAllowedInCurrentIRState:"The operation '{0}' is not allowed in current IntegrationRuntime state.",IRError_SSIS_UnsupportedIRType:"The IntegrationRuntime type '{0}' is not supported.",IRError_SSIS_CatalogPricingTierCannotBeNullOrEmptyForNonManagedInstance:"'catalogPricingTier' property cannot be null or empty when provisioning Integration Runtime with Azure SQL DB server that is not Managed instance.",IRError_SSIS_CatalogPricingTierCannotBeSpecifiedForManagedInstance:"'catalogPricingTier' property cannot be specified when provisioning Integration Runtime with Azure SQL DB server that is Managed Instance.",IRError_SSIS_NonAzureSqlDbServerIsNotAllowed:"Only Azure SQL DB server can be used when provisioning Integration Runtime.",IRError_SSIS_AzureSqlRegionError:"Provisioning in this region is not supported. Code {0}, Message : {1}",IRError_SSIS_InvalidCatalogPricingTier:"'catalogPricingTier' property is invalid. Code : {0}, Message : {1}",IRError_SSIS_CatalogEndpointCannotModifyForNonInitialIR:"'catalogServerEndPoint' property can only be updated when the IntegrationRuntime is in 'Initial' state.",IRError_SSIS_CatalogDbCreationFailure:"Failed to create catalog database. Sql Error Number: {0}, Message: {1}",IRError_SSIS_CatalogDbSchemaInstallationFailure:"Failed to create catalog tables/views and so on. Sql Error Number: {0}, Message: {1}. Refer to https://go.microsoft.com/fwlink/?linkid=2099434 for more details.",IRError_SSIS_CatalogDbUserCreationFailure:"Failed to create catalog user. Sql Error Number: {0}, Message: {1}. Refer to https://go.microsoft.com/fwlink/?linkid=2099434 for more details.",IRError_SSIS_CoreQuotaReached:"Your subscription has reached its quota for CPU cores.  Maximum allowed: {0}. If you would like to increase the quota, please contact customer support for assistance: https://azure.microsoft.com/support/options.",IRError_SSIS_CustomSetupScriptFailure:"Your Azure-SSIS IR has failed to start due to custom setup failure. Details: {0}.",IRError_SSIS_ExpressCustomSetupFailure_Format:"Your Azure-SSIS IR has failed to start due to express custom setup failure. Details: {0}",IRError_SSIS_ExpressCustomSetupFailure_Popup:"Failed to start the Azure-SSIS integration runtime: {0} because an express custom setup failure(s) occurred. For details, go to the ADF notification panel or Azure-SSIS IR monitoring page.",IRError_SSIS_ContainedDatabaseAuthenticationIsRequired:"To use Azure SQL Managed Instance with AAD authentication, please enable contained database authentication by running \"exec sp_configure 'contained database authentication', 1\".",IRError_SSIS_InvalidCatalogDb:"SSISDB is invalid. Sql Error Number: {0}, Message: {1}. Please delete SSISDB and start your SSIS Integration Runtime again.",IRError_SSIS_SqlInvalidElasticName:"Elastic pool name does not exist. Sql Error Number: {0}, Message: {1}",IRError_SSIS_AdfMsiNotFound:'"{0}" has no service identity generated. Please generate a service identity following https://aka.ms/O50nx3.',IRError_SSIS_InvalidMaxParallelCount:"The specified MaxParallelExecutionsPerNode is invalid. The value of MaxParallelExecutionsPerNode should be greater than 0.",IRError_SSIS_CustomSetupScriptBlobContainerInaccessible:"Your integration runtime cannot be upgraded and will eventually stop working, since we cannot access the Azure Blob container you provided for custom setup. Please ensure that the SAS URI does not expire and custom setup resources are always available during the whole lifecycle of your Azure-SSIS IR, from creation to deletion.",IRError_SSIS_CustomSetupScriptNotFound:"Your integration runtime cannot be upgraded and will eventually stop working, since there is no main.cmd file in the Azure Blob container you provided for custom setup. Please ensure that the SAS URI does not expire and custom setup resources are always available during the whole lifecycle of your Azure-SSIS IR, from creation to deletion.",IRError_SSIS_IntegrationRuntimeStartingLongerThanUsual:"Your integration runtime is taking longer than usual to start, so please restart or stop it.",IRError_SSIS_IntegrationRuntimeStoppingLongerThanUsual:"Your integration runtime is taking longer than usual to stop, so please retry to stop it.",IRError_SSIS_VNetResourceGroupLockedOrReferenceProvisionMessage:"We are unable to clean up unneeded resources after failing to start your IR. Please unlock the resource group containing your VNet and/or remove any reference to the resources created for your IR, and then stop your IR, fix the issues causing the failure, and restart your IR.",IRError_SSIS_VNetResourceGroupLockedOrReferenceDeprovisionMessage:"We are unable to clean up unneeded resources when stopping your IR.  Please unlock the resource group containing your VNet and/or remove any reference to the resources created for your IR, and then stop your IR, and then retry to stop your IR.",IRError_SSIS_JoinVNetPermissionNotGranted:"Express VNet injection method can't access this subnet ({0}). Consequently, please grant the required permission (Microsoft.Network/virtualNetworks/subnets/join/action) to ADF managed identity for your Azure-SSIS Integration Runtime to join this subnet.",IRError_SSIS_ExpressVNetInjectionNotEnabled:"Express VNet injection method isn't enabled for this subscription. To enable it, please contact customer support (https://azure.microsoft.com/support).",IRError_SSIS_ExpressVNetInjectionNotSupported:"Express virtual network injection feature is not supported for SSIS integration runtime in this region.",IRError_SSIS_InvalidPayloadFormat:"You can use either standard or express VNet injection method, but not both, for your Azure-SSIS Integration Runtime.",IRError_SSIS_ExpressVNetInjectionAlreadyUsed:"Express VNet injection method currently supports only one Azure-SSIS Integration Runtime per VNet and this VNet already has one ({0}). Consequently, please use another VNet.",IRError_SSIS_DiagnoseConnect_InvalideInput:"Please verify your input is correct.",IRError_SSIS_DiagnoseConnect_MisconfiguredDnsSettings:"If you\u2019re using your own DNS server in the VNet joined by your Azure-SSIS IR, please verify that it can resolve your host name.",IRError_SSIS_DiagnoseConnect_MisconfiguredNsgSettings:"Please verify that the NSG of your VNet allows outbound traffic through this port. If you\u2019re using Azure ExpressRoute and or a UDR, please verify that this port is open on your firewall/server.",IRError_SSIS_DiagnoseConnect_ServerNotAllowRemoteConenction:"Please verify that your server allows remote TCP connections through this port.",IRError_SSIS_DiagnoseConnect_FirewallOrNetworkIssue:"Please verify that this port is open on your firewall/server/NSG and the network is stable.  ",IRError_SSIS_DiagnoseConnect_NetworkInstable:"Test connection irregularly succeeded due to network instability.",IRError_SSIS_DiagnoseConnect_PSPingExecutionTimeout:"Test connection timeout, please try again later.",IRError_SSIS_DiagnoseConnect_GenericIssues:"Test connection failed due to generic issues.",IRError_SSIS_DiagnoseConnect_Succeeded:"Test connection succeeded.",IRError_Selfhosted_Offline:"Node is offline",IRError_Selfhosted_Upgrading:"Upgradation in progress",IRError_Selfhosted_NotSameVersion:"Self-hosted Integration Runtime version of this node is different than that of the integration runtime cluster. To resolve this, please upgrade all the nodes under this integration runtime so that they run on the same version.",IRError_Selfhosted_ConnecteToResourceManager:"This node has some connectivity issue with the dispatcher node. Please check the connectivity between the nodes within your network.",IRError_Selfhosted_ServiceBusConnection:"Cloud service cannot connect to the integration runtime through service bus. You may not be able to use the Copy Wizard to create data pipelines for copying data from/to on-premises data stores.\nTo resolve this, ensure there is no connectivity issues with Azure Relay. This requires enabling outbound communication to \u2018*.servicebus.windows.net\u2019 on Port \u2018443\u2019; either directly through this Integration Runtime or by using a Proxy Server.\nSee Ports and security considerations and Proxy server considerations in the Integration runtime article for details.\nAs a work-around in case Azure Relay connectivity cannot be established, code (or) Azure PowerShell to construct the pipelines (no UI authoring).",IRError_Selfhosted_HttpPort:'Credential Manager Application cannot connect to the Integration Runtime node through https port. You may not be able to set credentials for on-premises linked services. Please go to integration runtime machine, open "Integration Runtime Configuration Manager", switch to "Settings" tab and specify a valid port.',IRError_Selfhosted_CredentialNotSyncWorker:"The Integration Runtime (Self-hosted) node is trying to sync the credentials across nodes. It may take several minutes.\nIf this warning appears for over 10 minutes, please check the connectivity with Dispatcher node.",IRError_Selfhosted_CredentialNotSyncDispatcher:"The Integration Runtime (Self-hosted) node is trying to sync the credentials across nodes. It may take several minutes.\nIf this warning appears for over 10 minutes, you can try to force credential sync.",IRError_Selfhosted_ForceCredentialSync:"Force Credential Sync",IRError_Selfhosted_ForceCredentialSync_Start:"Starting to force sync the credentials for integration runtime.",IRError_Selfhosted_ForceCredentialSync_Finished:"Force sync the credentials for integration runtime is finished.",IRError_Selfhosted_NoEnoughDiskSpace:"We are unable to update your self-hosted integration runtime due to lack of free disk space. Please make sure you have 10GB of disk space for the update to happen.",IRError_NotExist:"Not Exist",IRError_NotFound:"Not Found",IRError_FailedToGetStatus:"Failed to get status",IRError_NotFoundInGit:"The integration runtime for {0} ({1}) cannot be found in the Git repository. Please make sure your Git repository is in-sync with your {2}.",IRError_NoPermission:"No permission. Please contact support or send feedback.",IRError_FailedQueryLog:"Failed to query log",IRError_ActivityLogMissingInformation:"Missing integration runtime or activity run information",IRError_NotFoud_Tooptip:'This integration runtime only exists in your Git repository but not in your {0}. Please make sure your Git repository is in-sync with your {0}. <a href="https://go.microsoft.com/fwlink/?linkid=2193931" target="_blank">Learn more</a>',IRError_ConcurrentJob_Min:"The value must be at least {0}",IRError_ConcurrentJob_Max:"The value must be at most {0}",IRError_ExpressCustomSetup_Invalid:"Invalid Setup Name",IRError_ExpressCustomSetup_ErrorMessage_Empty:"missing error message",IRError_ExpressCustomSetup_NotParsed:"Error message cannot be parsed: ",IRError_ExpressCustomSetup_ActivityId:"Activity ID: {0}",IR_EnableHATooltip:"High Availability is enabled when you have at least 2 nodes configured (running).",IR_SeeAzureSqlSettings:"See your Azure SQL Database/Managed Instance settings.",IR_CreatingSqlSettingsLink:"Creating Azure SQL Database/Managed Instance link...",IR_FailToFindSqlSettingsLink:"Cannot link to Azure SQL Database/Managed Instance. You may not have access to that.",IR_SeeFirstPublicIpSettings:"See your first static public IP address settings.",IR_SeeSecondPublicIpSettings:"See your second static public IP address settings.",IR_CreateFirstIpSettingsLink:"Creating the first static public IP address link...",IR_CreateSecondIpSettingsLink:"Creating the second static public IP address link...",IR_Resource_Start_Failed:"Failed to start the integration runtime: {0}.\nError: {1}",IR_Resource_Start_Failed_By_Express_Custom_Setup:"Failed to start the Azure-SSIS integration runtime: {0}",IR_Start_Succeeded_Title:"Successfully started",IR_Resource_Start_Succeeded:"Successfully started the integration runtime for {0} ({1}).",IR_Resource_Starting:"Starting the integration runtime: {0}.",IR_Resource_Stop_Failed:"Failed to stop the integration runtime: {0} ({1}).",IR_Stop_Failed_Title:"Failed to stop",IR_Successfully_Set_Up:"Successfully set up",IR_Successfully_Updated:"Successfully updated",IR_Failed_To_Set_Up:"Failed to set up",IR_Stop_Success_Title:"Successfully stopped",IR_Resource_Stop_Succeeded:"Successfully stopped the integration runtime for {0} ({1}).",IR_Resource_Stopping:"Stopping the integration runtime: {0} ({1}).",IR_VersionStatus_Title:"Current Version Status:",IR_VersionStatus_UpToDate:"The integration runtime is in latest version.",IR_VersionStatus_NewVersionNotPushed:"The auto update will be scheduled by {0}. You can manually update the integration runtime by downloading and installing the latest version from the download center.",IR_VersionStatus_UpdateAvailableScheduleTime:"An auto-update will be scheduled for {0}.",IR_VersionStatus_CannotGetScheduleTime:"You can manually update the integration runtime by downloading and installing the latest version from the download center.",IR_VersionStatus_NoNodeAvailable:"The auto update is not scheduled because there is no integration runtime node available.",IR_VersionStatus_Expiring:"The current version will be expired in 90 days. Please upgrade to latest version.",IR_VersionStatus_Expired:"The current version is expired. Please manually upgrade your integration runtime nodes to the latest version.",IR_VersionStatus_Unknown:"Unknown because there is no node attached to the integration runtime.",IR_VersionStatus_AutoUpdateDisabled_ButtonAvailable:'The auto-update has been turned off. To get the latest update, you can choose one of the following actions:\n1. Turn on the auto-update.\n2. Click "Update now" button above.\n3. Update the self-hosted integration runtime manually by downloading and installing the latest version from the download center.',IR_VersionStatus_AutoUpdateDisabled_ButtonUnAvailable:"The auto-update has been turned off. To get the latest update, you can choose one of the following actions:\n1. Turn on the auto-update.\n2. Update the self-hosted integration runtime manually by downloading and installing the latest version from the download center.",IR_VersionStatus_Loading:"Fetching version status...",IR_Current_Version:"Current version",IR_Update_Now:"Update now",IR_VersionStatus_NewVersion_Description:'A newer version ({0}) of self-hosted integration runtime is available in the {1}. <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/self-hosted-integration-runtime-auto-update#self-hosted-integration-runtime-auto-update">Learn more</a>',IR_AutoUpdate_Tooltip:"An auto-update will be scheduled for the next available version.",IR_ExpiredDate:"To be expired at {0}.",IR_Upgrade_ScheduleUpgradeTime_ChangeTitle:"Schedule upgrades at the following time:",IR_Upgrade_ScheduleUpgradeTime_ChangeButton:"Change schedule time",IR_Upgrade_ScheduleUpgradeTime_NotEnable_NoNode:"Integration runtime auto upgrade setting is not enabled due to there is no integration runtime node online.",IR_DisableAutoUpgrade_Title:"Turn off auto upgrade",IR_DisableAutoUpgrade_Desc:"Are you sure you want to turn off the auto upgrade on the integration runtime {0} ? Then you cannot automatically get the latest features of integration runtime.",IR_VNetValidation_Pass:"Validation successful!",IR_VNetValidation_Error:"VNet setting error:",IR_VNetValidation_Warning:"VNet setting warning:",IR_VNetValidation_ValidationError:"Validation error! '{0}'",IR_VNetValidation_SubnetValidationError:"Subnet validation error! '{0}'",IR_VNetValidation_VNetValidationError:"VNet validation error! '{0}'",IR_VNetValidation_ResourceGroupValidationError:"Resource group validation error! '{0}'",IR_VNetValidation_SubscriptionValidationError:"Subscription validation error! '{0}'",IR_VNetValidation_GetVNetFailed:"Get VNet '{0}' failed! {1}",IR_VNetValidation_GetSubnetFailed:"Get Subnet '{0}' failed! {1}",IR_VNetValidation_GetSqlVNetRuleFailed:"Get VNet rule(s) for SQL Server '{0}' failed! {1}",IR_VNetValidation_GetSqlFirewallRuleFailed:"Get firewall rule(s) for SQL Server '{0}' failed! {1}",IR_VNetValidation_GetNetworkSecurityGroupFailed:"Get network security group '{0}' failed! {1}",IR_VNetValidation_GetVNetUsageFailed:"Get usage of VNet '{0}' failed! {1}",IR_VNetValidation_GetSubnetUsageFailed:"Cannot find subnet '{0}' usage from vnet '{1}'!",IR_VNetValidation_GetResourceLockFailed:"We are unable to check the lock configured for resource containing the selected VNet. Please ensure that it does not restrict the provisioning of your IR. Please refer to the detailed error(s) below: {0}",IR_VNetValidation_GetResourceGroupAssignmentFailed:"We are unable to check the policy configured for resource group containing the selected VNet. Please ensure that it does not restrict the provisioning of your IR. Please refer to the detailed error(s) below: {0}",IR_VNetValidation_GetPolicyDefinitionFailed:"Get policy definition failed! {0}",IR_VNetValidation_GetAddressPrefexFailed:"Get address prefix failed for subnet '{0}'.",IR_VNetValidation_AzureServiceInboundBlock_Error:"Inbound port 29876, 29877 of Azure services is blocked by network security group '{0}'",IR_VNetValidation_AzureServiceInboundBlock_Warning:"Since the inbound ports 29876, 29877 of Azure services are blocked by network security group '{0}', please ensure Azure services can still access your Azure-SSIS Integration Runtime via its subnet.",IR_VNetValidation_AzureServiceOutboundBlock_Warning:"Since the outbound port 443 of Azure services is blocked by network security group '{0}', please ensure that your Azure-SSIS Integration Runtime can still access other Azure services via its subnet.",IR_VNetValidation_AzureSqlDatabaseOutboundBlock_Warning:"Since the outbound ports 1433, 11000-11999 of your Azure SQL Database server are blocked by network security group '{0}', please ensure that your Azure-SSIS Integration Runtime can still access SSISDB via its subnet.",IR_VNetValidation_GetRouteTableFailed:"Get route table '{0}' failed! {1}",IR_VNetValidation_RouteTable_NextHop_NoInternet:"Next Hop of 0.0.0.0/0 in route table '{0}' is '{1}'. Please either change Next Hop of this route to 'Internet' or add UDR for IP of BatchNodeManagement service tag.",IR_VNetValidation_CannotUseGatewaySubnet:"IR cannot join GatewaySubnet",IR_VNetValidation_CannotUseAzureFirewallSubnet:"IR cannot join AzureFirewallSubnet",IR_VNetValidation_CannotUseAzureBastionSubnet:"IR cannot join AzureBastionSubnet",IR_VNetValidation_CannotShareSubnetWithExternalResource:"IR cannot share subnet '{0}' with other external resources '{1}'",IR_VNetValidation_InsufficientIpAddress:"Insufficient subnet IP address range for your SSIS integration runtime: '{0}' IP addresses are required, but only '{1}' available.",IR_VNetValidation_AddressPrefexNotEnough:"Address prefix configuration is not meet with the requirement. Require a CIDR block below /{0}, but got /{1} for subnet.",IR_VNetValidation_DnsCanResolveAzureServices:"VNet is using custom DNS {0}. Please confirm it can resolve public Azure host names",IR_VNetValidation_ResourceCannotHasLock:"Please remove lock '{0}' from resource: '{1}'",IR_VNetValidation_ResourceGroupPolicyBlockNetwork:"Policy '{0}' of resource group '{1}' blocks either Microsoft.Network/LoadBalancers, Microsoft.Network/PublicIPAddresses or Microsoft.Network/NetworkSecurityGroups in some cases",IR_VNetValidation_SubnetDelegateService_Warning:"Subnet is delegated to services '{0}'. Integration Runtime start might fail if the subnet has delegation service, please switch to another subnet if fail",IR_VNetValidation_InvalidPublicIPCount_Error:"Please provide two static public IP addresses for your Azure-SSIS Integration Runtime.",IR_VNetValidation_SamePublicIps_Error:"The provided static public IP addresses are duplicated. Please provide two distinct ones for your Azure-SSIS Integration Runtime.",IR_VNetValidation_General_Warning:"Please ensure that your Azure-SSIS Integration Runtime in the selected VNet/subnet can access SSISDB hosted by the selected Azure SQL Database server with VNet service endpoints/firewall rules/private endpoint.",IR_VNetValidation_AllowAllAzureIps:"AllowAllWindowsAzureIps",IR_VNetValidation_SubnetNotDelegate_Batch:"Your selected subnet isn\u2019t delegated to the required service: Microsoft.Batch/batchAccounts. To delegate your subnet, see more info",IR_VNet_InteractiveQuery_Enabled:"Successfully enabled interactive authoring capability for {0} ({1}).",IR_VNet_InteractiveQuery_Disabled:"Successfully disabled interactive authoring capability for {0} ({1}).",IR_VNet_InteractiveQuery_Enable_ConfirmContent:"Are you sure you want to enable interactive authoring for integration runtime:\n{0}?",IR_VNet_InteractiveQuery_Disable_ConfirmContent:"Are you sure you want to disable interactive authoring for integration runtime:\n{0}?",IR_VNet_InteractiveQuery_NotReadyToEnable:"This integration runtime is not ready for enabling interactive authoring.",IR_VNet_InteractiveQuery_EnableFailed:"Failed to enable the interactive authoring capability.",IR_VNet_InteractiveQuery_DisableFailed:"Failed to disable the interactive authoring capability.",IR_VNet_InteractiveQuery_Slider_Tooltip_Status:"Interactive authoring status: {0}",IR_VNet_InteractiveQuery_Time:"This may take up to 2 minutes.",IR_VNet_InteractiveQuery_TestConnectionDisabled:"Currently unavailable. Enable interactive authoring in your integration runtime {0} for this capability.",IR_VNet_InteractiveQuery_TestConnectionDisabledNeedToCreate:"Currently unavailable. To use this capability, create a managed private endpoint.",IR_VNet_InteractiveQuery_TestConnectionDisabledNeedToApprove:"Currently unavailable. To use this capability, make sure the private endpoint is approved",Airflow_NonPreviewLabel:"Airflow",Airflow:"Airflow (Preview)",AirflowFeedback:"Please help us improve by sharing your valuable Managed Airflow Preview feedback by",AirflowFeedbackEmail:"emailing us",AirflowDescription:"Managed Airflow in Azure Data Factory is a managed orchestration service for Apache Airflow that simplifies the creation and management of Airflow environments on which you can operate end-to-end data pipelines at scale.",AirflowIRIsBeingSetUp:"Setting up airflow environment: {0} ({1}). This may take up to 20 minutes",AirflowIRIsBeingUpdated:"Updating airflow environment: {0} ({1}). This may take a minute",AirflowIRNotReady:"Airflow IR not ready yet",AirflowIRSetupSuccessful:"Succesfully set up airflow environment: {0} ({1})",AirflowIRUpdatedSuccessful:"Succesfully updated airflow environment: {0} ({1})",AirflowIRSetupFailed:"Failed to set up the airflow environment: {0} ({1}).",DeleteIR:"Delete IR",AirflowEntityName:"Airflow entity name",AddAirflowEntityText:"New airflow entity",ImportAirflowFiles:"Import airflow files",ImportingAirflowFiles:"Importing airflow files",ImportingAirflowFilesIntoAirflow:"Importing airflow files: {0} ({1}). This may take up to a minute",ImportAirflowFilesSuccessful:"Successfully imported airflow files ",ImportAirflowFilesFailed:"Failed to import airflow files",ImportAirflowFilesFailedIntoAirflow:"Failed to import airflow files: {0} ({1})",ImportAirflowFilesDescription:"Provide the FOLDER path that contains the desired python Airflow files to be imported or FILE path to import a singular Airflow file",ImportAirflowFilesBladeDescription:"You can import Airflow DAGs, hooks, operators, or sensors into your Data Factory through this interface. To do so, you will need to select a folder/file path from a blob storage account that contains your desired DAG python files",AirflowFileTypeLabel:"Airflow file type",SubTypeLabel:"Sub type",ZipFileScope:"Zip file scope",AirflowFileTypeDescription:"Select the type of airflow file you plan to import",SelectAirflowSubType:"Select an aiflow entity sub type",SelectAirflowFileType:"Select Airflow file type",SelectLinkedServiceAndFileOrFolderPath:"Select linked service and file/folder path",SelectLinkedServiceAndFolderPath:"Select linked service and folder path",ZipFilesNotSupported:"Zip file imports from here are not supported at the moment. Instead import directly into IR from 'Manage > Airflow (Preview) > Import Files'",AirflowZipFileAlreadyExistsAndWillBeUpdated:"Note: Re-importing a zip file will update Airflow Entities in the DF that are found in the zip file. This will also update airflow IRs that reference updated Airflow Entities",AirflowEntityAlreadyExistsAndWillBeUpdated:"Note: An Airflow Entity of the same name already exists in the DF. Importing the airflow file will update the Airflow Entity and any airflow IR referencing it",SubTypeDAG:"DAG",SubTypeHook:"Hook",SubTypeSensor:"Sensor",SubTypePlugin:"Plugin",AddOrRemoveAirflowEntities:"Add or Remove Airflow Entities",AddOrRemoveAirflowEntitiesBladeDescription:"Use this interface to add or remove airflow entities from your Airflow IR. Only airflow entities that have been added to this data factory will appear on the 'add' drop down. Likewise, only airflow entities that have been added to the airflow IR will appear on the 'remove' drop down",AddAirflowEntities:"Add airflow entities",AddingAirflowEntities:"Adding airflow entities...",AddAirflowEntitiesSuccessful:"Successfully added airflow entities",AddAirflowEntitiesSuccessfullytoAirflowIR:"Successfully added '{0}' to '{1}' ({2})",RemoveAirflowEntities:"Remove airflow entities",RemovingAirflowEntities:"Removing airflow entities...",RemoveAirflowEntitiesSuccessful:"Successfully removed airflow entities",RemovedAirflowEntitiesSuccessfullyFromAirflowIR:"Successfully removed '{0}' from '{1}' ({2})",ImportFiles:"Import Files",ImportFiletype:"Select import file type",ImportingFiles:"Importing files",ImportingFilesIntoAirflow:"Importing files: {0} ({1}). This may take up to a minute",ImportFilesSuccessful:"Successfully imported files",ImportFilesFailed:"Failed to import files",ImportFilesFailedIntoAirflow:"Failed to import files: {0} ({1})",ImportFilesDescription:"Provide the FOLDER path that contains folders named 'dags' and 'plugins' in order to import those into the Airflow environment",ImportFilesBladeDescription:"You can import DAGs and their dependencies through this interface. You will need to select a directory path from a blob storage account that contains folders named 'dags' and 'plugins' in order to import those into the Airflow environment.",ImportFilesError:"The selected directory path does not contain either a 'dags' or 'plugins' folder",ImportPathDoesNotExistError:"The selected directory path does not exist",ImportRequirement:"Import requirements",ImportRequirementDescription:"If requirements.txt file exists during import, requirements will be added to the airflow environment. Any existing requirements will be overwritten",ImportPathLocationLabel:"Import path location",DAGsImportLocationLabel:"DAG files import location",PluginsRootImportLocationLabel:"Other files root import location",DefaultImportPathLocationDescription:"By default, DAG files will be added to a 'dags' folder and plugin files to a 'plugins' folder. These path locations cannot be changed",CustomFolderPathDescription:"By default, all folders containing custom operators, hooks, sensors, etc, are added under 'plugins/'. You may change this path if you have a custom path as long as it starts with 'plugins/' or 'dags/' and only when there are no Airflow Entities in the IR",CustomFolderPathWrongDir:"Path must start with 'plugins/' or 'dags/'",DeleteDag:"Delete DAG",DeletingDag:"Deleting DAG...",DeleteDagSuccessfulV1:"DAG deleted successfully. Remember to delete DAG from Airflow UI as well to fully remove the DAG",DeleteDagSuccessfulV2:"DAG deleted successfully",DeleteDagFailed:"Failed to delete DAG",DeleteDagBladeDescriptionV1:"Use this interface to delete your DAGs. Deleting DAGs is a two step process for Airflow V1. First enter the DAG file name you would like to delete below (E.g. 'first_dag.py'). Once deleted, delete from the Airflow UI",DeleteDagBladeDescriptionV2:"Use this interface to delete your DAGs. Enter the DAG file name you would like to delete below (E.g. 'first_dag.py')",DagFileName:"DAG file name:",DagFileNameEmpty:"DAG file name cannot be empty",DagFileNameInvalidCharacters:"DAG file name contains invalid characters",DagFileNameMissingExtension:"DAG file name must contain '.py' or '.zip' file extension",PleaseSaveAirflowEntityChanges:"To add/remove airflow entity '{0}' to airflow IRs, please save your entity changes",AirflowIRsLabel:"Airflow IRs",AddAirflowEntityToAirflowIRs:"Add airflow entity to airflow IRs",RemoveAirflowEntityFromAirflowIRs:"Remove airflow entity from airflow IRs",AddAirflowEntityToAirflowIRsBladeDescription:"Use this interface to add an airflow entity to your Airflow IRs. Only airflow IRs that do not contain the airflow entitiy will appear on the drop down",RemoveAirflowEntityFromAirflowIRsBladeDescription:"Use this interface to remove an airflow entity from your Airflow IRs. Only airflow IRs that contain the airflow entitiy will appear on the drop down",AddingAirflowEntityToIRs:"Adding airflow entity to IRs...",RemovingAirflowEntityFromIRs:"Removing airflow entity from IRs...",AirflowEntityValidationLabel:"Airflow entity validation output",AirflowEntityValidationCompleteLabel:"Your airflow entity has been validated.",CreateAirflowSecret:"Create airflow secret",EditAirflowSecret:"Edit airflow secret",AirflowSecretBladeDescription:"Use this interface to create or edit a secret to be installed in your airflow environment",SecretNamespace:"Secret namespace",SecretType:"Secret type",CustomSecretType:"Custom secret type",SecretAnnotations:"Secret annotations",SecretData:"Secret data",SecretStringData:"Secret string data",SecretTypeDescription:"Select type of secret you would like to create",PrivateRegistryAuth:"Private registry auth",BasicAuth:"Basic auth",Generic:"Generic",CustomYaml:"Custom yaml",RegistryServerUrl:"Registry server url",RegistryUsername:"Registry username",RegistryPassword:"Registry password",SecretDataValidation:"Data name and value cannot be empty",SecretsRequireNameValidation:"All secrets require a name",SecretsCannotHaveSameName:"Multiple secrets cannot have the same name",Edit_endpoint:"Edit endpoint",NotAvailable:"Not available",Sort_Type:"Sort table by type",Sort_Name:"Sort table by name",Sort_Related:"Sort table by related",Sort_Annotation:"Sort table by annotation",IR_Sort_SubType:"Sort table by sub-type",IR_Sort_Status:"Sort table by status",IR_Sort_Region:"Sort table by region",IR_Sort_Version:"Sort table by version",IR_Sort_VNetName:"Sort table by VNet name",IR_Sort_CreateTime:"Sort table by created time",IR_Sort_CreatedOn:"Sort table by created on",IR_Sort_Auth:"Sort table by auth type",Sort_Path:"Sort table by path",SortedAsc:"Sorted in ascending order",SortedDesc:"Sorted in descending order",SortedHeaderInformation:"Enter table header, {0}, column {1} of {2}.",LinkedService_Delete_Confirm_Title:"Are you sure you want to delete linked service:\n{0}?",LookupActivityConfigurationFirstRowOnly:"First row only",LookupActivityNotFullySupportedMessage:"Lookup activity currently only supports limited functionality for Azure Cosmos DB (MongoDB API), MongoDB and Rest connector. It will be improved in the future.",LookupActivityNoSqlEncryption:"Lookup activity does not support SQL always encryption under Managed VNET IR.",CopyActivity_ImportSchemas_Label:"Import schemas",CopyActivity_LoadDataset_ErrorMessage:"Failed to load dataset",SelectAResourceLabel:"Select a {0}",Source_Label:"Source",Sources_Label:"Sources",Targets_Label:"Targets",SourceActivity_SourceTab_DatasetMissing_ErrorMessage:"Dataset is required.",SourceActivity_SourceTab_LinkedServiceMissing_ErrorMessage:"Linked service of the dataset is required",SourceActivity_SourceTab_SourceType:"Source type",SourceActivity_SourceTab_InlineSourceType:"Inline dataset type",SourceActivity_SourceTab_UseQueryDescription:"Use the custom SQL query to read data",SourceActivity_SourceTab_StoredProcedureMissingErrorMessage:"Stored procedure is required",SourceActivity_SourceTab_UnloadSetting_Missing_Linkedservice_ErrorMessage:"An s3 linked service is required when using redshift unload setting",SourceActivity_SourceTab_UnloadSetting_Missing_Bucketname_ErrorMessage:"Bucket name is required when using redshift unload setting",SourceActivity_SourceTab_UnloadSetting_No_S3_Linkedservice:"No S3 linked service selected",SourceActivity_SourceTab_UnloadSetting_Server_Name_Empty:"The linked service of source dataset must be an amazon redshift linked service",SourceActivity_SourceTab_UseDistCp_Label:"Use HDFS DistCp",SourceActivity_SourceTab_DistCpResourceManagerEndpoint_Label:"ResourceManager endpoint",SourceActivity_SourceTab_DistCpResourceManagerEndpoint_Description:"The Yarn ResourceManager endpoint",SourceActivity_SourceTab_DistCpTempScriptPath_Label:"Temp script path",SourceActivity_SourceTab_DistCpTempScriptPath_Description:"A folder path used to store temp DistCp command script. The script file is generated by this service and will be removed after Copy job finished.",SourceActivity_SourceTab_DistCpOptions_Label:"DistCp options",SourceActivity_SourceTab_DistCpOptions_Description:"Additional options provided to DistCp command.",UnloadSetting_MainGroupLabel:"Unload settings",DistCpSetting_MainGroupLabel:"DistCp settings",ModelName_PreserveRules:"Preserve",PreserveRules_ACL_Label:"ACL",PreserveRules_Owner_Label:"Owner",PreserveRules_Attributes_Label:"Attributes",GroupLabel:"Group",PreserveRules_ACL_Description:"Copy and preserve POSIX access control rights on files and directories. It will fully copy the existing access control list (ACL) from source to sink.",PreserveRules_Owner_Description:"Copy and preserve Owner on files and directories. Only for super-users in the sink side.",PreserveRules_Group_Description:"Copy and preserve Owner Group. Super-users or owning user access in the target group is needed.",PreserveRules_Attributes_Description:"Copy and preserve contentType, contentLanguage, contentEncoding, contentDisposition, cacheControl and all user specified metadata on files and directories.",Tab_Close_Confirm_Title:"Save changes?",Tab_Close_Confirm_ContentTemplate:"Do you want to save the changes to {0}?",Tab_Close_Confirm_Discard_Title:"Discard changes?",Tab_Close_Confirm_Discard_ContentTemplate:"{0} is not saved. If you continue, your changes will be lost.",DiscardChangesInfo:"Your changes are not saved. If you continue, your changes will be lost.",DiscardChanges_Confirm_Discard_Content:"You have unsaved changes. If you continue, these changes will be lost, Are you sure you want to discard your changes?",Browser_Close_Confirm_Title:"There are unsaved changes.",ConfirmAdfSwitch:"Make sure to sync your Data Factory entities before switching to ADF mode. Are you sure you want to switch to ADF mode?",ConfirmWorkspaceSwitch:"Make sure to sync your Workspace entities before switching to workspace mode. Are you sure you want to switch to workspace mode?",ConfirmGitSwitchTemplate:"There are unpublished changes to one or more items. To save your changes, use the 'Publish all' button.\n\nAre you sure you want to discard your changes?",GitDiscardUnsavedChanges:"You have unsaved changes. If you continue, these changes will be lost. Are you sure you want to discard your changes?",confirmGitLeave:"Make sure to sync your Data Factory entities before leaving. Are you sure you want to leave?",Parallel_Setting_Label:"Degree of copy parallelism",Amazon_Redshift_Unloadsettings_Label:"Unload data through Amazon S3 for better performance",Amazon_Redshift_Unloadsettings_Linkedservice_Label:"S3 linked service",EnablePartitionDiscovery:"Enable partition discovery",ColumnNameLabel:"Column name",ColumnNameExpression:"Column name expression",ColumnOrdinalLabel:"Column ordinal",StartTime:"Start time",StartTimeValidation:"Start time is required when column name is defined.",OutputStartTime:"Output start time",EndTime:"End time",TotalElapsedTime:"Total elapsed time",StartTimeUTC:"Start time (UTC)",EndTimeUTC:"End time (UTC)",IRSelection_IRNameLabel:"Connect via integration runtime",IRSelection_IRDescription:"Select an integration runtime",IRSelection_IREditButtonText:"Edit integration runtime",SSISIRSelection_IRNameLabel:"Azure-SSIS Integration Runtime",SSISIRSelection_IRDescription:"Select an Azure-SSIS Integration Runtime",IRSelection_AirflowEditButtonText:"Edit airflow environment",TriggerRunsLabel:"Trigger runs",Monitoring_SelectView:"Select view...",RequestContent:"Request content",RequestSteps:"Request steps",Monitoring_Navigate_BackToArtifact:"Back to artifact",Monitoring_Navigate_AllArtifacts:"All artifacts",Monitoring_Navigate_AllPipelineRuns:"All pipeline runs",MonitoringHub:"Monitoring hub",Monitoring_Navigate_AllTriggerRuns:"All trigger runs",Monitoring_Navigate_IntegrationRuntime_Details:"Resource monitor (details)",Showing0Items:"Showing 0 items",Showing1Item:"Showing 1 item",Grid_PageReportTemplate:"Showing {0} - {1} of {2} items",Grid_PageReportTemplateWithoutTotal:"Showing {0} - {1} items",Grid_TimerReport:"Last refreshed {0} minutes ago",Grid_TimerReportSingular:"Last refreshed {0} minute ago",Grid_TimerReportMax:"Last refreshed over 15 minutes ago",Monitoring_Dashboard_Pie_Chart:"Show pie chart",Monitoring_Dashboard_Line_Chart:"Show line chart",Monitoring_Dashboard_Line_Chart_Canvas:"Line chart canvas",Monitoring_Dashboard_Pie_Chart_Canvas:"Pie chart canvas",Monitoring_Dashboard_Accessibility_Explainability:"{0} has {1} succeeded runs, {2} In-progress runs, {3} failed runs, and {4} cancelled runs.",Monitoring_ViewAllPipelineRun:"View all pipeline runs",Monitoring_Gantt_View:"Gantt",Monitoring_Chart_View:"Chart",Monitoring_List_View:"List",Monitoring_Load_More_Data:"More data available",Monitoring_Group_Annotation:"Group by annotations",Monitoring_Close_Tooltip:"Close tooltip",Monitoring_LoadingAllPipelineNamesLabel:"Loading all pipeline names",Monitoring_ViewTriggerRuns:"View trigger runs",Monitoring_ViewAllPipelineRunsLabel:"View all rerun history",Monitoring_ViewCurrentPipelineRunsLabel:"Hide all rerun history",Monioring_ViewLatestRunsLabel:"Latest runs",Monioring_ViewAllRunsLabel:"All runs",Monioring_ViewRunsLabel:"Runs",Monitoring_PipelineNameLabel:"Pipeline name",Monitoring_ServiceError:"The error message wasn't provided by the service.",ActivityRunsLabel:"Activity runs",PipelineRunsLabel:"Pipeline runs",LinkConnectionsLabel:"Link connections",LinkConnectionLabel:"Link connection",NewLinkConnection:"New link connection",ADFCDC:"ADF CDC",CDCConnections:"CDC connections",ADFLinkConnections:"ADF link connections",AllLinkConnectionsLabel:"All link connections",EditLinkConnectionLabel:"Edit link connection",EditCDCLabel:"Edit CDC",DeleteCDCLabel:"Delete CDC",LinkConnectionName:"Link connection name",LinkConnectionId:"Link connection ID",LinkConnectionDetailsWarning:"Please click link connection name to view the details.",LinkConnectionDoesnotExist:"{0} does not exist",DeletedLinkConnectionCannotBeEdited:"If it is deleted but not yet published, please discard deletion before editing.",SourceSchemaTableName:"Source schema/table name",TargetSchemaTableName:"Target schema/table name",Monitoring_LastOutputTimeLabel:"Output watermark",Monitoring_LastOutputTimeRangeLabel:"Output watermark range",Monitoring_RunStartLabel:"Run start",Monitoring_RunEndLabel:"Run end",Monitoring_RuntimeLabel:"Runtimes & sessions",Monitoring_PoolsLabel:"Pools & sessions",Monitoring_PipelineRunUpdatePipelineLabel:"Update pipeline",DurationLabel:"Duration",DurationBreakdownLabel:"Duration breakdown",SourceToStagingDurationLabel:"Source to Staging duration",StagingToStagingDurationLabel:"Staging to Staging duration",StagingToDestinationDurationLabel:"Staging to destination duration",DataProcessed:"Data processed",Monitoring_DurationFormat:"HH:MM:SS",Monitoring_ActivityDetails_NoDetails:"There are no activity details available, please check back later",Monitoring_DatabricksActivity_RunPageUrl:"Run page url",Monitoring_FetchNotebookActivitySnapshotFailedTitle:"Fetch notebook snapshot failed",Monitoring_FetchNotebookActivitySnapshotFailed:"The request to fetch notebook snapshot for activity {0} failed with error {1}",Monitoring_TroubleshootActivityFailures:"Troubleshoot activity failures",Monitoring_TroubleshootConnectionFailures:"Troubleshoot connection failures",Monitoring_DatabricksActivity_MonitoriExecutionLabel:"Monitor real-time execution in Azure Databricks",Monitoring_DatabricksActivity_JobNotStarted:"Waiting for Databricks cluster to be ready. This might take several minutes, please check back later. Run page URL will be available after cluster is ready and job is submitted.",Monitoring_GetActivityProgress_Error:"There was an issue loading activity details. Please close this and try again.",Monitoring_TriggeredByLabel:"Triggered by",Monitoring_TriggeredByTypeLabel:"Triggered by type",Monitoring_BlankGridCellLabel:"Blank",Monitoring_MessageLabel:"Message",Component:"Component",Resolution:"Resolution",VertexFailed:"Vertex Failed",SystemError:"System Error",CompilationError:"Compilation Error",SevereWarning:"Severe Warning",ReviewAndContactSupport:"Review and contact support",ReviewAndSave:"Review + save",RunDimensionLabel:"Run Dimension",Monitoring_ParentPipelineNameLabel:"Parent pipeline name",RunIDWithVal:"Run ID: {0}.",Monitoring_RunIDLabel:"Run ID",Monitoring_RunGroupIDLabel:"Run Group ID",Monitoring_RunStatus:"Run status",expandRuns:"Expand runs",LastProcessed:"Last processed",RunIDWithSpace:"Pipeline run ID",ContinousRunID:"Continuous run ID",Monitoring_RunGroupIDWithSpace:"Run group ID",Monitoring_ActivityStatus:"Detail Status",Monitoring_ActivityRunIDLabel:"Activity run ID",Monitoring_PipelineRunActivityIDLabel:"Pipeline run activity ID",Monitoring_PipelineModifiedMessage:"Pipeline was modified after this run. The current pipeline configuration is shown.",Monitoring_PipelineObtainedLocally:"Pipeline details could not be fetched from the server. The local pipeline configuration is shown.",Monitoring_PipelineObtainedLocally_Debug:"This is a recent debug run. The local pipeline configuration is shown.",ActivityTypeLabel:"Activity type",Monitoring_TriggerLabel:"Trigger name",Monitoring_RunTypeLabel:"Run type",Monitoring_TriggerTimeLabel:"Trigger time",Monitoring_TriggerRunIdLabel:"Trigger Run ID",Monitoring_ScheduledTimeLabel:"Scheduled time",Monitoring_TriggerFileNameLabel:"Trigger file name",Monitoring_TriggerFileSizeLabel:"Trigger file size",Monitoring_CustomEventTypeLabel:"Event type",Monitoring_CustomEventTypesLabel:"Event types",Monitoring_CustomEventSubjectLabel:"Subject",Monitoring_Latest:"Latest",Monitoring_RerunLatest:"Rerun (Latest)",Monitoring_Original:"Original",PropertiesLabel:"Properties",Open_Properties:"Open properties",Open_Properties_Banner_text:"Top-level entity properties such as name and description now exist in the properties pane which can be accessed with the properties icon at the top-right.",Monitoring_TriggerTotalRuns:"Total runs",WindowLabel:"Window",Monitoring_ViewTriggerDependencies:"View trigger dependencies",Monitoring_ViewTriggerRunGroup:"View trigger rerun history",Monitoring_ViewTriggerRunDependency:"View trigger run dependency",Monitoring_ViewAllTriggerRunsLabel:"View all trigger rerun history",Monitoring_ViewCurrentTriggerRunsLabel:"Hide all trigger rerun history",Monitoring_ViewRunDimension:"View run dimension",Monitoring_TriggerRun_DependenciesTemplate:"{0} - run dependencies",Monitoring_TriggerRun_RunsTemplate:"{0} - runs",InputLabel:"Input",Inputs:"Inputs",Outputs:"Outputs",Monitoring_Bar_ViewErrorDetails:"View error details",Monitoring_ErrorDetailsTitle:"Error details",Monitoring_LineageDetailsTitle:"Lineage status details",Monitoring_WarningDetailsTitle:"Warning details",Monitoring_DataFlowLabel:"Data flow details",UserPropertiesLabel:"User properties",Monitoring_LastHourLabel:"Last hour",Monitoring_LastFourHourLabel:"Last 4 hours",Monitoring_Last24HourLabel:"Last 24 hours",Monitoring_LastWeekLabel:"Last 7 days",Monitoring_Last30DaysLabel:"Last 30 days",Monitoring_CreatedOn:"Created on",Monitoring_ManualTrigger:"Manual trigger",Monitoring_CopyActivity_DataRead:"Data read: ",Monitoring_CopyActivity_DataRead_Description:"Total amount of data retrieved from source data store, including data copied in this run and data resumed from the last failed run.",Monitoring_CopyActivity_DataResumed:"Data resumed: ",Monitoring_CopyActivity_DataResumed_Description:"Amount of data resumed from the last failed run.",Monitoring_CopyActivity_FilesRead:"Files read: ",Monitoring_CopyActivity_FilesRead_Description:"Total number of files copied from source data store, including files copied in this run and files resumed from the last failed run.",Monitoring_CopyActivity_FilesResumed:"Files resumed: ",Monitoring_CopyActivity_FilesResumed_Description:"Number of files resumed from the last failed run.",Monitoring_CopyActivity_RowsRead:"Rows read: ",Monitoring_CopyActivity_ObjectsRead:"Objects read: ",Monitoring_CopyActivity_RowsCopied:"Rows written: ",Monitoring_CopyActivity_RowsCopied_Description:"Number of rows copied to sink. This metric does not apply when copying files as-is without parsing them, for example, when source and sink datasets are binary format type, or other format type with identical settings.",Monitoring_CopyActivity_RowsSkipped:"Rows skipped: ",Monitoring_CopyActivity_RowsSkipped_Description:"Number of incompatible rows that were skipped. You can enable incompatible rows to be skipped by setting enableSkipIncompatibleRow to true.",Monitoring_CopyActivity_FilesSkipped:"Files skipped: ",Monitoring_CopyActivity_FilesSkipped_Description:'The number of files having not been copied to the destination store. <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2120726">Learn more</a>',Monitoring_CopyActivity_ConsistencyVerification_Description:"Tell the result whether the data has been verified to be consistent between source and destination store after being copied.",Monitoring_CopyActivity_ThrottlingErrors:"Throttling errors: ",Monitoring_CopyActivity_DataWritten:"Data written: ",Monitoring_CopyActivity_DataWritten_Description:"The actual amount of data written/committed to the sink. The size may be different from dataRead size, as it relates to how each data store stores the data.",Monitoring_CopyActivity_InterimDataWritten:"Interim data written: ",Monitoring_CopyActivity_InterimDataWritten_Description:"Size of data written into interim table. This value will be updated gradually.",Monitoring_CopyActivity_InterimRowsWritten:"Interim rows written: ",Monitoring_CopyActivity_InterimRowsWritten_Description:"Num of rows written into interim table. This value will be updated gradually.",Monitoring_CopyActivity_WritingToInterim:"Writing to interim",Monitoring_CopyActivity_WritingToInterim_Description:"Time spent for copying data from source into interim table.",Monitoring_CopyActivity_FilesWritten:"Files written: ",Monitoring_CopyActivity_FilesWritten_Description:"The number of files written/committed to the file-based sink.",Monitoring_CopyActivity_ActivityRunId:"Activity run id: ",Monitoring_CopyActivity_Throughput:"Throughput: ",Monitoring_CopyActivity_Throughput_Description:"Rate of data transfer, calculated by dataRead divided by copyDuration.",Monitoring_CopyActivity_PerfRecommendation:"Performance tuning tips: ",Monitoring_CopyActivity_CopyDuration:"Copy duration",Monitoring_CopyActivity_Queue:"Queue",Monitoring_CopyActivity_QueueDurationDescription:"The elapsed time until the copy activity actually starts on the integration runtime. If you use Self-hosted IR and this value is large, suggest to check the IR capacity and usage, and scale up/out according to your workload.",Monitoring_CopyActivity_QueueDurationDescription_1:"The elapsed time until the copy activity actually starts on the service.",Monitoring_CopyActivity_PreCopyScriptDuration:"Pre copy script",Monitoring_CopyActivity_PreCopyScriptDurationDescription:"The elapsed time between copy activity starting on IR and copy activity finishing executing the pre-copy script in sink data store. Apply when you configure the pre-copy script.",Monitoring_CopyActivity_PreCopyScriptDurationDescription_1:"The elapsed time between copy activity starting on service and copy activity finishing executing the pre-copy script in destination data store. Apply when you configure the pre-copy script.",Monitoring_CopyActivity_UsedParallelCopies:"Used parallel copies",Monitoring_CopyActivity_UsedParallelCopies_Description:"The effective parallelCopies during copy.",Monitoring_CopyActivity_UsedDMUs:"Used DMUs",Monitoring_CopyActivity_UsedDIUs:"Used DIUs",Monitoring_CopyActivity_UsedDIUs_Description:"The effective Data Integration Units during copy.",Monitoring_CopyActivity_SQLDWPolybase:"SQL DW PolyBase",Monitoring_CopyActivity_RedshiftUnlaod:"Redshift unload",Monitoring_CopyActivity_HDFSDistcp:"HDFS Distcp",Monitoring_CopyActivity_TimeToFirstByte:"Time to first byte",Monitoring_CopyActivity_TimeToFirstByteDurationDescription:"The elapsed time between the end of the previous step and the IR receiving the first byte from the source data store. Apply to non-file-based source. If this value is large, suggest to check and optimize the query or server.",Monitoring_CopyActivity_Transfer:"Transfer",Monitoring_CopyActivity_TransferDurationDescription:"The elapsed time between the end of the previous step and the IR transferring all the data from source to sink. The sub-steps under transfer run in parallel, and some operations are not shown now e.g. parsing/generating file format.",Monitoring_CopyActivity_TransferDurationDescription_1:"The elapsed time between the end of the previous step and the service transferring all the data from source to destination. The sub-steps under transfer run in parallel, and some operations are not shown now e.g. parsing/generating file format.",Monitoring_CopyActivity_Listing_Source:"Listing source",Monitoring_CopyActivity_Listing_Source_Description:"The amount of time spent on enumerating source files or data partitions.",Monitoring_CopyActivity_Reading_From_Source:"Reading from source",Monitoring_CopyActivity_Reading_From_Source_Description:"The amount of time spent on retrieving data from source data store.",Monitoring_CopyActivity_Writing_To_Sink:"Writing to sink",Monitoring_CopyActivity_Writing_To_Sink_Description:"The amount of time spent on writing data to sink data store.",Monitoring_CopyActivity_Office365LoadStatus:"Microsoft 365 load status",Monitoring_CopyActivity_HelpLinkMessage:"Learn more on copy performance details from here.",Monitoring_CopyActivity_MonitorLinkMessage:"Learn more on output details",Monitoring_CopyActivity_PeakConnections:"Peak connections: ",Monitoring_CopyActivity_SourcePeakConnections_Description:"Peak number of concurrent connections established to the source data store during the Copy activity run.",Monitoring_CopyActivity_SinkPeakConnections_Description:"Peak number of concurrent connections established to the sink data store during the Copy activity run.",Monitoring_CopyActivity_ReferToThis:"Refer to this ",Monitoring_CopyActivity_Document:"document",Monitoring_CopyActivity_AzureIRRegion:"Azure IR region",Monitoring_CopyActivity_SelfhostedIR:"Self-hosted IR",Monitoring_Filter_Invalid_Json:"Invalid JSON syntax.",Monitoring_Invalid_Time:"Invalid time",Monitoring_Invalid_Date:"Invalid date",Monitoring_Invalid_Time_Prefix:'Invalid time. Use "{0}".',Monitoring_LongPipelineRun:"The pipeline has been running for more than 5 minutes. Please manually refresh the run to get the latest result",Monitoring_LongPipelineRunDesc:"The status cannot be updated because {0} ({1}) has been running for more than 5 minutes. To see the latest status, select Refresh on the Output tab.",Monitoring_NeedMoreTime:"Pipeline run is still in progress. Please click refresh button manually to get the latest result",Monitoring_DateRange:"Current time and date range is {0} from {1} to {2}",Monitoring_SingleRange:"Current {0} is {1}",Monitoring_StartDate:"Start date",Monitoring_EndDate:"End date",Monitoring_Datepicker:"Date picker",Monitoring_NavigateBackTo:"Navigate back to {0}",Monitoring_AlertsAndMertrics:"Alerts & metrics",Monitoring_Alerts:"Alerts",Monitoring_Alert_Rule:"Alert rule",Monitoring_Metrics:"Metrics",Monitoring_NewParameters:"New parameters",Monitoring_CreateRunParameters:"Create run with new parameters",Monitoring_Working_Duration:"Working duration",Monitoring_ConfirmDebugCancel:"Are you sure you want to cancel run of pipeline {0}?",Monitoring_ConfirmTridentCancel:"Are you sure you want to cancel run of pipeline {0} and all children pipelines?",Monitoring_ConfirmTriggeredCancel:"Are you sure you want to cancel run of pipeline {0}? Canceling this pipeline will not cancel all children pipelines. To cancel all children pipelines, please use recursive cancel",Monitoring_ConfirmRecursiveCancel:"Are you sure you want to cancel all children pipelines of pipeline {0}?",Monitoring_ConfirmRerun:"Are you sure you want to rerun pipeline {0}?",Monitoring_ConfirmRerunFromFailed:"Are you sure you want to rerun pipeline {0} from failed activity?",Monitoring_ConfirmRerunFromActivity:"Are you sure you want to rerun pipeline {0} from activity {1}?",Monitoring_ConfirmMultipleReruns:"Are you sure you want to run {0} pipelines?",Monitoring_ConfirmMultipleCancel:"Are you sure you want to cancel {0} pipeline runs?",Monitoring_ConfirmMultipleCancelDebug:"Are you sure you want to cancel {0} debug sessions?",Monitoring_ConfirmTriggerRerun:"Are you sure you want to rerun trigger {0}?",Monitoring_CannotRerunInProgressRun:"Pipeline run is still in progress, please rerun after it's completed",Monitoring_CannotRerunFromActivity:"Cannot rerun from activity {0}",Monitoring_CannotRerunFromActivity_Detail:"Link criteria from activity {0} is not matched. Criteria: {1}, activity run status: {2}",Monitoring_Rerunning:"Rerunning",Monitoring_CreateAlertRule:"New alert rule",Monitoring_TriggerRun:"Trigger run",Monitoring_SelectRunProperty:"Select run property",Monitoring_SelectRunPropertyOption:"Select run property...",Monitoring_SelectRunProperty_SelectKey:"Select a trigger run...",Monitoring_SelectRunProperty_SelectProperty:"Select a property...",Monitoring_EditColumns:"Edit columns",Monitoring_EditColumnsDescription:"Add or remove columns. To change the column order, drag and drop a field.",Monitoring_EditAlertRule:"Edit alert rule",Monitoring_DataFlowModified:"Data flow modified",Monitoring_DataFlowModifiedDetails:"This data flow has been modified since it was run. Monitor will be available if the activity run completes successfully.",Monitoring_PipelineNotFound:"The pipeline was not found. This can happen if the pipeline has been deleted or modified since the last run, or if it has not yet been published.",Monitoring_PipelineNotFoundDesc:"The pipeline {0} ({1}) was not found. This can happen if the pipeline has been deleted or modified since the last run, or if it has not yet been published.",Monitoring_PipelineNotFoundTitle:"Pipeline not found",Monitoring_ResourceNotFound:"The resource was not found. This can happen if the resource has been deleted or modified since the last run, or if it has not yet been published.",Monitoring_ResourceNotFoundTitle:"Resource not found",Monitoring_UsingPE_Desc:"This status only means that a managed private endpoint is created for this data store or service, it does not mean that the managed private endpoint will be used when running the activity. Whether to use private endpoint depends on various factors such as activity configuration, integration runtime reference of source and sink.",MonitoringAlerts_Enabled:"ENABLED",MonitoringAlerts_AlertRuleName:"Alert rule name",MonitoringAlerts_Severity:"Severity",MonitoringAlerts_TargetCriteria:"Target criteria",MonitoringAlerts_AddCriteria:"Add criteria",MonitoringAlerts_UpdateCriteria:"Update criteria",MonitoringAlerts_EditCriteria:"Edit criteria",MonitoringAlerts_AddActionGoup:"Add action group",MonitoringAlerts_EnableRule:"Enable rule upon creation",MonitoringAlerts_CreateAlertRule:"Create alert rule",MonitoringAlerts_UpdateAlertRule:"Update alert rule",MonitoringAlerts_CriteriaText:"Select one metric to set up the alert condition.",MonitoringAlerts_ConfigureAlert:"Configure alert logic",MonitoringAlerts_ShowHistory:"Show history",MonitoringAlerts_SelectDimensions:"Selecting the dimension values will help you filter to the right time series.",MonitoringAlerts_DimensionName:"Dimension",MonitoringAlerts_DimensionValue:"Values",MonitoringAlerts_FailureType:"Failure type",MonitoringAlerts_SelectValue:"Select a value",MonitoringAlerts_CustomDimension:"Add custom {0} value",MonitoringAlerts_CustomDetailsValue:"Specify custom values which are not yet emmited",MonitoringAlerts_AlertLogic:"Alert logic",MonitoringAlerts_EditErrorMessage:"Could not fetch metric details for selected target criteria",ConditionLabel:"Condition",RulesLabel:"Rules",RuleLabel:"Rule",NewMappingCondition:"New mapping rule",MatchingCondition:"Matching condition",MonitoringAlerts_TimeAggregation:"Time aggregation",MonitoringAlerts_ThresholdCount:"Threshold count",MonitoringAlerts_Evaluate:"Evaluate based on",MonitoringAlerts_Period:"Period",MonitoringAlerts_ActionGroupText:"Notify your team via email and text messages or automate actions using webhooks, runbooks, functions logic apps or integrating with external ITSM solutions.",MonitoringAlerts_Privacy:" Privacy statement.",MonitoringAlerts_ActionGroupName:"Action group name",MonitoringAlerts_EmailGroupName:"EmailGroupAlpha",MonitoringAlerts_ActionGroupShortName:"Short name",MonitoringAlerts_AddAction:"Add Action",MonitoringAlerts_ActionText:"Learn more about ",MonitoringAlerts_ActionText1:"Learn more about our",MonitoringAlerts_SendEmail:"SendEmailAlert",MonitoringAlerts_ActionName:"Action name",MonitoringAlerts_ActionType:"Action type",MonitoringAlerts_ActionKind:"Email/SMS/Push/Voice",MonitoringAlerts_SelectNotification:"Select which notifications you'd like to receive",MonitoringAlerts_EmailText:"Enter email address",MonitoringAlerts_Sms:"SMS",MonitoringAlerts_SmsCountry:"Country code",MonitoringAlerts_SmsPhone:"Phone number",MonitoringAlerts_Carrier:"Carrier charges may apply.",MonitoringAlerts_AzurePush:"Azure app push notifications",MonitoringAlerts_Push:"Push",MonitoringAlerts_AzureText1:"Enter your email used to log into your Azure account.",MonitoringAlerts_AzureText2:"Learn about connecting to your Azure resources using the Azure app.",MonitoringAlerts_Voice:"Voice",MonitoringAlerts_ActionGroupType:"Action group type",MonitoringAlerts_AzureGroupsLink:"https://docs.microsoft.com/azure/monitoring-and-diagnostics/monitoring-action-groups",MonitoringAlerts_TargetCriteriaLink:"https://docs.microsoft.com/azure/monitoring-and-diagnostics/monitor-alerts-unified-usage?toc=/azure/azure-monitor/toc.json",MonitoringAlerts_Sev0:"Sev0",MonitoringAlerts_Sev1:"Sev1",MonitoringAlerts_Sev2:"Sev2",MonitoringAlerts_Sev3:"Sev3",MonitoringAlerts_Sev4:"Sev4",MonitoringAlerts_OdataType:"Microsoft.Azure.Monitor.SingleResourceMultipleMetricCriteria",MonitoringAlerts_DefaultName:"NewAlert",MonitoringWindowStartTime:"Window start time",MonitoringWindowEndTime:"Window end time",OverTheLastNHoursLabel:"Over the last {0} hours",GreaterThanLabel:"Greater than",MonitoringAlerts_GreaterThanOrEqual:"Greater than or equal to",LessThanLabel:"Less than",MonitoringAlerts_LessThanOrEqual:"Less than or equal to",MaximumLabel:"Maximum",MinimumLabel:"Minimum",MonitoringAlerts_PeriodOptionMinute:"Over the last {0} minutes",MonitoringAlerts_FrequencyMinute:"Every {0} minute",MonitoringAlerts_FrequencyHour:"Every hour",MonitoringAlerts_ConfigureAction:"Configure Email/SMS/Push/Voice notification",MonitoringAlerts_ConfigureNotification:"Configure notification",MonitoringAlerts_AddNotification:"Add notification",MonitoringAlerts_EditNotification:"Edit notification",MonitoringAlerts_ResourceType:"RESOURCE TYPE",Monitoring_Alert:"ALERT",MonitoringAlert_Actions:"ACTIONS",MonitoringAlert_Resources:"RESOURCES",MonitoringAlert_InvalidEmail:"Invalid email address",MonitoringAlert_InvalidPhone:"Invalid phone number",MonitoringAlert_CreateAndManageAction:"Create and manage action",MonitoringAlert_CreateAndManageActionGroups:"Create and manage action groups",MonitoringAlert_CreateAndManageMetricAlerts:"Create and manage metric alerts",MonitoringAlert_Threshold:"Whenever {0} metric is {1} {2}",MonitoringAlerts_EmailPlaceholder:"email@example.com",MonitoringAlerts_PhonePlaceholder:"1234567890",MonitoringAlert_UserError:"UserError",MonitoringAlert_SystemError:"SystemError",MonitoringAlert_BadGateway:"BadGateway",MonitoringAlerts_CreateAlertFailed:"Creation of alert failed",MonitoringAlerts_DeleteAlertFailed:"Deletion of alert failed",MonitoringAlerts_BillingInfo:"There will be a monthly rate for the configured criteria. Learn more about ",Monitoring_NotebookInputParameters:"Input parameters",Monitoring_DataFlowStreamInformation:"Stream information",Monitoring_DataFlowRowsCalculated:"Rows calculated",Monitoring_DataFlowSuccessRows:"Success rows",Monitoring_DataFlowSuccessRowsInfo:"Success row count",Monitoring_DataFlowErrorRows:"Error rows",Monitoring_DataFlowErrorRowsInfo:"Error row count",Monitoring_DataFlowTotalPartition:"Total partition",Monitoring_DataFlowSkewness:"Skewness",Monitoring_DataFlowSkewnessInfo:"Measures the asymmetry in data distribution",Monitoring_DataFlowKurtosis:"Kurtosis",Monitoring_DataFlowKurtosisInfo:"Measures the 'tailedness' of the data distribution",Monitoring_DataFlowSinkProcessingTime:"Sink processing time",Monitoring_DataFlowSinkPostProcessingTime:"Sink post processing time",Monitoring_DataFlowProcessingTime:"Processing time",Monitoring_DataFlowHighestProcessingTime:"Highest processing time",Monitoring_DataFlowRowsWritten:"Rows written",Monitoring_DataFlowTableOperationSQLDuration:"Table operations SQL duration",Monitoring_DataFlowTableOperationSQLDurationInfo:"The time spent moving data from temp tables to target table",Monitoring_DataFlowTempTable:"Temporary staging table",Monitoring_DataFlowTempTableInfo:"Name of the temporary table used by data flows to stage data in the database",Monitoring_DataFlowPostSQLDuration:"Post SQL duration",Monitoring_DataFlowPostSQLDurationInfo:"The time spent running post SQL commands",Monitoring_DataFlowPreSQLDuration:"Pre SQL duration",Monitoring_DataFlowPreSQLDurationInfo:"The time spent running pre SQL commands",Monitoring_DataFlowMergeDuration:"Merge files duration",Monitoring_DataFlowMergeDurationInfo:'The time spent merging the file, merge files are used for file based sinks when writing to single file or when "File name as column data" is used. If significant time is spent in this metric, you should avoid using these options.',Monitoring_DataFlowPreCommandsDuration:"Pre commands duration",Monitoring_DataFlowPreCommandsDurationInfo:"The time spent running any pre operations for file",Monitoring_DataFlowPostCommandsDuration:"Post commands duration",Monitoring_DataFlowPostCommandsDurationInfo:"The time spent running any post operations for file",Monitoring_DataFlowReadStageDurationParquet:"Read stage duration Parquet",Monitoring_DataFlowReadStageDurationParquetInfo:"The time spent reading data from Hive to parquet",Monitoring_DataFlowWriteStageDurationParquet:"Write stage duration Parquet",Monitoring_DataFlowWriteStageDurationParquetInfo:"The time to write the data to a staging location for parquet",Monitoring_DataFlowWriteStageDurationHive:"Write stage duration Hive",Monitoring_DataFlowWriteStageDurationHiveInfo:"The time spent loading data from parquet to Hive",Monitoring_DataFlowStagingDuration:"Staging duration",Monitoring_DataFlowStagingDurationInfo:"Total amount of time spent inside of Spark to complete the operation as a stage",Monitoring_DataFlowFileSystemInitDuration:"File system init duration",Monitoring_DataFlowFileSystemInitDurationInfo:"The time spent initializing file system",Monitoring_DataFlowReadStageDurationn:"Read stage duration",Monitoring_DataFlowReadStageDurationInfo:"The time spent writing to external table before read",Monitoring_DataFlowWriteStageDuration:"Write stage duration",Monitoring_DataFlowWriteStageDurationInfo:"The time spent staging to external table before write",Monitoring_DataFlowErrorProcExecutionDuration:"Error proc execution duration",Monitoring_DataFlowErrorProcExecutionDurationInfo:"The time spent running the stored procedure that captures error rows",Monitoring_DataFlowReadHiveStageSQLs:"Read Hive Stage SQLs",Monitoring_DataFlowReadHiveStageSQLsInfo:"Hive statements to stage data to parquet",Monitoring_DataFlowReadHivePreSQLs:"Read Hive Pre-SQLs",Monitoring_DataFlowReadHivePreSQLsInfo:"Pre-SQL Hive statements for read",Monitoring_DataFlowReadHivePostSQLs:"Read Hive Post-SQLs",Monitoring_DataFlowReadHivePostSQLsInfo:"Post-SQL Hive statements for read",Monitoring_DataFlowWriteHiveSql:"Write Hive SQL",Monitoring_DataFlowWriteHiveSqlInfo:"Hive statements to write data from external parquet table to sink",Monitoring_DataFlowWriteHivePreSqls:"Write Hive Pre-SQLs",Monitoring_DataFlowWriteHivePreSqlsInfo:"Pre-SQL hive statements for write",Monitoring_DataFlowWriteHivePostSqls:"Write Hive Post-SQLs",Monitoring_DataFlowWriteHivePostSqlsInfo:"Post-SQL hive statements for write",Monitoring_DataFlowWriteHiveTableOperationSqls:"Write Hive table operations SQL",Monitoring_DataFlowWriteHiveTableOperationSqlsInfo:"Table operation SQL hive statements for write",Monitoring_DataFlowWriteHivePostDdls:"Write Hive Post DDLs",Monitoring_DataFlowWriteHivePostDdlsInfo:"Post DDL hive statements for write",Monitoring_DataFlowDeleteOperationDuration:"Delete operation duration",Monitoring_DataFlowDeleteOperationDurationInfo:"Duration to delete records",Monitoring_DataFlowInsertOperationDuration:"Insert operation duration",Monitoring_DataFlowInsertOperationDurationInfo:"Duration to insert records",Monitoring_DataFlowUpdateOperationDuration:"Update operation duration",Monitoring_DataFlowUpdateOperationDurationInfo:"Duration to update records",Monitoring_DataFlowUpsertOperationDuration:"Upsert operation duration",Monitoring_DataFlowUpsertOperationDurationInfo:"Duration to upsert records",Monitoring_DataFlowPrecheckDuration:"Precheck duration",Monitoring_DataFlowPrecheckDurationInfo:"The time spent running precheck to validate data",Monitoring_DataFlowErrorRowOutputDuration:"Error row output duration",Monitoring_DataFlowErrorRowOutputDurationInfo:"The time spent writing to write error data to storage",Monitoring_DataFlowSapToStageDuration:"SAP to stage duration",Monitoring_DataFlowSapToStageDurationInfo:"Duration of reading SAP full/delta data into stage",Monitoring_DataFlowSapToStageSubscriberProcess:"SAP to stage subscriber process",Monitoring_DataFlowSapToStageSubscriberProcessInfo:"SubscriberProcess used to read SAP delta data",Monitoring_DataFlowSapToStageDetails:"SAP to stage details",Monitoring_DataFlowSapToStageDetailsInfo:"Other information about reading SAP data into stage",Monitoring_DataFlowSapToStageMetrics:"SAP to stage",Monitoring_DataFlowSapToStageMetricsExecuteDetailsTotalRowsCopied:"Rows copied",Telemetry:"Telemetry",Lineage:"Lineage",Monitoring_DataFlowDiagnosticsHeader:"{0} diagnostics",Monitoring_DataFlowDiagnosticsTransformations:"{0} ({1} transformations)",Monitoring_DataFlowStageTime:"Stage time",Monitoring_DataFlowPartitionChart:"Partition chart",Monitoring_DataFlowPartition:"Partition",RowCountLabel:"Row count",Monitoring_DataFlowNumberOfTransforms:"Number of transformations:",Monitoring_DataFlowStatus:"Data flow status:",Monitoring_ComputeAcquisitionDuration:"Cluster startup time:",Monitoring_DataFlowCached:"Cached data",Monitoring_ProgressDataNotAvailable:"No progress data available yet",Monitoring_DataFlowErrorLoadingTitle:"Error loading data flow",Monitoring_DataFlowSanboxErrorWaitForProgress:"Please wait for the data flow execution. There is no progress to report yet",Monitoring_DataFlowErrorLoadingSubtitle:"There was an error loading the selected data flow - this may occur if the name of the data flow has changed or if there is an issue with the server",Monitoring_DataFlowErrorLoadingSecureInputOutputSubtitle:"The selected data flow has secure input and/or secure output specified, which hides monitoring data - to see monitoring data, please disable the options and rerun the data flow",Monitoring_DataFlowMapped:"Mapped",Monitoring_DataFlowCalculated:"Calculated",Monitoring_DataFlowUsed:"Used",MethodLabel:"Method",Monitoring_DataFlowOriginalSource:"Original source",Monitoring_DataFlowMean:"Mean",Monitoring_DataFlowEditDataflow:"Edit dataflow",Monitoring_DataFlowEditTransformation:"Edit transformation",Monitoring_AverageRunTime:"Average run time",Monitoring_Deviation:"Duration deviation",Monitoring_MaxDuration:"Longest duration",Monitoring_MinDuration:"Shortest duration",Monitoring_SSIS_CRUD_More_Detail:"For more details, please refer to https://go.microsoft.com/fwlink/?linkid=2099434.\n",Monitoring_SSIS_CRUD_More_Detail_Ref:'For more details, please refer to <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2099434">https://go.microsoft.com/fwlink/?linkid=2099434</a>',Monitoring_SSIS_Delete_Lock_Message:"We won\u2019t be able to clean up the network resources created for your Azure-SSIS Integration Runtime, such as NSG and load balancer, due to a delete lock in the resource group containing your VNet/static public IP addresses. Please remove the delete lock for us to continue managing and upgrading your IR.",RenameLockWarningMessage:"Can not rename due to a delete lock on the resource group of this Data Factory",Monitoring_ViewRequests:"View requests",Monitoring_ViewApplications:"View applications",Monitoring_ViewJobs:"View jobs",ManagePools:"Manage pools",ManageSparkConfigurations:"Manage spark configurations",Monitoring_MonitorSqlPoolInAzurePortal:"Go to the Azure portal to monitor your dedicated SQL Pool (formerly SQL DW).",InvalidNewEntitySchemaName:"Invalid schema name",NewEntityTableNameExists:'Table name "{0}" already exists',Monitoring_ErrorDialog_TroubleshootingGuide:"Troubleshooting guide",TSG_Tooltip:"Open troubleshooting guide in a new tab",Advisor:"Advisor (Preview)",RuleName:"Rule name",PipelinesImpacted:"Pipelines impacted",DataflowsImpacted:"Data flows impacted",GettingMoreInformation:"Getting information for {0} resources...",ScanningResources:"Scanning resources...",Monitoring_PerformanceTuning:"Performance Tuning",PerformanceTuning_NoResultsDesc:"You are following all of our recommendations",Monitoring_PerformanceTuningAction:"Performance tuning action",Intelligent_PerformanceTuning:"Intelligent Performance Tuning Advisor",Monitoring_PerformanceTuningDescription:"Use the performance tuning advisor to learn how you can get started with tuning your data pipelines.",PerformanceTuningDataFlowDescription:"Performance Tuning Advisor for Data Flows provides intelligent and customized recommendations that enable you to maximize performance for your data pipelines and data flows.",Rule_SequentialDataflowDescription:"Pipeline contains multiple sequential data flows",Rule_SequentialDataflowTuningOpportunity:"Increase TTL for Azure IR",Rule_SequentialDataflowOverview:"By default, every data flow activity spins up a new Spark cluster based upon the Azure IR configuration. Cold cluster start-up time takes a few minutes and data processing can't start until it is complete. If your pipelines contain multiple\u202fsequential\u202fdata flows, you can enable a time to live (TTL) value.",Rule_SequentialDataflowAction:"Increasing a time to live value keeps a cluster alive for a certain period after its execution completes. If a new job starts using the IR during the TTL time, it will reuse the existing cluster and start up time will greatly reduce. After the second job completes, the cluster will again stay alive for the TTL time.",Rule_AzureSqlDatabaseSourceDescription:"Pipeline has one or more Azure SQL Database data sources",Rule_AzureSqlDatabaseSourceTuningOpportunity:"Change partition type to 'Source Partitioning'",Rule_AzureSqlDatabaseSourceOverView:"Azure SQL Database has a unique partitioning option called 'Source' partitioning. Enabling source partitioning can improve your read times from Azure SQL DB by enabling parallel connections on the source system.",Rule_AzureSqlDatabaseAction:"Enabling source partitioning can improve your read times from Azure SQL DB by enabling parallel connections on the source system.",Rule_AzureSqlDatabaseSinkOverview:"With Azure SQL Database, the default partitioning should work in most cases. There is a chance that your sink may have too many partitions for your SQL database to handle. If you are running into this, reduce the number of partitions outputted by your SQL Database sink.",Rule_AzureSqlDatabaseSinkDescription:"Pipeline has one or more Azure SQL Database data sinks",Rule_AzureSqlDatabaseSinkTuningOpportunity:"Change partition option to 'Use current partitioning'",AzureSqlDatabaseSourceTitle:"Data Flow Azure SQL Database Source Optimization",AzureSqlDatabaseSinkTitle:"Data Flow Azure SQL Database Sink Optimization",Rec_DatabaseSourceStepOne:"Select Optimize tab for Data flow source",Rec_DatabaseSourceStepTwo:"Set Partition option to 'Set partitioning'",Rec_DatabaseSourceStepThree:"Set Partition type to 'Source'",Rec_DatabaseSourceStepFour:"Choose partition column with high cardinality",SelectRecommendedTTL:"Select Azure IR with the recommended TTL value",VerifyIRNotRunningDataflows:"I have verified there are no dataflows using this IR that are currently running",UnexpectedBehaviorIR:"There might be unexpected behavior if there are currently running dataflows using this IR",Insights:"Insights",Monitoring_TuningOpportunities:"Tuning opportunities",GoToPublishResource:"Go to publish resource",AlreadyPublished:"Changes have been published",PerformanceTuning_PublishWarning:"Modifying the current IR will publish the changes upon selecting 'OK'",CurrentValue:"Current value",RecommendedValue:"Recommended value",ImpactedPipeline:"Impacted pipeline",ImpactedDataflow:"Impacted data flow",ImpactedResources:"Impacted resources",ImpactedActivities:"Impacted activities",PerformanceTuning_ModifyCurrent:"Modify current IR",PerformanceTuning_ChooseAnother:"Choose another IR",Activity_Category_DataLakeAnalytics:"Data Lake Analytics",Activity_Category_Batch:"Batch Service",Activity_Category_DataFlow:"Move & transform",Activity_Category_MachineLearning:"Machine Learning",Activity_Category_HDInsight:"HDInsight",Activity_Category_Iteration:"Iteration & conditionals",Activity_Category_Databricks:"Databricks",Activity_Category_DataExplorer:"Azure Data Explorer",ActivityDetail_LogLocation:"Log location",ActivityDetail_ComputeInformation:"Compute information",ActivityDetail_ExecutionProgress:"Execution progress",ActivityDetail_StderrSasUri:"SAS URI of stderr File",ActivityDetail_DurationInQueue:"Duration in queue",ActivityDetail_MLPipelineRunId:"Machine Learning pipeline run ID",ActivityDetail_LinkToMLPipelineRunInPortal:"Link to Machine Learning pipeline run in Azure portal",ActivityDetail_MLPipelineRunStatus:"Machine Learning pipeline run status",AzureLabel:"Azure",TridentLabel:"Teal",TridentDarkLabel:"Teal Dark",Dataset_Category_NoSQL:"NoSQL",Dataset_Category_GenericProtocol:"Generic protocol",Dataset_Category_ServiceAndApps:"Services and apps",ThirdParty:"Third party",Transformation_Category_MultipleIO:"Multiple inputs/outputs",Transformation_Category_SchemaModifier:"Schema modifier",Transformation_Category_RowModifier:"Row modifier",Transformation_Category_Formatters:"Formatters",DestinationLabel:"Destination",SelectEntities:"Select entities",SelectItem:"Select item...",SelectItemType:"Select item type...",DataFlowChangeFeed:"Change feed",DataFlowStartFrom:"Start from",DataFlowChangeFeedDescription:'If checked, you will get data from <a target="_blank" href="https://docs.microsoft.com/azure/cosmos-db/change-feed">Azure Cosmos DB change feed</a> which is a persistent record of changes to a container in the order they occur.',DataFlowChangeFeedStartFromBeginning:"Start from beginning",DataFlowChangeFeedStartFromBeginningDescription:"If checked, you will get initial load of full snapshot data in the first run, followed by capturing changed data in next runs.",AddKeyColumn:"Add key column",RemoveKeyColumn:"Remove key column",DataFlowEnableCdcLabel:"Enable change data capture",DataFlowReadAllDataInInitialLoad:"Read all data in initial load",DataFlowEnableCdcDescription:"Once enabled, this source will read the new data only since previous run or read all data for first run.",DataFlowEnableIncrementalExtractLabel:"Incremental column",DataFlowIncrementalColumnDescription:"This source will read new data based on this column value, since the previous run",DataFlowRunMode:"Run mode",DataFlowFullOnEveryRun:"Full on every run",DataFlowFullOnFirstRunThenIncremental:"Full on the first run, then incremental",DataFlowIncrementalChangesOnly:"Incremental changes only",DataFlowIncrementalChangesOnlyDescription:"Get incremental changes only, and the first run will mark the checkpoint only without loading any data",DataFlowIncrementalChangesOnlyDisabledTooltip:"This mode is currently not supported",DataFlowSapOdpChangeRunModeWarning:"You are changing the run mode from '{0}' to '{1}' while change data capture (CDC) in SAP is still enabled, please get the SAP subscriber process name from activity run output and stop it in your SAP system as well.",DataFlowEnableSQLNativeCDCLabel:"SQL Server CDC",DataFlowEnableSQLNativeCDCDescription:'Extract Change Data via <a target="_blank" href="https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-ver16">SQL Server Change Data Capture functions</a>',DataFlowEnableSQLNativeCDCNetChangesLabel:"Net changes",DataFlowEnableSQLNativeCDCNetChangesDescription:'Extract Net CDC changes via <a target="_blank" href="https://docs.microsoft.com/en-us/sql/relational-databases/system-functions/cdc-fn-cdc-get-net-changes-capture-instance-transact-sql?view=sql-server-ver16">SQL Server CDC Net Change functions</a>',DataFlowEnableSkipInitialLoadLabel:"Read all files in initial load",DataFlowEnableSkipInitialDescription:"Once enabled, this source will read all the new data read in the initial run.",DataFlowStartFromBeginningLabel:"Start reading from beginning",DataFlowStartFromBeginningDescription:"Get initial load of full data in the first run, followed by reading data incrementally.",DataFlowBatchSizeRowRelativeUrlError:"Row relative url is not supported when batch size > 1",DataFlowSkipRowRelativeUrlError:"Row relative url must be specified when skip row relative url is enabled",DataFlowRowRelativeUrl:"Row relative url",DataFlowSkipRowRelativeUrl:"Skip writing row relative url",DataFlowSkipRowRelativeUrlDescription:"Specifies whether to write the row relative url in the request body.",BatchActivityLabel:"Machine Learning Batch Execution",MLUpdateActivityLabel:"Machine Learning Update Resource",MapReduceActivityLabel:"MapReduce",PigActivityLabel:"Pig",WebActivityLabel:"Web",WebHookActivityLabel:"WebHook",StreamingActivityLabel:"Streaming",USqlActivityLabel:"U-SQL",CustomLabel:"Custom",GetMetadataActivityLabel:"Get Metadata",LookupLabel:"Lookup",ForEachActivityLabel:"ForEach",ExecutePipelineActivityLabel:"Execute Pipeline",InvokePipelineActivityLabel:"Invoke pipeline",RunPipelineActivityLabel:"Run pipeline",IfConditionActivityLabel:"If Condition",SwitchActivityLabel:"Switch",UntilActivityLabel:"Until",WaitActivityLabel:"Wait",FailActivityLabel:"Fail",CopyDataLabel:"Copy data",CopyDataInto:"Copy data into {0}",CopyWizardLabel:"Copy Data tool",ODIWizardLabel:"ODI Copy Wizard",NotebookActivityLabel:"Notebook",JarActivityLabel:"Jar",PythonActivityLabel:"Python",SsisActivityLabel:"Execute SSIS package",AzureFunctionActivityLabel:"Azure Function",ValidationActivityLabel:"Validation",ExtensibleActivityLabel:"Extensible",EmailActivityLabel:"Office365 Outlook",TeamsActivityLabel:"Teams",InvalidNamePattern:"{0} name is invalid, name can be up to {1} characters and only letters, numbers, '-' and '_' are allowed. Spaces are also only permitted in the middle.",InvalidNamePatternWithoutDash:"{0} name is invalid, only letters, numbers and '_' are allowed.",InvalidTimeoutPattern:"{0} value is invalid, Sample: 10.23:55:55.",Activity_ParametersHelpText1:"Add dynamic content above using any combination of ",AddDynamicContentBelowText:"Add dynamic content below using any combination of ",Activity_ParametersHelpText2:" Click any of the available System variables or Functions below to add them directly:",Activity_ParametersHelpTextExpressions:"expressions, functions",Activity_ParametersHelpTextSystem:"system variables",advancedSectionDescription:"See all the remaining properties that the service exposes in JSON format. The visual experience is working to add first class support for these properties. You can add/edit/delete these properties in JSON.",DatasetParameterizableProperties:"Dataset properties",LinkedServiceParameterizableProperties:"Linked service properties",LinkedService_ParametersHelpText:"Learn about linked service parameterization",datasetInfoMessage:"These parameterizable properties are defined on the {0} dataset. To create new {0} parameters edit the dataset in the {1}. Use expressions, functions, or refer to system variables in the 'value' column",DataFlowInfoMessage:"These parameterizable properties are defined on the data flow. To create new parameters edit the data flow '{0}'. Use data flow columns, parameters and expressions as value for this parameters",FlowletInfoMessage:"These parameterizable properties are defined on the flowlet. To create new parameters edit the flowlet '{0}'. Use dataflow columns, parameters and expressions as value for this parameters",DataFlowParameterExpressionDescription:"When false, the string parameter values will be wrapped into single quotes using the interpolation syntax",LinkedServiceParametersInfoMessage:"These parameterizable properties are defined on the {0} linked service. To create new {0} parameters edit the linked service in the {1}. Use expressions, functions, or refer to system variables in the 'value' column",HDIClusterLabel:"HDI Cluster",SparkActivityScriptTabLabel:"Script / Jar",ExecuteDataFlowActivityParametersLabel:"parameters",ExecuteDataFlowActivityLinkedServiceParametersLabel:"linked service parameters",ExecuteDataFlowActivityDataFlowParameters:"Data flow parameters",ExecuteFlowletTransformationParameters:"Flowlet parameters",ExecuteDataFlowActivityPipelineExpression:"Pipeline expression",ExecuteDataFlowActivityDataFlowExpression:"Data flow expression",ExecuteDataFlowActivityDataFlowParametersEmpty:"No data flow parameters",ExecuteDataFlowActivityDataFlowParametersNoDataFlow:"Please select a data flow",ExecuteDataFlowActivityParameterExpressionType:"Expression type",FirstRowOnlyDescription:"Select this property to only write the first row of cache sink to activity output",DatasetParameters:"Dataset parameters",S3TempCredential_ExpirationMessage:"AWS temporary credential expires between 15 minutes to 36 hours based on settings. Make sure your credential is valid when activity executes.",UseSparkGateway:"Use spark gateway",DatabricksClusterAdditionalSettings:"Additional cluster settings",TridentDataflowNotificationOptions:"Notification Option",MailOnCompletion:"Mail on completion",MailOnFailure:"Mail on failure",NoNotification:"No notification",DataFlowComputeSizeLabel:"Compute size",DataFlowComputeTypeLabel:"Compute type",DataFlowCoreCountLabel:"Core count",DataFlowCleanupLabel:"Quick re-use",DataFlowRunConcurrenlty:"Run in parallel",DataFlowContinueOnError:"Continue after sink error",DataFlowRunAllowCaching:"Allow caching.",DataFlowRunConcurrenltyDescription:"Allows sinks inside a sink group in the same stream to run concurrently.",DataFlowSourceStagingConcurrencyDescription:"Specify number of parallel staging for sources applicable to the sink.",DataFlowContinueOnErrorDescription:"Even if an error occurs in a sink, continue processing the other sinks.",DataFlowRunAllowCachingDescription:"Data flow job broadcast joins and rank caches are first built before the other segments of the data flow are run.",DataFlowCleanupDescription:"If enabled, cluster will not be recycled and it will be used in next data flow activity run until TTL (time to live) is reached.",DataFlowRunTimeLabel:"Data flow runtime",DataBricksClusterSelection:"Select cluster",DataBricksClusterSelection_NewCluster:"New job cluster",DataBricksClusterSelection_ExistingCluster:"Existing interactive cluster",DatabricksClusterSelection_ExistingPool:"Existing instance pool",DataBricks_SparkConfDescription:"An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively",DataBricks_SparkEnvVarsDescription:"Environment variables for spark",ClickToViewMoreInformation:"Click to view more information",DataBricks_ClusterLogDbfsPlaceholder:"e.g. dbfs:/logs",DataBricks_InitScript_Placeholder:"e.g. dbfs:/scripts/init.bash",DataBricks_DriverNodeType:"Driver node type",DataFlowDebugIRWarning:"Picking a custom IR will result in a cold cluster start. Allow a few minutes for the cluster to start. For the quickest debug session start-up experience, select the default auto-resolve IR.",DataFlowDebugTimeToLiveLabel:"Debug time to live",DataFlowTimeToLiveDescription:"The allowed idle time for the data flow compute. Specifies how long it stays alive after completion of a data flow run if there are no other active jobs.",GeneralPurpose:"General purpose",DataBricksClusterCategory_GeneralPurposeHDD:"General purpose (HDD)",DataBricksClusterCategory_MemoryOptimizedRemoteHDD:"Memory Optimized (Remote HDD)",DataBricksClusterCategory_MemoryOptimized:"Memory optimized",DataBricksClusterCategory_ComputedOptimized:"Compute optimized",DataBricksClusterCategory_StorageOptimized:"Storage optimized",DataBricks_Subscription_Description:"You can select an azure subscription to filter workspaces.",DataBricks_Workspace_Selection:"Databricks workspace",DataBricks_Workspace_Description:"You can select a workspace to auto-populate domain.",DataBricks_Invalid_Workspace_Or_Token_For_Query:"Invalid workspace or token",DataBricks_Empty_Workspace_Or_Token_For_Query:"Add workspace and token to list options",DataBricks_Empty_Workspace_For_Query:"Add workspace and {0} to list options",DataBricks_Update_Token_For_Query:"Update token to list options",DataBricks_Select_Cluster_Version_To_List_Node_Types:"Select cluster version to list options",DataBricks_Existing_Clusters_Description:"You can fill domain and token to auto-populate existing interactive clusters or fill existing interactive cluster ID directly",DataBricks_ChooseFromExistingPools:"Choose from existing pools",DataBricks_Existing_Pools_Description:"You can fill domain and token to auto-populate existing pools or fill existing interactive pool ID directly",DataBricks_Select_Cluster_Policy:"Select cluster policy",DataBricks_Select_Cluster_Policy_None:"No cluster policies available",DataBricks_Cluster_Policy_Description:'Limit cluster creation based on a set of rules. <a target="_blank" href="https://docs.microsoft.com/azure/databricks/administration-guide/clusters/policies">Learn more</a>',DataBricks_Cluster_Policy:"Cluster policy",DataBricks_ExternalComputeInformation:"External compute information",DataBricks_Endpoint:"Databricks endpoint",DataBricks_Endpoint_Description:"https://<your databricks>.azuredatabricks.net",WorkspaceId:"Workspace ID",ArtifactId:"Artifact ID",DataBricks_General_Validation:"Please ensure all values are entered for each external compute configuration.",DataBricks_Endpoint_Validation:"Your databricks endpoint should match the following format: https://<your databricks>.azuredatabricks.net",Mysql_SSL_Mode:"SSL mode",Mysql_SSL_Mode_Preferred:"Preferred",Mysql_SSL_Mode_Verify_Ca:"Verify_ca",Mysql_SSL_Mode_Verify_identity:"Verify_identity",Mysql_Not_Use_System_Trust_Store:"Not use system trust store",Postgresql_Encryption_Method:"Encryption method",Postgresql_Encryption_Method_No_Encryption:"No encryption",Postgresql_Encryption_Method_SSL:"SSL",Postgresql_Encryption_Method_RequestSSL:"RequestSSL",Postgresql_Validate_Server_Certificate:"Validate server certificate",Postgresql_Not_Validate_Server_Certificate:"Not validate server certificate",HDIActivity_HDIClusterTab_LinkedServiceToolTip:"Reference to the HDInsight cluster registered as a linked service.",JobGraphLabel:"Job graph",ScriptLabel:"Script",ScriptTooltip:"View the data flow script representation of this resource",HDIActivity_ScriptTab_AutoFillButtonLabel:"Auto-fill from script",JarLibsLabel:"Jar libs",Activity_Error_Configuration_InvalidName:"Parameter name can't be empty",Activity_Error_Configuration_InvalidType:"Parameter type can't be empty",Activity_Error_Configuration_InvalidDirection:"Parameter direction can't be empty",Activity_Error_Configuration_InvalidValue:"Parameter '{0}' value can't be empty",AMLActivity_Select_LinkedServiceToolTip:"Reference to the Azure Machine Learning Studio (classic) registered as a linked service.",AMLBatchExecutionActivityDisplayText:"Azure Machine Learning batch execution",AMLUpdateResourceActivityDisplayText:"Azure Machine Learning update resource",GlobalParametersLabel:"Global parameters",GlobalParametersConfigurationsLabel:"Global settings",NewParameterLabel:"New parameter",NewMapParameterLabel:"New map parameter",EditMapParameterLabel:"Edit map {0}",NewLocalVariableLabel:"New local variable",NewDatasetParameterLabel:"New dataset parameter",NewLinkedServiceParameterLabel:"New linked service parameter",AMLActivity_Batch_Execution_Key_Placeholder:"New key",AMLActivity_Batch_Execution_Inputs_Key_Title:"Input key",AMLActivity_Batch_Execution_Inputs_Val_Title:"Input storage",AMLActivity_Batch_Execution_Outputs_Key_Title:"Output key",AMLActivity_Batch_Execution_Outputs_Val_Title:"Outputs storage",AMLActivity_Update_Resource_TrainedModelName_Label:"Trained model",AMLActivity_Batch_Execution_Global_Key_Missing_Error:"Global parameter name is required",AMLActivity_Batch_Execution_Global_Val_Missing_Error:"Global parameter value is required",AMLActivity_Batch_Execution_Inputs_Key_Missing_Error:"Input key is required",AMLActivity_Batch_Execution_Inputs_Val_Missing_Error:"Value missing in Input storages",AMLActivity_Batch_Execution_Outputs_Key_Missing_Error:"Output key is required",AMLActivity_Batch_Execution_Outputs_Val_Missing_Error:"Value missing in Output storages",ImportParameters:"Import parameters",ImportDataPathAssignments:"Import data path assignments",AzureMLExecutePipelineActivity_DisplayText:"Azure Machine Learning Execute Pipeline",AzureMLExecutePipelineActivityLabel:"Machine Learning Execute Pipeline",AzureMLExecutePipelineActivity_Select_LinkedServiceToolTip:"Reference to the Azure Machine Learning Service registered as a linked service.",AzureMLExecutePipelineActivity_MLPipelineId_ToolTip:"ID of the Azure Machine Learning published pipeline.",AzureMLExecutePipelineActivity_MLPipelineName_ToolTip:"Name of the Azure Machine Learning published pipeline, select to list Machine Learning pipeline ID",AzureMLExecutePipelineActivity_MLPipelineEndpointName_ToolTip:"Name of the Azure Machine Learning published pipeline endpoint, select to list Machine Learning pipeline endpoint ID",AzureMLExecutePipelineActivity_MLPipelineEndpointVersion_ToolTip:"Version of the Azure Machine Learning published pipeline endpoint, select to list Machine Learning pipeline endpoint versions",AzureMLExecutePipelineActivity_MLExperimentName_ToolTip:"Experiment name of the Azure Machine Learning pipeline runs, default value is the Azure Machine Learning published pipeline name.",AzureMLExecutePipelineActivity_MLParentRunId_ToolTip:"You can supply the run ID to set the parent run of this Azure Machine Learning pipeline run.",AzureMLExecutePipelineActivity_MLContinueOnStepFailure_ToolTip:"Whether to continue execution of other steps in the Azure Machine Learning pipeline run if a step fails, default is false.",HDIActivity_ArgumentsToolTip:"Specify arguments seperated by space for a Hadoop job. The arguments are passed as command-line arguments to each task.",ScriptTab_GetDebugInfoToolTip:"Specifies when the log files are copied to the Azure Storage used by HDInsight cluster (or) specified by scriptLinkedService. Allowed values: None, Always, or Failure. Default value: None.",ScriptTab_ScriptPathToolTip:"Provide the path to the script file stored in the Azure Storage referred by scriptLinkedService. The file name is case-sensitive.",HiveActivity_ScriptTab_LinkedServiceToolTip:"Reference to an Azure Storage linked service used to store the Hive script to be executed. If you don't specify this linked service, the Azure Storage linked service defined in the HDInsight linked service is used.",HiveActivity_ScriptTab_ConfigToolTip:"Specify parameters as key/value pairs for referencing within the Hive script.",HiveActivity_ScriptTab_VariablesToolTip:"Specify the user defined variables under hivevar namespace.",HiveActivity_ScriptTab_QueryTimeoutToolTip:"Only valid when HDInsight linked service is Enterprise Security Package enabled.  It specifies the command timeout (in minute) of the hive query.",HiveActivity_ScriptTab_QueryTimeoutLabel:"Query Timeout",PigActivity_ScriptTab_LinkedServiceToolTip:"Reference to an Azure Storage linked service used to store the Pig script to be executed. If you don't specify this sinked service, the Azure Storage linked service defined in the HDInsight linked service is used.",PigActivity_ScriptTab_ConfigToolTip:"Specify parameters as key/value pairs for referencing within the Pig script.",MapReduceActivity_JarTab_GetDebugInfoToolTip:"Specifies when the log files are copied to the Azure Storage used by HDInsight cluster (or) specified by jarLinkedService. Allowed values: None, Always, or Failure. Default value: None.",MapReduceActivity_JarTab_ClassNameToolTip:"Name of the Class to be executed",MapReduceActivity_JarTab_JarlibsToolTip:"Provide the path to the Jar library files referenced by the job stored in the Azure Storage referred by jarLinkedService. The file name is case-sensitive.",MapReduceActivity_JarTab_JarPathToolTip:"Provide the path to the Jar files stored in the Azure Storage referred by jarLinkedService. The file name is case-sensitive.",MapReduceActivity_JarTab_LinkedServiceToolTip:"Reference to an Azure Storage linked service used to store the Jar files. If you don't specify this linked service, the Azure Storage linked service defined in the HDInsight linked service is used.",KustoActivity_KustoConnectionTab_LinkedServiceToolTip:"Reference to the Kusto registered as a linked service.",KustoActivity_CommandTab_DatabaseToolTip:"Specify the database name",KustoActivity_CommandTab_CommandToolTip:"Specify the command",KustoActivity_CommandTab_CommandTypeToolTip:"Specify the command type",KustoActivity_Control:"Control",AzureDataExplorerCommandActivity_LinkedServiceToolTip:"Reference to an Azure Data Explorer (Kusto) linked service used to run the command.",AzureDataExplorerCommandActivity_CommandDescription:'Specify ADX control command. Note that for async commands, ADX will poll the Operations table on behalf of the user, until the command is completed. In such a case, the result of this command will be the results of ".show operations <operationId>", click to view more information.',AzureDataExplorerCommandActivity_CommandTimeoutDescription:"Specify the wait time before the control command request times out. Default is 20 minutes (00:20:00), and maximum effective timeout is 1 hour (01:00:00).",AzureDataExplorerCommandActivity_CommandTimeoutError:"The maximum effective command timeout is 1 hour (01:00:00).",AzureDataExplorerResourceSelectionWithUser:"Use current login user",AzureDataExplorerResourceSelectionWithSP:"Use service principal",AzureDataExplorerResourceSelectionWithMI:"Use managed identity",AzureDataExplorerNotSupportUserCurrentLoginUserInParameter:'"Use current login user" is not supported when your linked service has parameters.',SelectAPIMethodLabel:"Select API method...",HeadersLabel:"Headers",AzureFunction_LinkedServiceLabel:"Azure Function linked service",AzureFunction_NewLinkedServiceLabel:"New Azure Function linked service",AzureFunctionActivity_LinkedServiceToolTip:"Reference to the Azure Function App registered as a linked service.",ErrorInvalidHeaderKey:"Header key can't be empty",ErrorInvalidHeaderValue:"Header value can't be empty",Activity_Error_BodyRequired:"A valid body is required for PUT and POST requests",AzureFunction_FunctionAppName_tooltip:"Name of the Azure App Function.",AzureFunction_FunctionAppUrl_tooltip:"URL for Azure Function App. https://<accountName>.azurewebsites.net",AzureFunction_FunctionAppKey_tooltip:"Access key for Azure Function. Click \u2018Manage\u2019 in your Azure Function to retrieve either the function key or the host key.",AzureFunction_AccountName_SelectionMethod:"Azure Function App selection method",AzureFunction_AccountName_SelectionMethod_Description:"You can select an Azure Function App url from the list of available Apps in your Azure subscriptions, in which case you don\u2019t need to enter the url manually in text fields.",AzureFunction_SelectionMethod:"Azure Function selection method",AzureFunction_SelectMethodDescription:"Select from a list of available Azure Functions in your Azure subscriptions or enter Azure Function endpoint manually.",AzureFunction_Subscription_Description:"You can select an azure subscription to filter the Azure Function Apps.",AzureFunction_AccountName_Select:"Azure Function App url",Databricks_SelectionMethod:"Azure Databricks selection method",Databricks_SelectMethodDescription:"Select from a list of available Azure Databricks in your Azure subscriptions or enter Azure Databricks endpoint manually.",OpenInPortal:"Open in Azure Portal",StreamingActivity_FileTab_TypePropertiesLabel:"Type properties",FileLinkedServiceLabel:"File linked service",FilePathForMapperLabel:"File path for Mapper",FilePathForReducerLabel:"File path for Reducer",StreamingActivity_FileTab_GetDebugInfoToolTip:"Specifies when the log files are copied to the Azure Storage used by HDInsight cluster (or) specified by fileLinkedService. Allowed values: None, Always, or Failure. Default value: None.",StreamingActivity_FileTab_MapperToolTip:"Specifies the name of the mapper executable",StreamingActivity_FileTab_ReducerToolTip:"Specifies the name of the reducer executable",StreamingActivity_FileTab_MapperPathToolTip:"Provide an array of path to the Mapper program stored in the Azure Storage referred by fileLinkedService. The path is case-sensitive.",StreamingActivity_FileTab_ReducerPathToolTip:"Provide an array of path to the Reducer program stored in the Azure Storage referred by fileLinkedService. The path is case-sensitive.",StreamingActivity_FileTab_InputToolTip:"Specifies the WASB path to the input file for the Mapper. Sample: wasb://<containername>@<accountname>.blob.core.windows.net/example/input/MapperInput.txt",StreamingActivity_FileTab_OutputPathToolTip:"Specifies the WASB path to the output file for the Reducer. Sample: wasb://<containername>@<accountname>.blob.core.windows.net/example/output/ReducerOutput.txt",StreamingActivity_FileTab_LinkedServiceToolTip:"Reference to an Azure Storage linked service used to store the Mapper and Reducer programs to be executed. If you don't specify this linked service, the Azure Storage linked service defined in the HDInsight linked service is used.",SpecifyParameterAsKeyValuePairs:"Specify parameters as key/value pairs.",StoredProcedureActivity_SQLServerTab_LinkedServiceToolTip:"Reference to the Azure SQL Database or Azure Synapse Analytics or SQL Server registered as a linked service.",StoredProcedureActivity_SQLAccountTab_Label:"SQL Account",StoredProcedureActivity_SProcTab_StoredProcedureParameterToolTip:"Specify values for stored procedure parameters.",Direction:"Direction",ScriptActivity_Query_Description:"Database statements that return one or more result sets.",ScriptActivity_NonQuery_Description:"Database statements that perform catalog operations (for example, querying the structure of a database or creating database objects such as tables), or change the data in a database by executing UPDATE, INSERT, or DELETE statements.",ScriptActivity_Size_Header_Description:"Required if the direction is output or inputouput and type is string or byte[].",ScriptActivity_ByteDirection_Invalid:"Byte[] type only supports Output Direction.",ScriptActivity_DateOffsetDirection_Invalid:"Datetimeoffset type does not support Oracle or Snowflake.",ScriptActivity_Int64Direction_Invalid:"Int64 type does not support Oracle.",ScriptActivity_LS_Description:"The target database the script runs on. It should be a reference to a linked service.",ScriptActivity_FullScript_Title:"Full script",ScriptActivity_FullScript_Plain:"Plain script",ScriptActivity_ScriptParameterToolTip:'For Oracle and Snowflake connectors, only positional parameter is supported. Use "?" as placeholders in the query for the parameter. The order of "?" needs to match the order of the provided parameters below.',ScriptActivity_EnableLogDescription:"When selecting this option, you can log the query output",ScriptActivity_LoggingDescription:'Output log is user-defined message generated by "PRINT" statement in SQL Server, Azure SQL, Azure SQL MI, and Azure Synapse Analytics linked services.',ScriptActivity_LogOutput_ActivityOutput:"Activity output",ScriptActivity_LogOutput_ActivityOutput_Description:'User-defined message will be put into the activity output. The maximum size limit of activity output is 4MB. Any exceeded message will be truncated, in which case we recommend "External storage" instead.',ScriptActivity_LogOutput_ExternalStorage:"External storage",ScriptActivity_LogOutput_ExternalStorage_Description:'User-defined message will be uploaded to external storage provided by the linked service. Use this option if messages are truncated with "Activity Output", as this has no size limitation.',ScriptActivity_LogOutput_Title:"Script log output",ScriptActivity_LogLinkedService_Desc:"The linked service of Azure Blob Storage to log the script activity.",ScriptActivity_PathDescription:"The folder path under which logs will be stored. ",ScriptActivity_BlockExecutionTimeout_Label:"Script block execution timeout (minutes)",ScriptActivity_BlockExecutionTimeout_Description:"This property specifies the wait time for the script block execution operation to complete before it times out.",ScriptActivity_BlockExecutionTimeout_ErrorMessage:"Script block execution timeout value must be a number in 1-1440.",ScriptActivity_SnowflakeLS_OauthAuth_ErrorMessage:"Script activity does not support Snowflake using OAuth authentication. Please change the authentication type or use another linked service",USqlActivity_ADLALinkedServiceSelectTab:"ADLA Account",USqlActivity_SettingTab_DegreeOfParallelismToolTip:"The maximum number of nodes that can be simultaneously used to run the job.",USqlActivity_SettingTab_PriorityToolTip:"Determines which jobs out of all that are queued should be selected to run first. The lower the number, the higher the priority.",USqlActivity_SettingTab_RuntimeVersionToolTip:"Runtime version of the U-SQL engine to use",USqlActivity_SettingTab_CompilationModeToolTip:"Compilation mode of U-SQL. Must be one of these values: Semantic: Only perform semantic checks and necessary sanity checks, Full: Perform the full compilation, including syntax check, optimization, code generation, etc., SingleBox: Perform the full compilation, with TargetType setting to SingleBox. If you don't specify a value for this property, the server determines the optimal compilation mode.",USqlActivity_SelectTab_ADLALinkedServiceToolTip:"Linked service to Azure Data Lake Analytics.",USqlActivity_SettingTab_ScriptPathToolTip:"Path to folder that contains the U-SQL script. Name of the file is case-sensitive.",USqlActivity_SettingTab_ScriptLinkedServiceToolTip:"Linked service that links the storage that contains the script",USqlActivity_SettingTab_ConfigToolTip:"Parameters for the U-SQL script",USqlActivity_NewADLALinkedServiceLabel:"New ADLA linked service",USqlActivity_Obsolete:'ADLA USql will be deprecated by Feb 29, 2024. Follow <a target="_blank" href="https://azure.microsoft.com/en-us/updates/migrate-to-azure-synapse-analytics/">link</a> for more info',ScopeActivity_FactoryRestriction:"Data Factory is not authorized to use Scope activity.",ScopeActivity_SettingTab_RuntimeVersionToolTip:"Runtime version of the Scope engine to use",ScopeActivity_SettingTab_MaxNodesToolTip:'The maximum number of nodes that can be simultaneously used to run the job. Only one of "max nodes" or "max nodes percentage" should be specified. If neither are set the default ADLA settings will be used.',ScopeActivity_SettingTab_MaxNodesPercentToolTip:'The maximum percentage of nodes simultaneously used to run the job. At most one of "max nodes" or "max nodes percentage" should be specified. If neither are set the default ADLA settings will be used.',ScopeActivity_SettingTab_ScriptPathToolTip:"Path to folder that contains the Scope script and its local resources.  The contents of this folder will be downloaded when submitting the script. Total size of this folder and its sub-folders must be less than 20MB.\n\nThe name of the file is case-sensitive.",ScopeActivity_SettingTab_ScriptLinkedServiceToolTip:"Linked service that provides access to the storage that contains the script",ScopeActivity_SettingTab_ParamsToolTip:"Parameters for the Scope script. Please note that string parameter values must be enclosed in double-quotes (per Scope guidelines).",ScopeActivity_SettingTab_NebulaArgumentsToolTip:"Additional Scope parameters to pass in during job submission. E.g. -flag1 -param1 value1 -param2 value2 -flag2",ScopeActivity_SettingTab_JobNameToolTip:"Azure Data Lake Analytics job name. Default value is ActivityName_{TimeStamp}.",ScopeActivity_SettingTab_JobOwnerToolTip:"Custom tag to be added to scope job to indicate job owner alias.",ScopeActivity_SettingTab_EmailRecipientsToolTip:"List of email addresses to be notified when the job reaches a terminal state.",ScopeActivity_SettingTab_ScopeScriptInclusionSetToolTip:"List of script resource file extensions separated by semi-colons. Only these files will be uploaded to ADLA as scope job resources.",ScopeActivity_SettingTab_CompileTimeHint:"Last compiled {0}",ScopeActivity_SettingTab_CompileResultsLabel:"View results",ScopeActivity_SettingTab_ParameterStaticType:"Static parameters",ScopeActivity_SettingTab_ParameterDynamicType:"Dynamic parameters",ScopeActivity_SettingTab_AutoFillButtonLabel:"Extract parameters from script",ScopeScript_CompileLabel:"Compile Script",ScopeScript_CompilationParameters:"Compilation parameters",ScopeScript_CompilationParameters_GridEmptyMessage:"No script parameters defined",Time_Of_Last_Processed_Data:"Time of last processed data",Time_Of_Last_Transaction_Commit:"Time of last data commit on source store",processedRowCountLabel:"Processed rows",processedDataVolumeLabel:"Processed data volume",ScopeScript_CompilationParameters_InputDatasetLinkedServiceLabel:"Linked service for input files",ScopeScript_CompilationParameters_InputDatasetLinkedServiceDescription:"This will be used as the linked service of the dataset that will be created/updated if any input files are detected.\n\nThis selection list is filtered to match the default ADLS account of your ADLA account ({0}), which should have an account name of '{1}'.\n\nIf you create a new linked service from here, the following values will be pre-populated match the default ADLS account:\n - Subscription ID: {2}\n - Tenant ID: {3}\n - Resource group: {4}\n - Account: {5}",ScopeScript_CompilationParameters_AdlsLinkedServiceLabel:"Linked service for ADLS output(s)",ScopeScript_CompilationParameters_AdlsLinkedServiceDescription:"ADLS datasets created from compilation will use this linked service.\n\nThis selection list is filtered to match the default ADLS account of your ADLA account ({0}), which should point to {1}.\n\nIf you create a new linked service from here, the following values will be pre-populated match the default ADLS account:\n - Subscription ID: {2}\n - Tenant ID: {3}\n - Resource group: {4}\n - Account Uri: {5}",ScopeScript_CompilationParameters_AdlsStructuredStreamLinkedServiceLabel:"Linked service for ADLS Cosmos Structured Stream output(s)",ScopeScript_CompilationParameters_AdlsStructuredStreamLinkedServiceDescription:"ADLS Cosmos Structured Stream datasets created from compilation will use this linked service.\n\nThis selection list is filtered to match the default ADLS account of your ADLA account ({0}), which should have an account name of '{1}'.\n\nIf you create a new linked service from here, the following values will be pre-populated match the default ADLS account:\n - Subscription ID: {2}\n - Tenant ID: {3}\n - Resource group: {4}\n - Account Name: {5}",ScopeScript_CompilationParameters_AdlsSelectionError:"Incorrect ADLS linked service",ScopeScript_CompilationParameters_AdlsSelectionErrorMessage:"The ADLS linked service that was chosen does not point to {0}, which is the default ADLS account for the ADLA account you've selected for this activity ({1}).<br/><br/><b>Please select an ADLS linked service that points to '{2}'</b>.",ScopeScript_CompilationParameters_AdlsStructuredStreamSelectionError:"Incorrect ADLS Cosmos Structured Stream linked service",ScopeScript_CompilationParameters_AdlsStructuredStreamSelectionErrorMessage:"The ADLS Cosmos Structured Stream linked service that was chosen does not have an account name of '{0}', which is the default ADLS account for the ADLA account you've selected for this activity ({1}).<br/><br/><b>Please select an ADLS Cosmos Structured Stream linked service that has an account name of '{2}'</b>.",ScopeScript_CompilationParameters_InputDatasetLinkedServiceSelectionError:"Incorrect linked service",ScopeScript_CompilationParameters_InputDatasetLinkedServiceSelectionErrorMessage:"The linked service that was chosen does not point to {0}, which is the default ADLS account for the ADLA account you've selected for this activity ({1}).<br/><br/><b>Please select a linked service that points to '{2}'</b>.",ScopeScript_CompilationParameters_EnableInputDetection:"Detect input files",ScopeScript_CompilationParameters_EnableInputDetectionDescription:"If enabled, compilation will yield a predecessor activity that checks for the existence of all input files of this script",ScopeScript_CompilationParameters_PipelineParameterName:"Date time parameter name",ScopeScript_CompilationParameters_PipelineParameterNameDescription:"Name of the pipeline parameter used to provide the date time of the run into the script (e.g. window start time in the case of tumbling window trigger).\n\nIf the script does not depend on the time of the run, this can be left unselected.",ScopeScript_CompilationParameters_InputFilesVariableName:"Input files variable name",ScopeScript_CompilationParameters_InputFilesVariableDescription:"Name of the pipeline variable into which detected file paths will be stored.\n\nMust be set for input detection.",ScopeScript_CompilationParameters_CompileDateTime:"Compile Date (UTC)",ScopeScript_CompilationParameters_CompileDateTimeDescription:"Time value to use for calculating offsets in resulting input files. Please note that this value must match any time value specified above in the compile time parameters.\n\nFor example, if one of your parameters use an expression to pass a date time for your script and you've specified '2018/11/19' above, you must pick the corresponding value here.",ScopeScript_CompilationParameters_CompileTimeParametersLabel:"Compile time parameters",ScopeScript_CompilationParameters_CompileTimeParametersDescription:"Provide values to use during script compilation. Values for any parameters using expressions must be manually provided.",ScopeScript_CompilationResults:"Compile results",CompileResults_AddToFolderLabel:"Dataset folder",CompileResults_AddToFolderTooltip:"Places all datasets to the specified folder. Folder name cannot be changed. Any new datasets from subsequent compilations will be added to the same folder.",CompileResults_AddToFolderPlaceholder:"Folder name...",CompileResults_DatasetNamePlaceholder:"Dataset name...",OutputStreamNameLabel:"Output stream name",Dataset_nameLabel:"Dataset name",Dataset_nameLabelSynapse:"Integration dataset name",CompileResults_LinkedServiceSelectionTitle:"Linked services for Compile Output datasets",ScopeCompile_CompilationProgressHeader:"Compiling...",ScopeCompile_CompilationProgressText:"Script compilation is in progress...",ScopeCompile_CompilationProgressTitle:"Compiling",ScopeCompile_CompilationProgressDesc:"Compilation of the Scope script is in progress.",ScopeCompile_CompilationSubmissionErrorText:"Script compilation submission has failed.",ScopeCompile_CompilationSuccessText:"Script compilation was successful.",ScopeCompile_CompilationErrorText:"Script compilation failed.",ScopeCompile_CompilationPollErrorText:"Encountered an error while checking on the status of the script compilation.",NotebookActivity_PathDescription:"The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. Relative paths will be supported in the future",NotebookActivity_Libraries_Key_Title:"Library type",NotebookActivity_Libraries_Configuration_Title:"Library configuration",AppendLibrariesLabel:"Append libraries",NotebookActivity_Append_Libraries_Type_Jar:"jar",NotebookActivity_Append_Libraries_Type_Egg:"egg",NotebookActivity_Append_Libraries_Type_Wheel:"wheel",NotebookActivity_Append_Libraries_Type_Pypi:"pypi",NotebookActivity_Append_Libraries_Type_Maven:"maven",NotebookActivity_Append_Libraries_Type_Cran:"cran",NotebookActivity_Libraries_Configuration_DBFS_Label:"DBFS URI",NotebookActivity_Libraries_Configuration_DBFS_Jar_Placeholder:"e.g. dbfs:/FileStore/jars/library.jar",NotebookActivity_Libraries_Configuration_DBFS_Egg_Placeholder:"e.g. dbfs:/FileStore/eggs/library.egg",NotebookActivity_Libraries_Configuration_DBFS_Wheel_Placeholder:"e.g. dbfs:/FileStore/whls/library.whl",NotebookActivity_Libraries_Configuration_DBFS_Description:"To get this DBFS URI of the library, use the Databricks CLI, click to view more information",NotebookActivity_Libraries_Configuration_Package_Pypi_Placeholder:"e.g. simplejson or simplejson==3.8.0",NotebookActivity_Libraries_Configuration_Package_Pypi_Description:"The name of the pypi package to install. An optional exact version specification is also supported.",NotebookActivity_Libraries_Configuration_Package_Cran_Description:"The name of the CRAN package to install.",NotebookActivity_Libraries_Configuration_Repo_Pypi_Description:"The repository where the package can be found. If not specified, the default pip index is used.",NotebookActivity_Libraries_Configuration_Repo_Cran_Description:"The repository where the package can be found. If not specified, the default CRAN repo is used.",NotebookActivity_Libraries_Configuration_Repo_Maven_Description:"Maven repo to install the Maven package from. If omitted, both Maven Central Repository and Spark Packages are searched.",NotebookActivity_Libraries_Configuration_Coordinates_Label:"Coordinates",NotebookActivity_Libraries_Configuration_Coordinates_Placeholder:"e.g. org.jsoup:jsoup:1.7.2",NotebookActivity_Libraries_Configuration_Coordinates_Description:"Gradle-style maven coordinates.",NotebookActivity_Libraries_Configuration_Exclusions_Label:"Exclusions",NotebookActivity_Libraries_Configuration_Exclusions_Placeholder:"e.g. slf4j:slf4j,*:hadoop-client",NotebookActivity_Libraries_Configuration_Exclusions_Description:"List of dependences to exclude. Dependences should be separated using comma",Open:"Open",EditNotebookLabel:"Edit notebook",OpenNotebookLabel:"Open notebook",OpenNbSnapshot:"Open notebook snapshot",MonitorApplication:"Monitor application",JarActivity_Main_Class_Name_Description:"The full name of the class containing the main method to be executed. This class must be contained in the JAR provided under append libraries.",JarActivity_Parameters_Section_Description:"Parameters that will be passed to the main method.",JarActivity_Libraries_Empty_Error:"Append libraries should contain at least one library",PythonActivity_Python_File_Label:"Python file",PythonActivity_Python_File_Placeholder:"e.g. dbfs:/my/pi.py",PythonActivity_Python_File_Description:"The URI of the Python file to be executed. DBFS paths are supported. To get this DBFS URI, use the Databricks CLI, click to view more information.",PythonActivity_Parameters_Section_Description:"Command line parameters that will be passed to the Python file.",DBFS_URI_Start_With_Error:"DBFS URI must starts with 'dbfs:'",Activity_Body:"Body",Authentication:"Authentication",WebActivityUseAzureKeyVaultDescription:"Check to select a secret stored in AKV",WebActivityBasicAuthUserNameDescription:"Please provide user name",WebActivityBasicAuthPasswordDescription:"Please provide password",WebActivity_UserNamePlaceholder:"Authorized User name",WebActivity_PasswordPlaceholder:"Select Password from Azure Key Vault",WebActivity_PfxPlaceholder:"Select Pfx from Azure Key Vault",WebActivity_SecretPlaceholder:"Select Secret from Azure Key Vault",WebActivity_CertificatePlaceholder:"Select Certificate from Azure Key Vault",WebActivity_BasicAuthDescription:"Specify user name and password to use with the basic authentication",WebActivity_MSIAuthDescription:"Specify the audience for which Azure Authentication token will be requested when using MSI Authentication (ex: https://management.azure.com/)",WebActivity_ClientCertAuthDescription:"Specify base64-encoded contents of a PFX file and the password",WebActivity_ServicePrincipalAuthDescription:"Specify Tenant, Service Principal id, Service Principal key/certificate and resource",WebActivity_UserAssignedManagedIdentityAuthDescription:"Specify the User Assigned Managed Identity credential for authentication",WebActivity_Pfx:"Pfx",WebActivity_AddDatasetReference:"Add dataset reference",WebActivity_AddLinkedServiceReference:"Add linked service reference",InvalidUrlLabel:"Invalid url",WebActivity_Error_UsernameRequired:"username is required for basic authentication, clientId (username) is required for service principal authentication",WebActivity_Error_ClientIdRequired:"Service principal id is required for service principal authentication",WebActivity_Error_PasswordRequired:"password is required for basic or client certificate authentication",WebActivity_Error_CertificateSecretRequired:"Key / Certificate is required for Service Principal authentication",WebActivity_Error_ResourceRequired:"Resource is required for MSI / Service Principal authentication",WebActivity_Error_CredentialRequired:"Credential is required for MSI / Service Principal authentication",WebActivity_Error_UserTenantRequired:"Tenant is required for service principal authentication",WebActivity_Error_PfxRequired:"Pfx is required for client certificate authentication",WebActivity_Error_PfxAKVOnly:"Pfx should be an AKV secret",WebActivity_Error_CertificateSecretAKVOnly:"Key / Certificate should be an AKV secret",WebActivity_Error_PasswordAKVOnly:"Password should be an AKV secret",Method_Post:"POST",CrossTenant:"Cross tenant sign in",CrossTenantDialogDescription:"Please sign in with the account you wish to use to connect to Azure DevOps.",CrossTenantInformationLabel:"Sign-in information",CrossTenantSignInInformation:"Name: {0}\nEmail: {1}\n Directory: {2} ({3})",CrossTenantAuthErrorTitle:"Access Issue",CrossTenantAuthErrorDescription:"There seems to be an access issue to the configured Azure DevOps repository. Please sign in with an account that has access to the Azure DevOps repository.",CrossTenantLearnMoreLink:'<a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2185875">Learn more</a>',CrossTenantA365LearnMoreLink:'<a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2189612">Learn more</a>',CrossTenantAccountId:"Azure DevOps username:",CrossTenantUsername:"Azure DevOps name:",WebHookActivity_Timeout_tooltip:"The timeout within which the web hook should be called back. Default timeout value is 10mins. Format is in TimeSpan i.e. [d.]hh:mm:ss",WebHookActivity_Timeout_Error:"Timeout is invalid. The value must be formatted as D.HH:MM:SS",WebHookActivity_Invalid_Body:"Body has to be a valid JSON for Webhook Activity",WebHookActivity_Error_InvalidTimeout:"Invalid timeout format. Expected TimeSpan format i.e [d.]hh:mm:ss ",WebHookActivity_ReportStatusOnCallBack:"Report status on callback",WebHookActivity_ReportStatusOnCallBack_ToolTip:"Whether to report status on callback request. Default to false. When set to true, you can set statusCode, output and error in callback request body to be consumed by webhook activity. You can fail the activity by setting statusCode >= 400",WebHookActivity_BannerMessage:"Please note that 'Disable certificate validation' & 'Report status on callback' properties are moved to Advanced section.",Schema_Add_Label:"New column",Schema_Total_Columns_Label:"Total number of columns: ",Schema_Number_of_Columns_Label:"No. of columns: ",Schema_Sample_Data_Label:"Sample data",Format_Label:"Format",Schema_Format_Placeholder:"Format: MM/dd/yyyy",Format_Select_Label:"Select format",Schema_Format_Edit_Label:"Specify format",Schema_Culture_Label:"Culture",Schema_Culture_Placeholder:"Culture: en-US",Schema_Column_Name_With_Space_Around_Warning:"Warning: Space around the column name",Schema_Global_Filter_Placeholder:"      Filter columns",Schema_Column_Default_Type_Placeholder:"ColumnType",Schema_Column_Output_Type:"Output column type",Schema_Excluded_Columns:"Excluded columns",Schema_Excluded_Columns_Detail:"The following columns are excluded for copy because of unsupported data type",Sample_Dataset_Placeholder:"No dataset selected",Validator_ErrorMessage_Pattern:"Value should be in the form of {0}",Validator_ErrorMessage_Pattern_AorB:"Value should be in the form of {0} or {1}",Validator_ErrorMessage_Percentage:"Value is not a number between [0, 100]",Validator_ErrorMessage_Number:"Value is not a number",Validator_ErrorMessage_Integer:"Value is not an Integer",Validator_ErrorMessage_Boolean:"Value is not a boolean",Validator_ErrorMessage_Unique:"Provided name is not unique, provide a unique name",Validator_ErrorMessage_Unique_Factory:"Provided factory name is already in use. Please provide a unique name",Validator_Short_ErrorMessage_Unique_Name:"Name is not unique",Validator_ErrorMessage_Unique_With_Name:"Provided name '{0}' is not unique. Please provide a unique name",Validator_ErrorMessage_AlphaNumberic:"Provided name contains invalid characters, only alphanumeric characters are supported and must start with an alpha character",SparkActivityConfigurationGetDebugInfo:"Debug information",SparkActivityGetDebugInfoAlways:"Always",SparkActivityProxyUser:"Proxy user",SparkActivityAddScript:"Add/Edit script",AddScript:"Add Script",SparkActivityEditScript:"Edit script",SparkActivityJobLinkedServiceToolTip:"The Azure Storage linked service that holds the Spark job file, dependencies, and logs. If you do not specify a value for this property, the storage associated with HDInsight cluster is used.",SparkActivityClassNameToolTip:"Application's Java/Spark main class",SparkActivityArgumentsToolTip:"Specify command-line arguments seperated by space to the Spark program.",SparkActivityProxyUserToolTip:"The user account to impersonate to execute the Spark program.",SparkActivityConfigToolTip:"Specify values for Spark configuration properties listed in the topic: Spark Configuration - Application properties.",SparkActivityGetDebugInfoToolTip:"Specifies when the Spark log files are copied to the Azure storage used by HDInsight cluster (or) specified by sparkJobLinkedService. Allowed values: None, Always, or Failure. Default value: None.",SparkHistoryServer:"Spark history server",SparkUI:"Spark UI",UntilActivityExpressionToolTip:"Expression of Until activity, the value must evaluate to true or false",AddActivityLabel:"Add activity",ActivityName:"Activity name",InsertActivity:"Insert activity",WaitActivity_WaitTime:"Wait time in seconds",FailActivity_FailMessage:"Fail message",FailActivity_FailMessageRequired:"Fail message is required.",FailActivity_ErrorCodeRequired:"Error code is required.",FailActivity_FailMessageNotString:"Fail message must be a string.",FailActivity_ErrorCodeNotString:"Error code must be a string.",FailActivity_FailMessageWhitespacesError:"Fail message cannot be whitespaces.",FailActivity_ErrorCodeWhitespacesError:"Error code cannot be whitespaces.",FailActivity_FailMessageTooltip:"Message displayed in fail activity error body",FailActivity_ErrorCodeTooltip:"Code, such as 500, categorizing error types",ExtensibleActivity_ConnectionPlaceholder:"Choose a Connection to use",ExtensibleActivity_OperationLabel:"Operation",AzurePostgreSqlWriteMethodDescription:"Azure Postgresql write method",SsisActivity_DisplayText:"SSIS package",SsisActivitySsisIrDisplayName:"Azure SSIS IR",RuntimeDisplayName:"Runtime",LoggingPathLabel:"Logging path",SsisActivityLogging:"Logging",SsisActivityParameters:"SSIS parameters",SsisActivityConnectionManager:"Connection manager",SsisActivityConnectionManagers:"Connection managers",SsisActivityConnectionManagerProperty:"Property",SsisActivityPropertyOverrides:"Property overrides",SsisActivityPropertyPath:"Property path",SsisActivityAzureSsisIrLabel:"Azure-SSIS IR",SsisActivityAzureSsisIrDescription:"The designated Azure-SSIS Integration Runtime to execute your package",SsisActivityPackageLocationLabel:"Package location",SsisActivityPackageLocationDescription:"The location of your project/package",SsisActivityPackageLocationSSISDBLabel:"SSISDB",SsisActivityPackageLocationPackageFileLabel:"File system (Package)",SsisActivityPackageLocationProjectFileLabel:"File system (Project)",SsisActivityUse32bitRuntimeLabel:"32-bit runtime",SsisActivityUse32bitRuntimeCheckboxDescription:"Check if your package needs 32-bit runtime to execute",SsisActivityUse32bitRuntimeTextDescription:"Resolve to \u2018x86\u2019/\u2019x64\u2019 for 32/64 -bit runtime, respectively",SsisActivityUseWinauthCheckboxDescription:"Check if your package uses Windows authentication to access data stores, e.g. SQL Servers/file shares on premises, Azure Files, etc",SsisActivityUseWinauthLinkHTML:'(See more info <a target="_blank" href="https://docs.microsoft.com/sql/integration-services/lift-shift/ssis-azure-connect-with-windows-auth?view=sql-server-2017">here</a>)',SsisActivityUseAzureKeyVaultDescription:"Check to use a secret stored in AKV",DomainLabel:"Domain",SsisActivityPackageAccessDomainPlaceholder:"Your package access domain",SsisActivityLogAccessDomainPlaceholder:"Your logging access domain",SsisActivityConfigAccessDomainPlaceholder:"Your configuration access domain",SsisActivityDomainPlaceholder:"Your package execution domain",SsisActivityDomainDescription:'The domain for your package execution\u2013 For example, if it needs to access Azure Files, the domain is "Azure"',SsisActivityPackageAccessDomainDescription:'The domain to access your project/package/configuration \u2013 For example, if you store them in Azure Files, the domain is "Azure"',SsisActivityLogAccessDomainDescription:'The domain to access your log folder \u2013 For example, if you store your logs in Azure Files, the domain is "Azure"',SsisActivityConfigAccessDomainDescription:'The domain to access your configuration \u2013 For example, if you store it in Azure Files, the domain is "Azure"',SsisActivityPackageAccessUserNamePlaceholder:"Your package access username",SsisActivityLogAccessUserNamePlaceholder:"Your logging access username",SsisActivityConfigAccessUserNamePlaceholder:"Your configuration access username",SsisActivityUserNamePlaceholder:"Your package execution username",SsisActivityUserNameDescription:'The username for your package execution\u2013 For example, if it needs to access Azure Files, the username is "<storage account name>"',SsisActivityPackageAccessUserNameDescription:'The username to access your project/package/configuration \u2013 For example, if you store them in Azure Files, the username is "<storage account name>"',SsisActivityLogAccessUserNameDescription:'The username to access your log folder \u2013 For example, if you store your logs in Azure Files, the username is "<storage account name>"',SsisActivityConfigAccessUserNameDescription:'The username to access your configuration \u2013 For example, if you store it in Azure Files, the username is "<storage account name>"',SsisActivityPackageAccessPasswordPlaceholder:"Your package access password",SsisActivityLogAccessPasswordPlaceholder:"Your logging access password",SsisActivityConfigAccessPasswordPlaceholder:"Your configuration access password",SsisActivityPasswordPlaceholder:"Your package execution password",SsisActivityPasswordDescription:'The password for your package execution\u2013 For example, if it needs to access Azure Files, the password is "<storage account key>"',SsisActivityPackageAccessPasswordDescription:'The password to access your project/package/configuration \u2013 For example, if you store them in Azure Files, the password is "<storage account key>"',SsisActivityLogAccessPasswordDescription:'The password to access your log folder \u2013 For example, if you store your logs in Azure Files, the password is "<storage account key>"',SsisActivityConfigAccessPasswordDescription:'The password to access your configuration \u2013 For example, if you store it in Azure Files, the password is "<storage account key>"',SsisActivityPackageCredentialLinkHTML:'(See more info <a target="_blank" href="https://docs.microsoft.com/sql/integration-services/lift-shift/ssis-azure-files-file-shares?view=sql-server-2017">here</a>)',SsisActivityLoggingLevelLabel:"Logging level",SsisActivityLoggingLevelDescription:"The predefined/customized scope of logging for your package execution",SsisActivityLoggingLevelCustomizedLabel:"Customized",SsisActivityLoggingLevelPerformanceLabel:"Performance",SsisActivityLoggingLevelVerboseNoneLabel:"Verbose",SsisActivityLoggingLevelEditDescription:"Check to enter the name of your customized logging level. Uncheck to select a predefined logging level",SsisActivityPackagePathLabel:"Package path",SsisActivityPackagePathPlaceholder:"e.g. FolderName/ProjectName/PackageName.dtsx",SsisActivityPackagePathDescription:"The path to identify your deployed package in SSISDB",SsisActivityFilePackagePathPlaceholder:"\\\\FileShare\\PackageName.dtsx",SsisActivityFilePackagePathDescription:'The UNC path of your package file with its dtsx extension \u2013 For example, if you store your package in Azure Files, its package path will be "\\\\<storage account name>.file.core.windows.net\\<file share name>\\<package name>.dtsx"',SsisActivitiesPackageAccessCredentialDisplayName:"Package access credentials",SsisActivitiesConfigurationAccessCredentialDisplayName:"Configuration access credentials",SsisActivityProjectPathLabel:"Project path",SsisActivityProjectPathPlaceholder:"\\\\FileShare\\ProjectName.ispac",SsisActivityProjectPathDescription:'The UNC path of your project file with its ispac extension \u2013 For example, if you store your project in Azure Files, its project path will be "\\\\<storage account name>.file.core.windows.net\\<file share name>\\<project name>.ispac"',SsisActivityPackageNameLabel:"Package name",SsisActivityPackageNamePlaceholder:"PackageName.dtsx",SsisActivityPackageNameDescription:"The name of selected package in your project with its dtsx extension",PasswordLabel:"Encryption password",SsisActivityPackagePasswordPlaceholder:"Your package encryption password",SsisActivityPackagePasswordDescription:"The password to encrypt your package",ConfigurationFileLabel:"Configuration path",SsisActivityConfigFilePlaceholder:"\\\\FileShare\\ConfigurationName.dtsConfig",SsisActivityConfigFileDescription:'The UNC path of your configuration file with its dtsConfig extension \u2013 For example, if you store your configuration in Azure Files, its configuration path will be "\\\\<storage account name>.file.core.windows.net\\<file share name>\\<configuration name>.dtsConfig"',SsisActivityEnvrionmentPathLabel:"Environment path",SsisActivityEnvironmentPathPlaceholder:"e.g. FolderName/EnvironmentName",SsisActivityEnvironmentPathDescription:"The path to identify your defined execution environment in SSISDB",SsisActivityLogPathPlaceholder:"\\\\FileShare\\FolderName",SsisActivityLogPathDescription:'The UNC path of your log folder \u2013 For example, if you store your logs in Azure Files, your logging path will be "\\\\<storage account name>.file.core.windows.net\\<file share name>\\<log folder name>"',SsisActivitiesLogAccessCredentialDisplayName:"Logging access credentials",SsisActivityPackageCredentialToolTip:"The credentials to access your package",SsisActivityConfigurationCredentialToolTip:"The credentials to access your configuration",SsisActivityLogCredentialToolTip:"The credentials to access your logging folder",SsisActivityFolderDescription:"The name of your existing folder in SSISDB",SsisActivityRefreshMetadataDescription:"Click to fetch the current folders/projects/packages/environments from SSISDB for you to browse and select, then based on the selected project/package, display only the existing SSIS parameters/connection managers for you to assign values to them \u2013 This may take a few minutes",TeamLabel:"Team",PortfolioLabel:"Portfolio",ProjectLabel:"Project",SsisActivityProjectDescription:"The name of your deployed project in SSISDB",PackageLabel:"Package",SsisActivityPackageDescription:"The name of your deployed package in SSISDB with its dtsx extension",EnvironmentLabel:"Environment",SsisActivityEnvironmentDescription:"The name of your configured execution environment in SSISDB",SsisActivityConfigurationModeLabel:"Manual entries",SsisActivityConfigurationModeDescription:"Check to enter your package/environment paths and existing SSIS parameters/connection managers manually. Uncheck to browse and select your folders/projects/packages/environments from SSISDB and assign values to your existing SSIS parameters/connection managers \u2013 This requires your running Azure-SSIS Integration Runtime",SsisActivityCopyCredentialForLogLabel:"Same as package access credentials",SsisActivityCopyCredentialForLogDescription:"Check to use the same credentials for your package access",SsisActivityCopyCredentialForPackageLabel:"Same as package execution credentials",SsisActivityCopyCredentialForPackageDescription:"Check to use the same credentials for your package execution",SsisActivityStorageAccountSelectPackageDescription:"You can select a storage account to list packages",SsisActivityStorageAccountSelectProjectDescription:"You can select a storage account to list projects",SsisActivityStorageAccountSelectConfigurationDescription:"You can select a storage account to list configurations",SsisActivityStorageAccountSelectLogDirectoryDescription:"You can select a storage account to list directories",SsisActivityDuplicateParameter:"Parameter name cannot be duplicated.",SsisActivityParameterNameCannotBeEmpty:"Parameter name cannot be emtpy.",SsisActivityDuplicateConnectionManager:"Connection manager name cannot be duplicated.",SsisActivityDuplicateConnectionManagerProperty:"Connection manager property name cannot be duplicated.",SsisActivityDuplicatePropertyOverridePath:"Property override path cannot be duplicated.",SsisActivityConnectionManagerNameCannotBeEmpty:"Connection manager name cannot be empty.",SsisActivityConnectionManagerPropertyCannotBeEmpty:"Connection manager property name cannot be empty.",SsisActivityPropertyOverridePathCannotBeEmpty:"Property override path cannot be empty.",SsisActivityMetadataNotFound:"Cannot find {0}: {1}",SsisActivityFolderListNotFound:"Fetching your folder list from SSIDB failed: {0} Please click Refresh to fetch your newly added folders/projects/packages/environments from SSISDB or enter them manually.",SsisActivityProjectListNotFound:"Fetching your project list from SSISDB failed: {0} Please click Refresh to fetch your newly added folders/projects/packages/environments from SSISDB or enter them manually.",SsisActivityProjectNotFound:"Fetching your project: '{0}' from SSISDB failed: {1} Please click Refresh to fetch your newly added folders/projects/packages/environments from SSISDB or enter them manually.",SsisActivityPackageListNotFound:"Fetching your package list from SSISDB failed: {0} Please click Refresh to fetch your newly added folders/projects/packages/environments from SSISDB or enter them manually.",SsisActivityEnvironmentListNotFound:"Fetching your environment list from SSISDB failed: {0} Please click Refresh to fetch your newly added folders/projects/packages/environments from SSISDB or enter them manually.",SsisActivityEnvironmentNotFound:"Fetching your environment: '{0}' from SSISDB failed: {1} Please click Refresh to fetch your newly added folders/projects/packages/environments from SSISDB or enter them manually.",MetadataError:"Fetching your folders/projects/packages/environments from SSISDB failed: {0} Please retry later or enter them manually.",SsisActivityParameterListDescritpion:"When your Azure-SSIS Integration Runtime is running and the setting for manual entries is unchecked, the existing SSIS parameters in your selected project/package from SSISDB will be displayed for you to assign values to them.  Otherwise, you can enter them one by one and assign values to them manually \u2013 Please ensure that they exist and are correctly entered for your package executions to succeed",SsisActivityConnectionManagerListDescritpion:"When your Azure-SSIS Integration Runtime is running and the setting for manual entries is unchecked, the existing connection managers in your selected project/package from SSISDB will be displayed for you to assign values to them.  Otherwise, you can enter them one by one and assign values to them manually \u2013 Please ensure that they exist and are correctly entered for your package executions to succeed",SsisActivityPropertyOverrideListDescritpion:"You can enter the paths of existing properties in your selected package from SSISDB one by one and assign values to them manually \u2013 Please ensure that they exist and are correctly entered for your package executions to succeed",SsisActivityChildPackageListDescritpion:"Enter the path that identifies your child package and upload your child package to embed it directly in activity payload \u2013 Once embedded, you can download it later for editing",SsisActivityEnterManuallyForIrIsNotStarted:"Your Azure-SSIS Integration Runtime is not running. Please start it to browse and select your folders/projects/packages/environments from SSISDB or enter them manually.",SsisActivityCannotRefreshMetadataForIrIsNotStarted:"Cannot refresh because Integration Runtime is not running !",SsisActivityRefreshMetadataTimeout:"Cannot finish within {0} seconds. Timeout!",SsisActivityGetRefreshOperationFailed:"Get refresh operation status failed over {0} times! {1}",SsisActivityLaunchRefreshWhenInitFailedDueToMetadataNotFound:"Init ssis activity failed due to metadata not found! Launch refresh!",SsisActivityRequiredParameterIsNotSet:"Required parameter value is not set.",SsisActivityRemoveUnknowParameterOrConnectionManagerProperty:"Previously added SSIS parameters/connection managers properties that do not currently exist in SSISDB have been removed.",SsisFilePackagePathFormatValidatorMessage:"{0} {1} has an invalid package path. The format of the package path is incorrect.",SsisFilePackagePathValidatorMessage:"{0} {1} has an invalid package path. It should end with file extension '.dtsx'",SsisFilePackagePathNotExistPackageStoreValidatorMessage:"{0} {1} has an invalid package path. The package store '{2}' does not exist in the integration runtime.",SsisProjectPathValidatorMessage:"{0} {1} has an invalid project path. It should end with file extension '.ispac'",SsisPackageNameValidatorMessage:"{0} {1} has an invalid package name. It should end with file extension '.dtsx'",SsisConfigPathValidatorMessage:"{0} {1} has an invalid configuration path. It should end with file extension '.dtsConfig'",SsisAccessStorageAuthenticationFailedMessage:"An authorization failure occurred. Recent changes to 'Firewalls and virtual networks' settings may not be in effect yet. If you have recently changed these settings, try waiting for up to a minute, then revisit this experience.",SsisPackageStoreNoPackageStoreValidatorMessage:"{0} {1} has an invalid package path. No package store is found in the integration runtime.",SsisPackageStoreNotFound:"No package store is found in your selected IR.",SsisActivityInlinePackageNameDisplayName:"Embedded package name",SsisActivityPackageContentDisplayName:"Package content",SsisActivityPackageLastModifiedDateDisplayName:"Package last modified date",SsisActivityChildPackagesDisplayName:"Child packages",SsisActivityUseChildPackageLabel:"Execute package task",SsisActivityUseChildPackageDescription:"Check if your package uses execute package task and add child packages manually below",SsisActivityUsePackageConfigurationLabel:"Use package configuration",SsisActivityInlinePackageLabel:"Embedded package",SsisActivityInlineChildPackagePathPlaceholder:"Input child package path",SsisActivityInlineChildPackagePathLabel:"Child package path",SsisActivityInlineChildPackageContentLabel:"Child package content",SsisActivityInlinePackagePlaceholder:"Drag & drop your package here",SsisActivityInlinePackageInputDescription:"Upload your package to embed it directly in activity payload \u2013 Once embedded, you can download it later for editing",SsisActivityInlinePackageParameterizeDescription:"Create a pipeline parameter with your embedded package and use it in the payload of your activity/multiple activities",SsisActivityInlineConfigurationLabel:"Inline Configuration",SsisActivityInlineConfigurationPlaceholder:"Drag configuration file to here",SsisActivityInlineConfigurationDescription:"Use inline configuration to execute SSIS package on Integration Runtime",SsisActivityInlinePackageTooltip:"Inline package",SsisActivitySqlLinkedServiceLabel:"SQL linked service",SsisActivitySqlLinkedServiceDescription:"Select the linked service of SQL server",SsisActivityPackageStoreLabel:"Package store",SsisActivityPackageStoreDescription:"The name of your package store",SsisActivityPackageStoreNameLabel:"Package store name",SsisActivityPackageStore_PackagePath_Description:"The path of your package file in its package store without its dtsx extension",SsisActivityPackageStore_PackagePath_Placeholder:"FolderName\\PackageName",SsisActivityPackageStore_Placeholder:"PackageStoreName",SsisActivityInvalidSSISPackageExtension:"Invalid package file. Your package file should have a dtsx extension.",SsisActivityReadSelectedPackageFailed:"Read package failed. {0}",SsisActivityParsePackageFailed:"Fail to parse package. {0}. If your package uses Execute Package Task, please add child packages manually.",SsisActivityCantParseProtectionLevel:"Fail to parse package protection level. Embedded package supports DontSaveSensitive/EncryptSensitiveWithPassword/EncryptAllWithPassword/EncryptSensitiveWithUserKey protection levels. If your package uses Execute Package Task, please add child packages manually",SsisActivityCantSupportEncryptAllWithUserKey:"Package configured with EncryptAllWithUserKey protection level is not supported. Embedded package supports only DontSaveSensitive/EncryptSensitiveWithPassword/EncryptAllWithPassword/EncryptSensitiveWithUserKey protection levels.",SsisActivityCantDecryptSensitiveFromEncryptSensitiveWithUserKey:"Sensitive data cannot be decrypted from package configured with EncryptSensitiveWithUserKey protection level. Please re-enter all sensitive data on SSIS Parameters, Connection Managers, or Property Overrides tabs.",SsisActivityCantFindChildPackageFromEncryptedPackage:"Failed to identify child packages in a parent package configured with {0} protection level. If your package uses Execute Package Task, please add child packages manually",SsisActivityActivityInlinePackageOverSize:"Total embedded package size is limited to {0}KB per pipeline and your total package size is {1}KB. Please parameterize your embedded packages using pipeline parameters.",SsisActivityPipelineInlinePackageOverSize:"Pipeline payload size is limited to {0}KB and your pipeline size is {1}KB. Please separate your activities with embedded packages into different pipelines or use file systems/file shares/Azure Files for your package locations.",SsisActivityCompressPackageFailed:"Fail to compress package. {0}. Please confirm selected package is correct or contact Microsoft support.",SsisActivityCannotHandleProjectReferenceChildPackage:"Task [{0}] refers to a package using project reference, it is not supported with embedded package.",SsisActivityCannotHandleSqlServerChildPackage:"Task [{0}] refers to package in SQL Server, please make sure that it is accessible from your Azure-SSIS Integration Runtime.",SsisActivityCannotHandleExpressChildPackage:"Task [{0}] refers to a package using expression. If it is in SQL Server, please make sure that it is accessible from your Azure-SSIS Integration Runtime. Otherwise, please add the child package manually.",Validation_StorageAccountNameGen1:"Storage account names must be 3 or more characters in length and may contain numbers and lowercase letters only.",Validation_SapCDCSubscriberNameAndProcess:"{0} is invalid, only letters, numbers, '+', '-', '$', '.' and '_' are allowed.",Validation_SapCDCSubscriberNameRequired:"'Subscriber name' is required in linked service '{0}' when run mode is '{1}'.",Validation_StorageAccountName:"Storage account names must be between 3 and 24 characters in length and may contain numbers and lowercase letters only.",Validation_MinimumsizeFileError:"MinimumSize is not valid for dataset pointing to a folder",Validation_ChildItemsFolderError:"ChildItems is not valid for dataset pointing to a file",ValidationActivity_TimeoutTooltip:"Specifies the timeout for the activity to run. If no value is specified, default value is 12 hours. Format is D.HH:MM:SS",ValidationActivity_TimeoutError:"Timeout value is invalid. The value must be formatted as D.HH:MM:SS",ValidationActivity_Sleep:"Sleep",ValidationActivity_SleepTooltip:"A delay in seconds between validation attempts. If no value is specified, default value is 10 seconds.",ValidationActivity_MinimumSize:"Minimum size",ValidationActivity_MinimumSizeTooltip:"Minimum size of a file in bytes. If no value is specified, default value is 0 bytes",ChildItemsLabel:"Child items",ValidationActivity_ChildItems_Ignore:"Ignore",ValidationActivity_ChildItems_Ignore_Tooltip:"Check if the folder exists only",ValidationActivity_ChildItems_True_Tooltip:"Check if the folder exists and has items in it",ValidationActivity_ChildItems_False_Tooltip:"Check if the folder exists and that it is empty",ValidationActivity_DatasetTooltip:"Activity will block execution until it has validated this dataset reference exists, or timeout has been reached.",CustomActivityAzureBatchLinkedService:"Azure Batch linked service",CustomActivitySettingsCommandTooltip:"Command to be executed. Use 'Resource linked service' and 'Folder path' in the Settings if a custom application is referenced but not installed on each node in the Batch pool.",CustomActivitySettingsResourceLinkedService:"Resource linked service",CustomActivitySettingsResourceLinkedServiceToolTip:"Azure Storage linked service to the Storage account where the custom application is stored",FolderPath_Label:"Folder path",FolderOrTable_Label:"folder path or table",FolderPaths:"Folder paths",CustomActivitySettingsFolderPathTooltop:"Path to the folder of the custom application and all its dependencies",BrowseLocal_Label:"Browse local",BrowseStorage_Label:"Browse storage",BrowseFileStorage_Label:"Browse file storage",CustomActivitySettingsReferenceObjects:"Reference objects",CustomActivitySettingsReferenceObjectsTooltip:"An array of existing linked services and datasets. The referenced linked services and datasets are passed to the custom application in JSON format so your custom code can reference resources of the {0}",CustomActivitySettingsAddReferenceObject:"Add reference object",CustomActivitySettingsExtendedProperties:"Extended properties",CustomActivitySettingsExtendedPropertiesTooltip:"User-defined properties that can be passed to the custom application in JSON format so your custom code can reference additional properties",CustomActivitySettingsRetentionPeriod:"Files retention time in days",CustomActivitySettingsRetentionPeriodLabel:"Retention time in days",CustomActivity_LoadLinkedService_ErrorMessage:"Failed to load linked service",JarLinkedServiceLabel:"Jar linked service",PolybaseSetting_AllowPolybase_Label:"Allow PolyBase",PolybaseSetting_Polybase_Label:"PolyBase",PolybaseSetting_RejectType_Label:"Reject type",PolybaseSetting_RejectValue_Label:"Reject value",PolybaseSetting_RejectSampleValue_Label:"Reject sample value",PolybaseSetting_UseTypeDefault_Label:"Use type default",PolybaseSetting_PolybaseToolTip:"PolyBase is a high-throughput mechanism to load large amounts of data into Azure Synapse Analytics or SQL Pool. ",PolybaseSetting_Error_RejectValueRange:"Reject value should be a number between [0, 100].",PolybaseSetting_Error_RejectValueWhole:"Reject value should be a whole number.",PolybaseSetting_Error_RejectSampleValueRequired:"Reject Sample value is required.",PolyBaseSetting_Warning_SourceMSI:"Please make sure to assign Storage Blob Data Contributor RBAC role to the SQL Database server.",Staging_DataFlow_Tooltip:"Staging should only be configured when your data flow has Azure Synapse Analytics as a sink or source.",CopyMethod_Label:"Copy method",CopyMethod_BulkInsert_Label:"Bulk insert",CopyCommand:"Copy command",CopyCommand_AllowCopyCommand_Label:"Allow copy command",CopyCommand_AdditionalOptions_Label:"Additional options",CopyCommand_MaxErrors_Label:"Max errors",CopyCommand_DateFormat_Label:"Date format",CopyCommand_DateFormat_Default:"Session default",CopyCommand_DefaultValues_Label:"Default values",CopyCommand_CopyMethod_CopyCommandAdditionalPropertiesErrorTemplate:"{0} in copy command settings can only be {1}.",CopyCommand_CopyMethod_CopyCommandAdditionalPropertiesErrorNumber:"a number",CopyCommand_CopyMethod_CopyCommandNotSupported_Error_Template:"Direct copying data to {0} using Copy command is not supported {1}.",CopyCommand_CopyMethod_CopyCommandOnlySupported_Error_Template:"Direct copying data to {0} using Copy command is only supported ",CopyCommand_CopyMethod_CopyCommandSnowflakeSource_Error_Template:"Direct copying data to {0} using Copy command is not supported for this source type. Choose PolyBase instead, or use a separate copy activity to stage data in Blob or ADLS Gen2 first.",CopyCommand_CopyMethod_CopyCommandSource_Error_Template:"Direct copying data to {0} using Copy command is not supported for this source type. Please enable staging.",CopyCommand_CopyMethod_CopyCommandSource_Error:"when source dataset is DelimitedText, Parquet, Orc with Azure Blob Storage or Azure Data Lake Storage Gen2 linked service.",CopyCommand_CopyMethod_CopyCommandSourceCompression_Error:'when {0} dataset "Compression type" is set as {1}',CopyCommand_CopyMethod_CopyCommandSourceAuthType_Error:'when {0} linked service "Authentication method" is set as {1}.',CopyCommand_CopyMethod_CopyCommandSourceAuthType_Error_Trident:'when {0} connection "Authentication method" is set as {1}.',CopyCommand_CopyMethod_CopyCommandRowDelimiter_Error:'when source dataset "Row delimiter" is set as "No delimiter" or "Default"',CopyCommand_CopyMethod_CopyCommandRowDelimiter_Error_Trident:'when source "Row delimiter" is set as "No delimiter" or "Default"',CopyCommand_CopyMethod_CopyCommandEncodingName_Error:'when {0} dataset "Encoding name" is set as {1}',CopyCommand_CopyMethod_CopyCommandNullValue_Error:'when source dataset "Null value" is set as empty.',CopyCommand_CopyMethod_CopyCommandSkipLineCount_Error:'when copy source "Skip line count" is 0',CopyCommand_CopyMethod_CopyCommandEscapeQuoteChar_Error:"when source dataset {0} are the same and both not empty.",CopyCommand_CopyMethod_EscapeChar:'"Escape character" and "Quote character"',CopyCommand_CopyMethod_CopyCommandModifiedTime_Error:"when source dataset {0} are both empty",CopyCommand_CopyMethod_ModifiedTime:'"Modified datetime start" and "Modified datetime end"',CopyCommand_CopyMethod_CopyCommandRecursive_Error:'when copy source "{0}" is set as true',CopyCommand_CopyMethod_CopyCommandWildcardFileName_Error:"when copy source \"WildCard file name\" is set as '*'.",CopyCommand_CopyMethod_CopyCommandWildcardFileName_NewError:"when copy source \"WildCard file name\" is set as '*' or '*.*'.",CopyCommand_CopyMethod_CopyCommandWildcardFolderPath_Error:'when copy source "WildCard folder path" is set',CopyCommand_CopyMethod_CopyCommandFaulttoleranceDataset_Error:'when source dataset is Parquet or Orc and fault tolerance is set as "Skip and log incompatible rows"',CopyCommand_CopyMethod_CopyCommandFaulttoleranceLinkedservice_Error:"when fault tolerance linkedservice is Azure Blob Storage or Azure Data Lake Storage Gen2 linked service.",CopyCommand_CopyMethod_CopyCommandJSONFilePattern_Error:'when copy sink "File pattern" is set as "Set of objects".',CopyCommand_CopyMethod_CopyCommandPleaseFixTemplate:' Please fix "{0}" or enable staging.',CopyCommand_Settings:"Copy command settings",FaultToleranceWarningInSink:'You can log the skipped incompatible rows by enabling log settings under "Settings" tab.',FaultToleranceWarningInSettings:"To enable fault tolerance for this source-sink copy pair, also go to Sink ",FaultToleranceWarningInSettingsConfigure:" and configure ",FaultToleranceWarningInSettingsForPolybase:"reject type and value.",FaultToleranceWarningInSettingsForSqlDWCopyCommand:'additional option "MAXERRORS".',FaultToleranceWarningInSettingsForSnowflake:'"ON_ERROR".',FaultToleranceWarningInSettingsForStagedCopy:'Please select "Skip incompatible rows" in fault tolerance setting to fully enable fault tolerance.',CopyCommand_Description:"Use COPY statement to load data from Azure storage into Azure Synapse Analytics or SQL Pool. ",DirectCopyOnlySuppported_ErrorTemplate:"is only supported {0}",DirectCopyNotSuppported_ErrorTemplate:"is not supported {0}",PleaseSpecifyStaging_Error:", for other dataset or linked service, ",DirectCoppy_Prefix_Error:'when copy source "Prefix" is empty,',DirectCopy_AdditionalColumns_Error:'when copy source specifies "Additional columns",',GitCommandOnlyAppliesToGitMode:"These shortcuts only apply when Git mode is enabled.",ShortcutApplicabeGitModeOrLiveGitDisconnect:"This shortcut only applies when Git mode is enabled or in live mode after disconnecting from a Git configuration.",Snowflake_DirectCopy_Template:"Direct copying data {0} Snowflake ",Snowflake_StagedCopy:"Staged copying data to Snowflake ",Snowflake_DirectCopySupported_Source:"when source dataset is DelimitedText, Parquet, JSON with Azure Blob Storage or Amazon S3 linked service",Snowflake_DirectCopySupported_Sink:"when sink dataset is DelimitedText, Parquet or JSON with Azure Blob Storage linked service",Snowflake_DirectCopyRowDelimiter_Error:'when {0} dataset "Row delimiter" is set as single character or \\r\\n,',Snowflake_DirectCopy_RowDelimiterisCRLF:' or "Row delimiter" is \\r\\n,',Snowflake_DirectCopy_CopyCommandFirstRowAsHeader_Error:'when source dataset "First row as header" is set as false',Snowflake_DirectCopy_CopyMapping_Error:"when copy mapping is not specified,",Snowflake_DirectCopy_QuoteChar_Error:'when {0} dataset "Quote character" is set as {1}',Snowflake_SQLDW_ErrorMessage_Template:'Copying data from Snowflake to {0} is only supported when Copy method is "Copy command" or "Bulk insert", and staging is specified.',Snowflake_SinkLinkedService_OnlySupportAzure_Error:"Copying data into Snowflake only supports Snowflake account hosted on Microsoft Azure.",Snowflake_JSON_Schema_Error:"Please note that JSON dataset can produce one and only one column of type variant or object or array in Snowflake dataset.",Snowflake_AccountName_Desc:"The full name of your Snowflake account, e.g. xy12345.east-us-2.azure.",Snowflake_UserName_Desc:"The login name of the user for the connection.",Snowflake_Password_Desc:"The password for the user.",Snowflake_Database_Desc:"The default database to use once connected. It should be an existing database for which the specified role has privileges.",Snowflake_Warehouse_Desc:"The virtual warehouse to use once connected. It should be an existing warehouse for which the specified role has privileges.",Snowflake_Role_Desc:"The default access control role to use in the Snowflake session. The specified role should be an existing role that has already been assigned to the specified user. The default role is PUBLIC.",Snowflake_AuthType_Desc:"OAuth authentication now supports external OAuth for Microsoft Azure AD.",Snowflake_Oauth_Disabled_Error:"OAuth authentication is disabled.",AzureDatabricksDeltaLake_DirectCopy_Template:"Direct copying data {0} Azure Databricks Delta Lake ",AzureDatabricksDeltaLake_DirectCopySupported_Source:"when source dataset is DelimitedText, Parquet, Avro with Azure Blob Storage or Azure Data Lake Storage Gen2",AzureDatabricksDeltaLake_DirectCopySupported_Sink:"when sink dataset is DelimitedText, Parquet or Avro with Azure Blob Storage linked service or Azure Data Lake Storage Gen2",AzureDatabricksDeltaLake_WildcardFileName_Error:"when wildcard file name doesn't contain '?' or '^'",AzureDatabricksDeltaLake_WildcardFolderPath_Error:"when wildcard folder path is empty",AzureDatabricksDeltaLake_Recursive_Error:"when recursive is set false or recusive is set true with wildcard file name being set as '*'",AzureDatabricksDeltaLake_DirectCopy_To_RowDelimiter_Error:'when {0} dataset "Row delimiter" is set as single character or auto detect',AzureDatabricksDeltaLake_DirectCopy_From_RowDelimiter_Error:'when {0} dataset "Row delimiter" is set as single character',AzureDatabricksDeltaLake_DirectCoppy_SkipLineCount_Error:"when skip line count is specified,",AzureDatabricksDeltaLake_Sink_Folder_Error:"when sink is a folder",AzureDatabricksDeltaLake_Sink_CSV_FileExtension_Error:"when sink file extension is '.csv'",ImportOrExportSettings_Label:"Import or export settings",ImportSettings_Label:"Import settings",ExportSettings_Label:"Export settings",Additional_Snowflake_CopyOptions_Label:"Additional Snowflake copy options",Additional_Snowflake_FormatOptions_Label:"Additional Snowflake format options",Additional_Snowflake_CopyOptions_Desc:"Specify additional Snowflake copy options which will be used in Snowflake COPY statement to load data.",Additional_Snowflake_FormatOptions_Desc:"Specify additional Snowflake format options which will be used in Snowflake COPY statement to load data.",AzureFileStorageAuthType_Error:'when source linked service "Authentication method" is Account Key" or "SAS URL".',AzureFileStorageBasicAuthType_Warning:"To copy data from/to Azure File Storage, use other authentication types which are more performant than Basic auth.",DataFlowActivityIncompatibleRowsTooltip:"You have different options when dealing with incompatible rows. You can either choose to abort and fail the copy activity upon encountering the first incompatibility, or continue copying data by skipping all incompatible rows. You also have the option to log the incompatible rows so you can examine the cause for failure, fix the data on the data source, and retry.\nThe types of incompatibility we detect include: native data types unsupported by .NET type system, type incompatibility between source native type and destination native type, column mismatch between source and destination, and primary key violation while loading data into a relational database.",DataMovementUnitTooltip:'Specify the maximum value of DIU to be used. Value can be 2-256. When you choose "Auto", the optimal DIU setting is dynamically applied based on your source-sink pair and data pattern.',ParallelCopyTooltip:"Specify the degree of parallelism that data loading would use.",DataMovementUnitErrorMessage:"Value must be a number in 2-256",ValueMustBeAPositiveNumber:"Value must be a positive number",PipelineRunAlreadyExist:"Pipeline run is already in progress, please wait for its completion.",PipelineRunAlreadyCompletion:"Pipeline run is already completed",PipelineRunAlreadyExistDesc:"Can't start the debug run {0} ({1}) because the run is already in progress. Wait for the run to complete before creating a new one.",Empty_Linked_Service_Error_Message:"Please select a linked service",Empty_TableName_Error_Message:"Please select a table",Empty_DatabaseName_Error_Message:"Please select a database",Empty_WorkSheetName_Error_Message:"Please select a work sheet",Empty_FilePath_Error_Message:"Please select a file",Empty_String_Error_Message:"Empty string is not allowed",Empty_SQL_Pool_Error_Message:"Please select a sql pool",Preview_NoFileFormat_Error_Message:"Please disable binary copy in dataset to preview data.",Schema_NoFileFormat_Error_Message:"Please disable binary copy in dataset to import schema.",Timestamp_Format_Label:"Timestamp format",TypeConversion_AllowDataTruncation_Label:"Allow data truncation",TypeConversion_AllowDataTruncation_Description:"Allow data truncation when converting source data to sink with different type during copy, e.g. from decimal to integer, from DatetimeOffset to Datetime.",TypeConversion_TreatBooleanAsNumber_Label:"Treat boolean as number",TypeConversion_TreatBooleanAsNumber_Description:"Treat boolean as number, e.g. true as 1.",TypeConversion_DatetimeFormat_Label:"DateTime format",TypeConversion_DatetimeFormat_Description:'Format string when converting between dates without time zone offset and strings, e.g. "yyyy-MM-dd HH:mm:ss.fff"',TypeConversion_DatetimeOffsetFormat_Label:"DateTimeOffset format",TypeConversion_DatetimeOffsetFormat_Description:'Format string when converting between dates with time zone offset and strings, e.g. "yyyy-MM-dd HH:mm:ss.fff zzz".',TypeConversion_TimeSpanFormat_Label:"TimeSpan format",TypeConversion_TimeSpanFormat_Description:'Format string when converting between time periods and strings, e.g. "dd\\.hh\\:mm\\:ss". Click to learn more.',TypeConversion_Culture_Description:'Culture information to be used when convert types, e.g. "en-us", "fr-fr".',TypeConversionSettings_MainGroup_Label:"Type conversion settings",TypeConversion_Enable_Label:"Type conversion",TypeConversion_Enable_Description:"Enable the new data type conversion experience.",StagingSetting_EnableLabel:"Enable staging",StagingSetting_DisableWarning:"It is highly recommended that you enable staging in production workloads to get better performance.",StagingSetting_Recommended:"We recommend enabling staging to improve performance with Azure Synapse Analytics datasets.",StagingSetting_EnableDescription:"Specify whether to copy data via an interim staging store. Enable staging only for the beneficial scenarios, e.g. load data into Azure Synapse Analytics via PolyBase, load data to/from Snowflake, load data from Amazon Redshift via UNLOAD or from HDFS via DistCp, etc.",StagingSetting_EnableDescriptionTrident:"Specify whether to copy data via an interim staging store. Enable staging only for the beneficial scenarios.",StagingSetting_LinkedServiceLabel:"Staging account linked service",StagingSetting_ConnectionLabel:"Staging account connection",StagingSetting_LinkedServiceDescription:"Specify the name of an AzureStorage linked service, which refers to the instance of Storage that you use as an interim staging store. You cannot use Storage with a shared access signature to load data into Azure Synapse Analytics or SQL Pool via PolyBase. You can use it in all other scenarios.",StagingSetting_LinkedServiceDescription_SimpleVersion:"Specify the connection of an Azure storage data source as an interim staging store.",Dataflow_Snowflake_StagingSetting_LinkedServiceDescription:"Specify the name of an Azure Blob Storage linked service, which refers to the instance of Storage that you use as an interim staging store. You must use Storage with a shared access signature.",StagingSetting_MainGroupLabel:"Staging settings",StagingSetting_PathLabel:"Storage Path",StagingSetting_PathDescription:"Specify the Blob storage path that you want to contain the staged data. If you do not provide a path, the service creates a container to store temporary data. Specify a path only if you use Storage with a shared access signature, or you require temporary data to be in a specific location.",StagingSetting_ContainerDescription:"Specify the Blob storage container that you want to contain the staged data. If you do not provide a container, the service creates a container to store temporary data. If you specify a folder path, only the container part will be used.",StagingSetting_EnableCompressionLabel:"Enable Compression",StagingSetting_EnableCompressionDescription:"Specifies whether data should be compressed before it is copied to the destination. This setting reduces the volume of data being transferred.",StagingSetting_ErrorMessage_ShouldNotBe_Template:'{0} "Authentication method" should not be {1}.',StagingSetting_ErrorMessage_ShouldBe_Template:'{0} "Authentication type" should be {1}.',StagingSetting_ErrorMessage_Format:"Source format: ",StagingSetting_ErrorMessage_DirectPolybaseNotMet:". Conditions are not met to run PolyBase copy directly,",StagingSetting_ErrorMessage_PleaseSpecifyStaging:" please enable staging",StagingSetting_ErrorMessage_FixConditionsTemplate:" or fix {0}.",StagingSetting_ErrorMessage_SourceType_Template:"Source is not Blob Storage, Azure Data Lake Storage Gen1 or Azure Data Lake Storage Gen2. Staging account is required to load data into {0} using PolyBase.",StagingSetting_ErrorMessage_JsonType_Template:"Staging account is required to load data into {0} using PolyBase.",StagingSetting_ErrorMessage_MissingLinkedService:"Please select a {0} for staging",StagingSetting_ErrorMessage_InvalidLinkedServiceType:"Invalid staging linked service type. Supported types are: {0}.",StagingSetting_BlobFSLinkedServiceNotSupportedTemplate:"Azure Data Lake Storage Gen2 is not supported in staging settings when using {0}",DistCpFormatValidation:"HDFS Distcp setting is supported only when source dataset and sink dataset have same format",Delete_MaxConcurrentConnections_Description:"The upper limit of concurrent connection count to access to the data store. Supported value is in range 1-10.",DeleteLoggingSetting_EnableLabel:"Enable logging",DeleteLoggingSetting_EnableDescription:"Specify whether you want to log the deleted file names in a logging store.",LoggingSetting_LinkedServiceLabel:"Logging account linked service",LoggingSetting_ConnectionLabel:"Logging account connection",DeleteLoggingSetting_LinkedServiceDescription:"The linked service of Azure Blob Storage, Azure Data Lake Storage Gen1 or Azure Data Lake Storage Gen2 to log the deleted file names.",DeleteLoggingSetting_PathDescription:"The path of the log file that contains the deleted file names.",DeleteLoggingSetting_ErrorMessage_MissingLinkedService:"Please select a {0} for logging.",DirectPolybase_ErrorMessage_BlobFormat:"PolyBase copy can be run directly from blob only in case that blob is in Text, ORC or Parquet format",DirectPolybase_ErrorMessage_RowDelimiter:"Row delimiter has to be \\n",DirectPolybase_ErrorMessage_ColumnDelimiter:"Column delimiter must be set",DirectPolybase_ErrorMessage_EmptyAsNull:"Empty string should be treated as Null and Null value must be set to empty string",DirectPolybase_ErrorMessage_NullValue:"Null value must be set to empty string",DirectPolybase_ErrorMessage_EscapeChar:"Escape character cannot be set",DirectPolybase_ErrorMessage_QuoteChar:"Quote character cannot be set",DirectPolybase_ErrorMessage_Encoding:"Encoding has to be UTF-8",DirectPolybase_ErrorMessage_Compression:"Compression type has to be GZip or Deflate, or no compression",DirectPolybase_ErrorMessage_CompressionForV2:"Compression type has to be gZip or deflate, or no compression",DirectPolybase_ErrorMessage_StoreSettings:"PolyBase copy can only be run directly with source {0} setting as {1}",WorkerOptions:"Worker options",WorkerOptions_Autoscaling:"Autoscaling",WorkerOptions_SingleNodeJobCluster:"Single node",Worker_Auto_Scaling_Tip:'<a target="_blank" href=https://docs.microsoft.com/azure/databricks/clusters/configure#cluster-size-and-autoscaling>Autoscaling</a> enables the cluster to automatically scale between the minimum and maximum number of nodes, based on load. <a target="_blank" href=https://docs.microsoft.com/azure/databricks/clusters/single-node>Single node clusters </a> consists of a driver but no workers.',Worker_Workers_Label:"Workers",Worker_Min_Workers_Label:"Min Workers",Worker_Max_Workers_Label:"Max Workers",Worker_Zero_Worker_Warning:"Warning: a cluster needs at least 1 worker to run Spark commands or import tables.",Worker_Min_Greater_Than_Max_Error:"Min must be less than Max",Worker_Out_Of_Range_Error:"Must be between {0} and {1}",Worker_Min_Out_Of_Range_Error:"Min must be between {0} and {1}",Worker_Max_Out_Of_Range_Error:"Max must be between {0} and {1}",Worker_Require_Error:"Worker count is required",Worker_Min_Max_Require_Error:"Min or max worker count is required",Worker_SpotInstances:"Spot instances",Worker_SpotInstances_Description:'<a target="_blank" href="https://docs.microsoft.com/azure/virtual-machines/spot-vms">Learn more</a>',Authoring_OpenResource_Error:"Failed to open resource",Authoring_Sidebar_CreateEntityError:"Failed to create resource. Resource type '{0}' was not recognized.",Authoring_Home_Prompt:"Use the resource explorer to select or create a new item",Authoring_Collapse_Toolbox:"Collapse toolbox pane",Authoring_Expand_Toolbox:"Expand toolbox pane",AuthoringCollapseFactoryResources:"Collapse resources pane",AuthoringExpandFactoryResources:"Expand resources pane",Authoring_Expand_Activities:"Expand all activities",Authoring_Collapse_Activities:"Collapse all activities",Authoring_Properties_Panel:"Properties panel",Authoring_Pipeline_Toolbox:"Pipeline toolbox",Authoring_Add_New_Resource:"Add new resource",Authoring_ExpandResourceExplorer:"Expand resource explorer",Authoring_CollapseResourceExplorer:"Collapse resource explorer",Authoring_CloseUnmodifiedTabs:"Close unmodified tabs",Authoring_CloseAllTabs:"Close all tabs",Authoring_CloseAllOtherTabs:"Close all other tabs",Authoring_CloseAllToRightTabs:"Close all to right",ExpandTOC:"Expand table of contents",CollapseTOC:"Collapse table of contents",toc:"Table of contents",CollapseValidationPane:"Collapse validation pane",ExpandValidationPane:"Expand validation pane",Tooltip_Unavailable:"Currently unavailable. Contact the workspace admin to obtain the right role or permission.",Tooltip_Unavailable_Suffix:" - currently unavailable. Contact the workspace admin to obtain the right role or permission.",PreserveRulesValidationBinaryCopyMessage:"Preserve could only be enabled when source or sink dataset enables 'Binary copy'.",PreserveRulesValidationStagingMessage:"Preserve cannot be enabled when staging is enabled",ColumnMappingAutoMapText:"Auto mapping",ColumnMappingSourceStatusTemplate:"Source fields: {0} / {1} mapped",ColumnMappingSinkStatusTemplate:"Sink fields: {0} / {1} mapped",ColumnMappingSourceHeader:"Field / Type",ColumnMappingFieldHeader:"Field",ColumnMappingIncludeHeader:"Include",ColumnMappingSelectColumnPlaceholder:"Select column",ColumnMapping_EmptyLinkedService_ErrorMessage:'The linked service of dataset "{0}" is missing. Please select a linked service in the connection tab of the dataset.',ColumnMapping_IncompatibleType_ErrorMessage:"Incompatible column types",ColumnMapping_NotSupportType_ErrorMessage:"Type '{0}' is not supported",ColumnMapping_IncompatibleType_Validation_ErrorMessage:"Incompatible column types in the mapping",ColumnMapping_SourceColumnDuplicated_Validation_ErrorMessage:"Column '{0}' defined in the mapping is duplicated",ColumnMapping_AlternateKeyDuplicated_Validation_ErrorMessage:"Alternate key '{0}' defined in the mapping is duplicated",ColumnMapping_SinkColumnNotMapped_Validation_ErrorMessage:"Column '{0}' defined in the sink schema is not mapped in the mapping",ColumnMapping_ColumnNotFoundInSourceSchema_Validation_ErrorMessage:"The mapping is out of synchronization, this might be due to the fact that you modified the source schema. Please re-import schemas.",ColumnMapping_ColumnArrayNotSupport_Validation_ErrorMessage:'JSON path "{0}" is not correct. Array accessor like [0] is not supported in the schema mapping sink.',ColumnMapping_ColumnComplexNotSupport_Validation_ErrorMessage:"Complex JSON path is not supported in the sink.",ColumnMapping_ColumnInvalidPath_Validation_ErrorMessage:"JSON path {0} is invalid.",Git_Command:"Git command",Pipeline_Editor:"Pipeline editor",Dataflow_Editor:"Dataflow editor",Resource_list:"Resource list",DiscardAllChanges:"Discard all changes",DiscardChanges:"Discard changes",PublishLabel:"Publish",PublishBranch:"Publish branch",PublishBranchDescription:"The Publish branch is the branch in your repository where publishing related ARM templates are stored and updated.",Disconnect:"Disconnect",DisconnectTooltip:"Remove the associated repository. It does not delete anything from the repository",Disconnecting:"Disconnecting",Disconnect_Title:"Disconnect?",OverwriteLiveModeTitle:"Preview overwrite live mode?",OverwriteLiveModeBanner:"This will overwrite all published resources with the JSON value in your repository",OverwriteLiveModeBannerWithDelete:"This will overwrite all published resources and delete any resources not in git",OverwriteLiveModeConfirmMessage:"Overwrites your entire {0} using the git repository collaboration branch. It will overwrite everything in your {0} by publishing each resource in git. Deployment time may vary, depending on the size and number of resources. Are you sure you want to continue?",OverwriteLiveModeConfirmMessageWithDelete:"This will overwrite everything in your {0} by publishing each resource in the git repository and deleting any resources that do not exist in the git repository. Deployment time may vary, depending on the size and number of resources. Are you sure you want to continue?",OverwriteLiveModeWithDeleteWarning:"{0} resources not in found in your collaboration branch will be deleted. This action is not reversible.",OutdatedLocalWorkingBranchWarningTitle:"Remote branch change has been detected!",OutdatedLocalWorkingBranchWarning:"Refresh the {0} to reload resources from Git before making changes.",ResetGitRepository:"Overwrite live mode",ResetGitRepositoryTooltip:"Overwrite the code from your collaboration branch into the live mode. It will consider the code in your repository as the source of truth",ResetGitRepositoryTitle:"Overwrite {0} live mode?",ResetGitRepositoryOk:"Preview overwrite",ResetGitRepositoryDelete:"Preview overwrite and delete",ResetGitRepositoryConfirm:"Preview Changes",EditGitRepository:"Edit Git repository",RepositorySettingsTitle:"Repository settings",RepositorySettings:"Configure a repository",RepositorySettingsUpdated:"Repository settings has been successfully updated",SetupCodeRepositoryDescription:"Azure Data Factory allows you to configure a Git repository with either Azure DevOps or GitHub. Git is a version control system that allows for easier change tracking and collaboration.",AddKey:"Add key",NoConfiguredCustomerManagedKey:"No customer managed key configured",CustomerManagedKey:"Customer managed key",CustomerManagedKeyNotAvailable:"Customer managed key is currently not available in your Azure Data Factory.",CustomerManagedKeySettings:"Customer managed key settings",CustomerManagedKeyDescription:"A customer managed key allows you to encrypt your Azure Data Factory using your own key.",KeyVaultKeyUrl:"Azure Key Vault key URL",KeyIdentifierUrl:"https://baseUrl/keys/keyName (and optionally /keyVersion)",CICDMessage:"Using a continuous integration and delivery (CI/CD) pipeline enables teams to collaborate and track changes using source control within a repository.",CICDBestPractices:"CI/CD best practices",PublishBranchTooltip:"Publish branch is not editable",CollabBranchTooltip:"This is your collaboration branch",EditRepoConfirmMessage:"Updating these settings could cause problems the next time you publish and/or affect your CI/CD configurations if any, do you want to proceed?",UpdateRepositorySettings:"Update repository settings",RemoveGit:"Remove Git",RemoveGitWarningText:"This will remove the Git repository associated with your Data Factory. Make sure to Publish all the pending changes to Data Factory service before removing the Git associated with your Data Factory to avoid losing any changes.",RemoveGitFromWorkspaceWarningTemp:"Disconnecting from the repository does not delete anything. All published resources remain in the {0}. These changes affect all users.",RemoveGitEnterFactoryName:"Enter your {0} name to disconnect from repository",RemoveGitEnterFactoryNameDescription:"Enter your {0} name below and click confirm to remove Git configuration.",Repository:"Repository",Branch:"Branch",DisassociateRepository:"Disconnect from repository",DisassociateRepositoryFailed:"Failed to disassociate from repository",ImportBranchExistingResourcesWarning:"You are attempting to import resources into a repository branch that already contains resources. We recommend that you only import resources into an empty branch.",TenantLabel:"Azure Active Directory",VisualStudioAccountLabel:"Azure DevOps Account",VisualStudioOrgLabel:"Azure DevOps Organization",DevOpsOrganizationName:"Azure DevOps organization name",ProjectNameLabel:"Project name",RepoNameLabel:"Repository name",RepoLinkLabel:"Git repository link",AzDevOpsRepoLinkLabel:"Azure Devops link",AzDevOpsProjectMissingInLink:"Project name missing in repository link. Please check that the link is valid.",AzDevOpsProjectNotFoundError:"Verify that your Azure DevOps account is connected to the AAD account, the Azure DevOps repository is in the {0} tenant, and that your current ADF user account has been added to the Azure DevOps organization.",CollaborationBranchNotFoundError:"Collaboration branch is not found in the Git repository.",GitAccountNameInvalid:"Incorrect account name for the given tenant.",GitRepoUnknownError:"Got error while connecting to git, please check the entered information.",CheckingGitInformation:"Checking information",BadProjectNameError:"Incorrect project name.",BadBranchNameError:"Collaboration branch name not provided.",BadRepoNameError:"Repository not found.",SavingGitInformation:"Saving information",CreatingGitBranch:"Creating",CreateNewGitBranch:"New branch [Alt+N]",BranchLabel:"{0} branch",CreateNewPR:"Create pull request [Alt+P]",GitBranchExists:"Branch already exists",Gitbranchname:"Branch name",GitBranchDropdown:"Git Branch Switcher Dropdown",TenantDescription:"Select the Azure Active Directory (Azure AD) account to use when signing in into the repository.",AccountNameDescription:"Displays the DevOps account name used for the repository.",ProjectNameDescription:"Displays the project name to which you are connecting.",GitRepoNameDescription:"Displays the name of the repository to which you are connecting.",GitImportBranch:"Import resource into this branch",GitImportSelection:"Select branch",GitImportBranchTooltip:"Use your collaboration branch (generally \u2018master\u2019), create new branch or use any existing branch in your repository",GitImportBranchExists:"Import branch you are trying to create already exists",GitRepoCollabBranchDescription:"Select the branch name where you will collaborate with others and from which you will publish.",GitCollaborationBranch:"Collaboration branch",GitNewCollaborationBranch:"Create a new branch",GitCreateBranchErrorMessage:"Failed to create branch '{0}' for repository '{1}'. Reason: {2}",GitCreateNewBranchErrorMessage:"Failed to create branch '{0}' based on '{1}'. Reason: {2}",GitWorkingBranch:"Working branch",Settings:"Settings",WorkspaceSettings:"Workspace settings",GitConfigTooltip:"Git configuration settings must be modified in your collaboration branch",GitOverwriteLiveModeTooltip:"Overwrite live mode must be done from your collaboration branch",GitImportVNetResourcePanelTitle:"Import managed Virtual Network resources",GitImportVNetResourcePanelDescription:"To enable the managed Virtual Network, please import the generated VNet resources into your git repository. This is required to use a managed Virtual Network.",GitImportVNetResourceBannerMessage:"Git repository does not have managed Virtual Network resources.",GitImportVNetResourceBannerLink:"Import VNet resources",GitConfigurationDescription:"Connect your workspace with your Git repository just within few clicks. To learn more about best practices about CI/CD please view document here.",ManagementHub_Git_Description_A365:"Git repository information associated with your Synapse workspace.",SelectGitWorkingBranch:"Select working branch",BranchNotAvailable:"Previously used working branch '{0}' is not available.",SetWorkingBranch:"Please set the working branch.",SetWorkingBranchTitle:"Set working branch",GitRootFolder:"Root folder",rootFolderDesc:'Only "Files" root folder is supported for Lakehouse source.',storeTypeDesc:'"Workspace" data store type is not supported for Microsoft 365 source.',versionAsOfDesc:"Query an older snapshot by version",timestampAsOfDesc:"Query an older snapshot by timestamp",GitLastCommitId:"Last published commit",UpdatePublishBranch:"Update publish_config.json",CreatePublishBranch:"Create publish_config.json",GitRootFolderDescription:"Displays the name of the folder to the location of your Azure Data Factory JSON resources are imported.",EnterGitInformationDescription:"Specify the settings that you want to use when connecting to your repository.",EnterGitInformationDescriptionA365:"Connect your workspace with your Git repository just within few click.",AuthenticationErrorDescription:"Authentication error - you do not have access to provided Azure DevOps account.",TenantChangeAuthenticationErrorDescription:"Authentication error - you do not have permissions to set a different tenant. Administrator permissions for the Azure subscription used by the factory is required for the operation.",GitHubRepoConfAuthenticationErrorDescription:"Authentication error - you do not have permissions to associate GitHub repository. Administrator permissions for the Azure subscription used by the factory is required for the operation.",RepositoryTypeLabel:"Repository type",GitHubLabel:"GitHub",Git:"Git",GitSelectRepository:"Select repository",GitUseRepoLink:"Use repository link",GitRepoConnected:"Repo Connected",GitSuccessfullyConnected:"Repository has been successfully connected",GitBaseOn:"Base on",SourceControl:"Source control",VSTSGitLabel:"Azure DevOps Git",VSTSNotSupportedGovtCloud:"Azure DevOps Git is not supported in government clouds.",GitHubSupportedInGAText:"GitHub repositories are supported only in GA version factories or higher!",ConflictsLabel:"{0} Conflicts",ChangeUpdated:"(Edited)",ChangeNew:"(New)",ChangeDeleted:"(Deleted)",ChangeRenamed:"(Renamed)",ChangeRenamedEdit:"(Edited, Renamed)",CollaborationLabel:"Collaboration",FileExplorerMoveupLabel:"Navigate back",NavigateBackToLabel:"Navigate back to",FailedToUpdateData:"Failed to update data",GitUnauthMessage:"Invalid Git configuration. You need to gain access to the repository before you can publish any changes. Details: {0}",GitNoAccessToRepo:"You do not have access to the repository.",PublishLinkedServiceForDebug:"If this entity is a linked service, please ensure it has been published before using in debug runs.",PublishLinkedServiceErrorForNestedLinkedService:"This linked service you are trying to create contains secrets and has to be published immediately, but the referenced entity above has to be published first.",PublishLinkedServiceErrorForDeletedLinkedService:"This linked service does not seem to exist anymore, please delete it and create a new one or try again later.",SelectiveDeploymentEditSettingsTitle:"Edit selective deployment settings",SelectiveDeploymentAddRule:"Add exclusion rule",SelectiveDeploymentExclusionType:"Exclusion type",SelectiveDeploymentExcluded:"Excluded",SuccessfullyUpdatedSelectiveDeploymentSettings:"Successfully updated selective deployment settings",FailedtoUpdateSelectiveDeploymentSettings:"Failed to update selective deployment settings.",SelectiveDeployment:"Selective deployment",SelectiveDeploymentDescription:"Control which resources are excluded in the ARM templates generated by ADF.",SelectiveDeploymentSidenavDescription:"Configure rules to exclude resources from the ARM templates generated by ADF. Resources can be excluded by their type, name, and folder, and will not appear in the selectiveTemplates folder.",PendingChangesHeader:"Pending changes",GatheringChangesCollaboration:"Gathering your new changes from collaboration branch",SuccessfulPublishing:"Successfully published from collaboration branch",NothingToPublish:"No new changes to publish from collaboration branch",ErrorWhilePublishing:"Got error while publishing from collaboration branch",PublishCircularDependenciesError:"Circular dependencies found, please review the following dependency path: {0}",BranchCreationPermissionForPublishing:"Unable to create '{0}' as the publish branch. Please check if you have branch creation permissions in Git. You can alternatively choose an existing Git branch to be used instead, refer this link for details: https://docs.microsoft.com/azure/data-factory/author-visually#configure-publishing-settings",GatheringChanges:"Gathering your new changes",GatheringChangesNotGoIntoGit:"Gathering your changes that won't be saved into git",AdfPublishing:"Publishing new changes",AdfSuccessfulPublishing:"Successfully published",AdfNothingToPublish:"No new changes to publish",AdfErrorWhilePublishing:"Got error while publishing",SelectRepositoryType:"Select the repository type that you want to use to store your artifacts.",SelectRepositoryTypeWorkSpace:"Select the repository type that you want to use to store your artifacts for this Synapse Analytics workspace.",CreateFirstPipeline:"Create pipeline",UploadMappings:"Upload Mappings",CreateDataFlow:"Create data flow",ExploreTemplate:"Create pipeline from template",TemplateScreenshotDescription:"The screenshot of the template",AutoFillParameter:"Auto-fill parameters",NoLinkedServiceSelectedError:"No linked service selected",SSISVnetWithUDRIPRangeUpdateWaring:'If you\u2019ve configured UDRs between Azure Batch and your SSIS IR inside a VNet, please update them to add new IP ranges as recommended <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network#route">here</a>.',ThirdPartyCookiesBlockedWarning:'Your browser is blocking third party cookies, this can happen if you are in incognito mode or have an ad blocker enabled. Please go to your browser privacy settings to verify that it is not blocking third party cookies. <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/data-factory-ux-troubleshoot-guide#third-party-cookies-blocked">Learn more</a>',AzureDataFactory:"Azure Data Factory",OverviewTitle:"Let's get started",OverviewSurveyTitle:"Thank you so much for using Data Factory! We\u2019re always looking for ways to make your experience better with ADF. Would you mind spending a few minutes to provide your feedback?",OverviewSurveyLink:"Survey link",VideoTilesSectionTitle:"Videos",OverviewVideoViewAll:"View all videos",OverviewTutorialViewAll:"View all tutorials",OverviewTemplateViewAll:"View all templates",OverviewScrollLeft:"Scroll left",OverviewScrollRight:"Scroll right",OverviewVideoDatabricks:"Ingest, prepare, and transform using Azure Databricks and Data Factory",OverviewVideoIterativeDevelopment:"Iterative development and debugging with Azure Data Factory",OverviewVideoContinuousIntegration:"Continuous integration and deployment using Azure Data Factory",OverviewVideoADFGit:"Azure Data Factory visual tools now integrated with GitHub",OverviewVideoVisuallyBuildPipelines:"Visually build pipelines for Azure Data Factory",OverviewVideoDataQuality:"ADF Data Quality: Dedupe your data and find NULLs",OverviewVideoMonitorLog:"Monitor Data Factory pipelines using Azure Monitor and Log Analytics",OverviewVideoEventBased:"Event-based data integration with Azure Data Factory",OverviewVideoEnhancedProductivity:"Enhanced productivity using Azure Data Factory visual tools",OverviewVideoHybridDataMovement:"Hybrid data movement across multiple Azure Data Factories",OverviewVideoIntegrateWithDatabricks:"Azure Data Factory new features and integration with Azure Databricks",OverviewVideoRunAzureFunctions:"Run Azure Functions from Azure Data Factory pipelines",OverviewVideoProactiveAlerts:"Monitor your Azure Data Factory pipelines proactively with alerts",OverviewVideoCodeFree:"Code-free data transformation at scale using Azure Data Factory",OverviewVideoQuicklyBuild:"Quickly build data integration pipelines using templates",OverviewVideoEnhancedMonitoring:"Enhanced monitoring capabilities and tags/annotations",OverviewVideoRerunActivities:"Rerun activities inside your Azure Data Factory pipelines",OverviewVideoCodeFreeDW:"Code-free modern data warehouse using Azure Synapse Analytics",OverviewVideoBuildCollaboratively:"Build ETL pipelines collaboratively using Git integration",OverviewVideoCreateDependent:"Create dependent pipelines in your Azure Data Factory",OverviewVideoTransformData:"Transform Data in ADF using Data Flows",OverviewVideoGanttViews:"Monitor Azure Data Factory pipelines visually using Gantt views",OverviewVideoSCDPatterns:"Generic SCD Type II patterns with ADF",OverviewVideoWrangling:"Prepare data using Power Query",AboutRegistrationTitle:"Data registration",AboutDiscoveryTitle:"Data discovery",AboutExplorationTitle:"Data exploration",AboutRegistrationDesc:"Expand your governance and cataloging scope by registering new data.",AboutDiscoveryDesc:"Find relevant data through the global search bar based on keywords and advanced filtering.",AboutExplorationDesc:"Connect data to your {0} and analyze those data with {1}.",BabylonNoAccountTitle:"No Purview account connected",FailedToGetResourceID:"Failed to get resource ID",BabylonNoAccountDesc:"Try connecting to a Purview account",FeaturedTutorialsSectionTitle:"Tutorials",CommonQuestionsIntroduction:"Introduction to Data Factory",CommonQuestionsLiftShift:"Lift & shift SSIS packages",FeaturedTutorialsCopyData:"Copy data in cloud",FeaturedTutorialsDatabricks:"Transform data with Azure Databricks",FeaturedTutorialsCopyOnPrem:"Copy on-prem data to cloud",FeaturedTutorialsTransformData:"Transform data using Mapping data flows",FeaturedTutorialsControlFlow:"Control flow",FeaturedTutorialsBulkCopy:"Bulk copy (multiple tables)",FeaturedTutorialsIncrementalCopy:"Incremental copy",FeaturedTutorialsTransformHive:"Transform data using Hive on HDInsight in Azure Virtual Network",ConfigureRepository:"Set up code repository",Configure:"Configure",NoConfiguredGit:"No Git repository configured",ConnectToGitDescription:"Connect to a repository for source control and collaboration for work on your {0} pipelines.",CreateIR:"Configure SSIS Integration Runtime",versionNotSupportedForEditing:"Upgrade this factory to newer version to support editing",OverviewVideo:"Overview of Azure Data Factory",ReleaseNotesLabel:"Release notes/updates",ReleaseNotes:"Release notes",DownloadCenter:"Download center",NoReleaseNotes:"There are no release notes available right now",SavingToTemplate:"Saving template",TemplateSaved:"Template saved",TemplateServicesUsed:"Services used",TemplateResetAllFilter:"Reset filters",Template_NoSearchResult:"No templates to display. Your filters maybe too tightly defined. You can save your pipelines as templates and view them here. Saving pipelines as templates require you to enable Git integration with your data factory. You can also try changing your filters if you don't see what you're looking for.",Template_SearchText:"Search templates",ADFHomeIngestTitle:"Ingest",ADFHomeIngestDescription:"Copy data at scale once or on a schedule.",ADFHomeOrchestrateTitle:"Orchestrate",ADFHomeOrchestrateDescription:"Code-free data pipelines.",ADFHomeTransformTitle:"Transform data",ADFHomeTransformDescription:"Transform your data using data flows.",ADFHomeMonitorDescription:"Visually monitor pipeline and activity runs.",ADFHomeSSISTitle:"Configure SSIS",ADFHomeSSISDescription:"Manage & run your SSIS packages in the cloud.",MoreIsPossible:"More is possible with Azure Data Factory",ADFMonthlyWhatsNew:"What's New Monthly Update",ADFOneClickSetUp:"One-click to Try Azure Data Factory",RecentResourcesTitle:"Recent resources",RemoveFromFavorites:"Remove resource from favorites",AddToFavorites:"Add resource to favorites",RecentRunsTitle:"Recent runs - {0}",RecentRuns:"Recent runs",LastOpenedByYou:"Last opened by you",ShowMore:"Show more",ShowAll:"Show all",ShowLess:"Show less",NoRecentResources:"Your recently opened resources will show up here.",UsefulLinks:"Useful links",ProductOverview:"Product overview",StackOverflow:"Stack Overflow",Youtube:"YouTube",Twitter:"Twitter",QuestionsAndAnswers:"Questions & answers",GiveFeedback:"Give feedback",CommunityContent:"Community content",Training:"Training",FollowADF:"Follow Azure Data Factory by",Blog:"Blog",ADFHomeTutorialsText:"Learn how to perform common data engineering tasks in Azure Data Factory by following along to our online tutorials.",ADFHomeVideosText:"Browse an extensive library of step-by-step walk-through videos of Azure Data Factory from introductions to advanced topics.",ADFHomeCommunityContentText:"The Azure Data Factory community is a leading resource for learning and best practices. Browse a collection of community-driven ADF content.",CommunityCenterActionText:"Visit the community center",FeatureShowcase:"Feature showcase",FeatureShowcasePurviewTitle:"Integrate Microsoft Purview with Azure Data Factory",FeatureShowcasePurviewDescription:"Connect a data factory to Microsoft Purview to discover trusted and accurate data across your hybrid environment.",FeatureShowcasePowerQueryTitle:"Power Query activity for Data Wrangling",FeatureShowcasePowerQueryDescription:"Wrangling in ADF empowers users to build code-free data prep and wrangling at cloud scale using the familiar Power Query data-first interface, natively embedded into ADF.",FeatureShowcaseAutomatedPublishTitle:"Continuous integration and deployment improvement with Automated Publish capability",FeatureShowcaseAutomatedPublishDescription:'The "Automated publish" improvement takes the validate all and export Azure Resource Manager (ARM) template functionality from the ADF UI and makes the logic consumable via a publicly available npm package. This allows you to programmatically trigger these actions.',FeatureShowcaseCDMandDeltaLakeTitle:"CDM and Delta Lake Connectors available in data flows",FeatureShowcaseCDMandDeltaLakeDescription:"With data flows, you can build powerful ETL processes using CDM formats and then also generate updated manifest files that point to your new, transformed data using CDM as a sink. With ADF data flows, you can read from Delta Lake folders, transform data, and even update, upsert, insert, delete, and generate new Delta Lake folders using the Delta Lake sink format. ",FeatureShowcaseSAPTemplateAreaDescription:"Azure Data Factory has a comprehensive knowledge center with information on SAP data integration scenarios. This includes SAP connectors, templates, and more.",Umask:"Umask",UmaskDescription:"Set the read, write and execute permissions for newly created files",FilePrePostCommands:"Pre/post commands",FilePrePostCommandsDescription:'Set commands to execute before or after writing to the ADLS sink. Supported commands are cp, mv, rm, mkdir. By default, folders are created under user/root. Refer to the top level container with /. <a href="https://aka.ms/dataflowsinklink" target="_blank">Click to learn more.</a>',FileCommandsDescription:"mkdir, mv, cp, rm",PreCommandLabel:"File pre command",PostCommandLabel:"File post command",LearnMoreVideo:"Overview video",LearnMorePricing:"Pricing",ResourcesStatus:"Data Factory status",ResourcesRegions:"Region availability",ResourcesSupport:"Support options",ResourcesUserFeedback:"User feedback",ResourcesMSDN:"MSDN forum",ServiceUpdates:"Service updates",ServiceUpdatesManagedSSIS:"Lift your SQL Server Integration Services packages to Azure with Azure Data Factory",ServiceUpdatesNewCapabilities:"Azure Data Factory: new capabilities in Public Preview",ServiceUpdatesToolsEnabled:"Azure Data Factory new capabilities are now generally available",Tour_NotAvailableMessage:"Sorry! No guided tour is available for this section.",Tour_Home_SetUpRepository:"Click to set up the code behind repository and start building your pipelines.",Tour_Home_NewPipeline_Title:"Create a pipeline",Tour_Home_NewPipeline:"Click to create your first pipeline.",Tour_Home_IngestData_Title:"Open copy wizard",Tour_Home_IngestData:"Click to ingest data",Tour_Home_CreateIR:"Click here to create Azure-SSIS Integration Runtime to lift & shift SSIS packages",Tour_Home_HelpInformation_Title:"Get started with samples",Tour_Home_HelpInformation:"Launch getting started samples and guided tour.",Tour_Home_KnolwedgeCenter_HelpInformation:"Launch knowledge center and guided tour.",Tour_Author_SwitchMode:"Work directly with this service or configure code repository for source control, collaboration, versioning",Tour_Author_NewResource_Title:"Add a resource",Tour_Author_NewResource:"Click '+' to build your pipeline(s) or a dataset(s)",Tour_Author_NewResource_DataView_Title:"Add data source",Tour_Author_NewResource_DataView:"Create new databases or datasets.",Tour_Author_NewResource_AnalyzeView:"Create new code artifacts or PowerBI reports to analyze your data.",Tour_Author_NewResource_OrchestrateView:"Click to create new data pipelines.",Tour_OE_Dataset:"Create and manage the datasets here.",Tour_OE_Pipeline:"Edit, monitor, or manage a pipeline by clicking on the ellipsis.",Tour_LeftNavigation_Home:"Recent code artifacts and shortcuts for common tasks, videos, and documentation.",Tour_LeftNavigation_Data:"Browse databases, datasets, and storage accounts.",Tour_LeftNavigation_Analyze:"Create, browse, and manage code artifacts.",Tour_LeftNavigation_Analyze_SubItem:"Hover over an item in the list and click on the ellipses to access actions for the code artifact.",Tour_LeftNavigation_Orchestrate:"Create, browse, and manage data pipelines.",Tour_LeftNavigation_Monitor:"See operations, requests, status, and create alerts for your analytics resources and code artifacts.",Tour_LeftNavigation_Management:"Create new analytics pools, manage workspace access, and connect to external sources and services (such as PowerBI).",Tour_Author_NewConnection:"Click 'Connections' to create your linked service connection(s) and integration runtime(s)",Tour_Authoring_Trigger_NewTrigger_Title:"Create a trigger",Tour_Authoring_Trigger_NewTrigger:"Create trigger(s) to kick off your pipeline(s). Hover on a row to view available actions. Click 'Start' icon to activate triggers after associating the trigger(s) with pipeline(s).",Tour_Connection_IR_Title:"Create integration runtime",Tour_Connection_IR:"Click 'Integration runtimes' to create Azure or Self-Hosted Integration Runtime",Tour_Connection_LinkedService_Title:"Create linked service",Tour_Connection_LinkedService:"Click 'New' to create linked service connection(s)",Tour_Pipeline_DragDropActivity:"Drag and drop Activities on the pipeline canvas",Tour_Pipeline_DragDropExisting:"Drag and drop existing data flows on pipeline authoring canvas to create data flow activities",Tour_Pipeline_Configure:"Configure your Activity settings",Tour_Pipeline_Validate_Testrun_Title:"Validate changes",Tour_Pipeline_Validate_Testrun:"Validate your pipelines, perform test runs for iterative debugging",Tour_Pipeline_Trigger_Title:"Add a trigger",Tour_Pipeline_Trigger:"Trigger pipelines on-demand or on a schedule",Tour_Pipeline_Sync_Publish:"Sync & publish your changes to the {0} service",Tour_Dataset_Configure_Title:"Configure dataset",Tour_Dataset_Configure:"Configure your dataset settings",Tour_Dataset_Save_Title:"Save your changes",Tour_Dataset_Save:"Click 'Save' to save your authored dataset",Tour_Pipelinerun_FirstTime:"Visually monitor pipeline executions using simple list view.",Tour_Pipelinerun_Datetime:"Click to filter pipeline runs by date and time. Use options to quickly view your runs for 'Last 24 hours', 'Last 7 days' or 'Last 30 days'.",Tour_Pipelinerun_Timezone:"Click to change to a different timezone. Search by UTC or timezone names.",Tour_Pipelinerun_Columns:"Resize columns or sort by run start time.",Tour_Pipelinerun_ViewRun:"Click the pipeline name to view all the activity runs of the selected pipeline.",Tour_Pipelinerun_Filters:"Click to add, remove, or edit pipeline run filters. Available filter options are pipeline names and annotations.",Tour_Activityrun_Title:"Monitor activity runs",Tour_Activityrun_Content:"Visually monitor activity runs corresponding to each pipeline run using simple list view.",Tour_Activityrun_StatusFilter_Title:"Filter by status",Tour_Activityrun_StatusFilter:"Click on quick 'Status' filters to filter activity runs by execution status.",Tour_Activityrun_InputOutput_Title:"See activity input and output",Tour_Activityrun_InputOutput:"Click on 'Input' or 'Output' icons in each activity run row to view the corresponding activity run inputs and outputs.",Tour_Activityrun_AllPipeline_Title:"Navigate to pipeline run view",Tour_Activityrun_AllPipeline:"Click 'Pipelines' to navigate back to the pipeline runs list view.",Tour_SelectDataFactory_SelectSubscription_Title:"Select subscription",Tour_SelectDataFactory_SelectSubscription:"Select your Azure subscription and {0}",Tour_PropertiesSidebar_Open:"You can toggle the properties pane from this icon.",NoResult:"No result",NoRecordsToShow:"There are no records to show you right now.",VerifyFilters:"If you expected to see results, try updating or clearing your filters.",LoadingPipelineRuns:"Loading pipeline runs",LoadingGroupRuns:"Loading group runs",LocationType:"Location type",OperationType:"Operation Type",LoadingActivityRuns:"Loading activity runs",LoadingConsumptionReport:"Loading consumption report",LoadingForLongTime:"Still loading hang tight",LoadingTriggerRuns:"Loading trigger runs",LoadingDataflowRun:"Loading flow run",LoadingIntegrationRuntimes:"Loading integration runtimes",LoadingIntegrationRuntime:"Loading integration runtime",LoadingIntegrationRuntimeNodes:"Loading integration runtime nodes",LoadingPipeline:"Loading pipeline",LoadingSparkApplications:"Loading spark applications",LoadingSqlRequests:"Loading SQL requests",LoadingLinkConnections:"Loading link connections",LoadingAdfDataMapper:"Loading CDCs",AdfDatamapper_Monitoring_ConfirmStartStop:"Are you sure you want to {0} the CDC {1}?",AdfDatamapper_Monitoring_ConfirmDelete:"Are you sure you want to delete the CDC {0}? Click Ok and then publish the changes for deletion.",AdfDatamapper_Monitoring_DeleteWarning:"CDC can not be deleted while it is running. Please stop the CDC before deletion.",LoadingKqlRequests:"Loading KQL requests",LoadingSqlPools:"Loading SQL pools",LoadingSparkPools:"Loading Spark pools",FailedToLoadingPipeline:"Failed to load pipeline, reason: {0}",SapExplorer:"SAP explorer",TableExplorer:"Table explorer",LoadingResources:"Loading resources",ExportingTemplate:"Exporting template",ExportingTemplateInProgress:"Exporting template...",ProcessingResources:"Processing {0}",Example:"Example: {0}",LoadingLinkTables:"Loading link tables",LoadingAdfDataMapperDetail:"Loading CDC detail",StartingAdfDataMapper:"Starting CDC",StoppingAdfDataMapper:"Stopping CDC",LinkTableId:"Link table ID",Step_Index:"Step index",Error_ID:"Error ID",Dataflow_ConnectionMode:"Connection mode",Dataflow_SourceFormat:"Source format",Dataflow_SinkFormat:"Sink format",Dataflow_DatasetFileNameOptionLabel:"File name option",Dataflow_DatasetFileNameOptionPatternLabel:"Pattern",Dataflow_DatasetFileNameOptionPatternPlaceholder:"fileName[n]",Dataflow_DatasetFileNameOptionPerPartitionLabel:"Per partition",Dataflow_DatasetFileNameOptionSingleFileLabel:"Output to single file",Dataflow_DatasetFileNameOptionSingleFilePlaceholder:"Enter file name here",Dataflow_DatasetFileNameOptionSingleFileDescription:"Enter a file name which will contain all output content.",Dataflow_DatasetSingleFileInfo:'Optimization is disabled when file name option is set to "Output to single File". Using other file name options will enable optimization settings.',Dataflow_DatasetFileNameOptionPatternFileNames:"File names",Dataflow_DatasetSQLPreScriptLabel:"Pre SQL scripts",Dataflow_DatasetSQLPostScriptLabel:"Post SQL scripts",Dataflow_DatasetSQLPreScriptSinkDescription:"Enter multi-line SQL scripts that will execute before data is written to your Sink database",Dataflow_DatasetSQLPostScriptSinkDescription:"Enter multi-line SQL scripts that will execute after data is written to your Sink database",Dataflow_DatasetSQLPreScriptSourceDescription:"Enter multi-line SQL scripts that will execute before data is written to your source database",Dataflow_LinkedService_NotSupported:"Dataset is using '{0}' linked service type, which is not supported in data flow.",Dataflow_LinkedService_Source_NotSupported:"Dataset is using '{0}' linked service type which is not supported as source in data flow.",Dataflow_LinkedService_Sink_NotSupported:"Dataset is using '{0}' linked service type which is not supported as sink in data flow.",Dataflow_LinkedService_SQLEncryption_NotSupported:"Dataset is using '{0}' linked service type with SQL encryption, which is not supported in data flow.",Dataflow_Sink_LinkedService_SQLEncryption_NotSupported:"Dataset is using '{0}' linked service type with SQL encryption, which is not supported in data flow sink.",Dataflow_LinkedService_SPN_MI_AccountKindError:'Azure Blob linked service with account kind as empty or "Storage" (general purpose v1) \u200b\u200b\u200b\u200b\u200b\u200bdoes not support managed identity or service principal authentication in Data Flow. Specify the account kind, or choose a different authentication, or upgrade your storage account.',Dataflow_LinkedService_SPN_MI_Cred_AccountKindError:'Azure Blob linked service with account kind as empty or "Storage" (general purpose v1) \u200b\u200b\u200b\u200b\u200b\u200bdoes not support managed identity, service principal or credential authentication in Data Flow. Specify the account kind, or choose a different authentication, or upgrade your storage account.',Dataflow_LinkedService_AuthTypeError:"{0} does not support SAS, MSI, or Service principal authentication in data flow.",Dataflow_LinkedService_New_AuthTypeError:"{0} does not support SAS authentication in data flow.",Dataflow_LinkedService_WinAuthError:"{0} does not support windows authentication in data flow.",Dataflow_LinkedService_MSIAuthTypeError:"{0} does not support MSI authentication in data flow.",Dataflow_LinkedService_MSIStagingAuthTypeError:"Staging does not currently support ADLS Gen2 with MSI authentication.",Dataflow_LinkedService_IRError:"Linked service with Self-hosted Integration runtime is not supported in data flow.",Dataflow_Oracle_IRError:'Only "Self-hosted" Integration runtime is supported for Oracle in data flow.',Dataflow_SqlServer_IRError:'Linked service with Self-hosted Integration runtime is not supported in data flow. Please utilize the Azure IR with managed vnet using this <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/tutorial-managed-virtual-network-on-premise-sql-server">tutorial</a>',Dataflow_SqlServer_AuthTypeError:"Sql server linked service with windows authentication is not supported in data flow",Dataflow_LinkedService_IRVNetError:"Linked service with AzureVNet Integration runtime is not supported in data flow.",Dataflow_Dataset_TableNameRequired:"Table is required for data flow entity '{0}'",Dataflow_Cosmos_Auth_Error:'Only "account key" authentication is supported for Azure Cosmos DB (SQL API) in data flow.',Dataflow_Snowflake_OAuthError:"Snowflake linked service with OAuth authentication is not supported in data flow.",Anonymous_BlobStorage_ErrorTemplate:"Azure blob storage linked service with Anonymous authentication is not supported in {0}.",DataFlow_IR_Core_Options_4:"4 (+ 4 Driver cores)",DataFlow_IR_Core_Options_8:"8 (+ 4 Driver cores)",DataFlow_IR_Core_Options_16:"16 (+ 4 Driver cores)",DataFlow_IR_Core_Options_32:"32 (+ 4 Driver cores)",DataFlow_IR_Core_Options_64:"64 (+ 4 Driver cores)",DataFlow_IR_Core_Options_128:"128 (+ 4 Driver cores)",DataFlow_IR_Core_Options_256:"256 (+ 4 Driver cores)",DataFlow_IR_Core_Options_8_newversion:"8 (+ 8 Driver cores)",DataFlow_IR_Core_Options_16_newversion:"16 (+ 16 Driver cores)",DataFlow_IR_Core_Options_32_newversion:"32 (+ 16 Driver cores)",DataFlow_IR_Core_Options_64_newversion:"64 (+ 16 Driver cores)",DataFlow_IR_Core_Options_128_newversion:"128 (+ 16 Driver cores)",DataFlow_IR_Core_Options_256_newversion:"256 (+ 16 Driver cores)",Dataflow_Staging_Schema_Name:"Staging schema name",Dataflow_StagingLinkedService:"Staging linked service",DataFlow_StagingPath_PlaceHolder:"container/folder",Dataflow_StagingLinkedService_BlobSASError:"Staging account does not support SAS URI authentication in Data Flow Activity.",Dataflow_StagingLinkedService_BlobAuthTypeError:"Staging account does not support MSI or Service principal authentication in Data Flow Activity.",Dataflow_StagingLinkedService_Gen2AuthTypeError:"ADLS Gen2 staging account does not support service principal id + cert authentication in Data Flow Activity.",Dataflow_Snowflake_StagingLinkedService_BlobAuthTypeError:"The Snowflake connector only supports Azure blob storage with SAS URI authentication for the staging storage account.",Dataflow_StagingLinkedService_BlobAccountKindError:'Azure Blob linked service with account kind as empty or "Storage" (general purpose v1) \u200b\u200b\u200b\u200b\u200b\u200bdoes not support managed identity or service principal authentication in staging of Data Flow Activity. Specify the account kind, or choose a different authentication, or upgrade your storage account.',Dataflow_StagingLinkedService_AzureVNetError:"Staging account does not support a linked service with AzureVNet Integration runtime in Data Flow Activity.",Dataflow_StagingLinkedService_NotNeeded:"This data flow does not require a staging linked service because it is not using a Azure Synapse Analytics as a sink or source, or staging is not enabled. Please remove it.",NewDataflow_StagingLinkedService_NotNeeded:"This data flow does not require a staging linked service because it is not using a Azure Synapse Analytics or SAP ODP as a sink or source, or staging is not enabled. Please remove it.",DataFlow_No_AutoResolve_IntegrationRuntime_Error:"Only an Azure integration runtime with 'Auto Resolve' location can be used in Data Flow Activity.",DataFlow_IntegrationRuntime_NotSameRegion_Error:"Please make sure the Data flow Integration runtime is 'Auto Resolve' or in the same region as the Data factory if the Data flow contains Linked services that use self-hosted Integration runtime.",DataFlow_Invalid_Region_Error:"Mapping data flows are not available for {0} in the following regions: {1}. To use this feature, please create one in a supported region.",DataFlow_SAPCDC_StagingLinkedService_Error:"{0} with self-hosted integration runtime can not be used as staging linked service in SAP CDC Data Flow Activity.",DataFlow_Invalid_TraceLevel_Error:"This data flow has cache sink output selected. Logging level must be set to None.",Dataflow_StagingPath:"Staging storage folder",Dataflow_StagingLinkedServiceDescription:"For SQL DW, please specify a staging location for PolyBase.",DataFlow_MaxConcurrentConnections_MinError:"Value must be >= 1",DataFlow_RowUrl:"Column to store file name",DataFlow_RowUrl_Description:"Use this to create a new column name in your data stream that is the source file name and path.",DataFlow_ColumnData:"Column data",DataFlow_ColumnWithRowUrl:"Name file as column data",DataFlow_ColumnWithRowUrl_Description:"Use the value of a column in your data to specify a file path. This path starts at the dataset container root.",DataFlow_ColumnWithRowFolderUrl:"Name folder as column data",DataFlow_ColumnWithRowFolderUrl_Description:"Use the value of a column in your data to specify a folder path. This path starts at the dataset container root.",DataFlow_NumConnection_Label:"Max connection count",DataFlow_PartitionColumn_Label:"Partition column",DataFlow_PartitionColumn_Description:"Data will be partitioned based on the specified column values",DataFlow_PartitionRootPath:"Partition root path",DataFlow_PartitionRootPath_Description:"For file data that is partitioned, you can enter a partition root path in order to read partitioned folders as columns",DataFlow_PartitionPath:"Partition path",DataFlow_PartitionPathDesc:"Location where the data partition files will be written. By default {rootLocation}/{entityName}/{yyyy-MM-dd}.",DataFlow_EntityPath:"Entity path",DataFlow_RootLocationDesc:"The root location of the CDM data folder",DataFlow_ManifestFile:"Manifest file",DataFlow_Manifest:"Manifest",DataFlow_ModelJson:"Model.json",DataFlow_ModelJsonDescription:"Used by applications that are consuming CDM versions earlier than 1.0. This file name should be 'model.json'",DataFlow_ManifestJsonDescription:"Used by applications that are consuming CDM version 1.0 or later. This file name should look like '*.manifest.cdm.json'",DataFlow_ManifestDefault:"Manifest name (default)",DataFlow_ManifestDesc:"Specify the path and name of the manifest file within the root location. If left empty, the name 'default' will be used.",DataFlow_ManifestDesc2:"The name of the CDM manifest",DataFlow_ManifestDescWithManifestType:"Specify the name and type of the CDM manifest",DataFlow_CDM_LocalEntityDescription:"A local entity is owned by the manifest, which should point to the location of both the entity file and the data partitions.",DataFlow_CDM_CustomEntityDescription:"A custom entity is a logical entity model that is created by an organization. This entity should exist within your own linked service location.",DataFlow_CDM_StandardEntityDescription:"A standard entity is one that is created and centrally managed by the common CDM Github repository. This entity exists within the CDM Github repository.",DataFlow_EntityPathDesc:"The location of the entity starting at the specified folder path",DataFlow_Entity:"Entity",DataFlow_EntityDesc_Manifest:"The entity to read/write data for. For this field, {0}",DataFlow_CDM_LocalEntityExample:"specify the name of the entity within the manifest file. Eg. '{0}'",DataFlow_CDM_CustomEntityExample:"specify the path to the entity within an entity file, starting from the corpus folder location. Eg. '{0}'",DataFlow_CDM_StandardEntityExample:"specify the path to the entity within an entity file, starting from the schemaDocuments folder in the CDM Github repository. Eg. '{0}'",DataFlow_CDM_StandardEntityPathExample:"/core/applicationCommon/Account.cdm.json/Account",DataFlow_CDM_CustomEntityPathExample:"/corpusFolder/Common/Account.cdm.json/Account",DataFlow_EntityDesc_Model:"Name of the entity within the model.json to read/write data for. Eg. '{0}'",DataFlow_Corpus:"Corpus folder",DataFlow_CorpusDesc:"The folder path location of the CDM corpus directory. The corpus contains all the unresolved entities.",DataFlow_EntityFromCorpusDesc:"Use an entity reference from a CDM corpus, which contains unresolved entity files.",DataFlow_RepositoryDescription:"Specify which GitHub repository to use",DataFlow_BranchDescription:"The branch to use within the GitHub repository",DataFlow_FolderPathDescription:"The folder path location of your CDM folder. This folder contains the manifest and entity files",DataFlow_CDMSourceLinkedServiceContainerDescription:"A container within the source linked service",DataFlow_CDMSchemaLinkedServiceContainerDescription:"A container within the schema linked service",DataFlow_FaultToleranceLinkedServiceContainerDescription:"A container within the error row linked service",DataFlow_LowerUpperBound_Description:"In case of numeric column, optionally specify upper/lower bound to avoid auto range detection cost",DataFlow_DetectDataType:"Detect data type",DataFlow_DetectDataTypeInProgress:"Detecting data type",DataFlow_DetectDataTypeInProgressTitle:"Detecting",DataFlow_DetectDataTypeInProgressDesc:"Successfully started detecting the data types for {0} ({1}).",DataFlow_DetectDataTypeSuccessTitle:"Successfully detected",DataFlow_DetectDataTypeSuccessDesc:"Successfully detected the data types for {0} ({1}).",DataFlow_DetectDataTypeFailedDesc:"Failed to detect the data types for {0} ({1}).",DataFlow_ResetDataType:"Reset schema",DataFlow_OverrideDataTypes:"Overwrite schema",DataFlow_DisableOverrideDataTypes:"Disable overwrite schema",DataFlow_DefineDefaultFormat:"Define default format",DataFlow_OutputDefaultFormat:"Output format",DataFlow_DefaultFormat:"Default format",DataFlow_DefaultBooleanTrueFormat:"Boolean true",DataFlow_DefaultBooleanFalseFormat:"Boolean false",DataFlow_DefaultFormatOkDisabledTooltip:"Default boolean format must be provided for both true and false",DataFlow_ImportSchema_Description:"Configure the behavior of the schema import",DataFlow_ProjectionOptions_Description:"Configure your source projection",DataFlow_DefaultFormat_Description:"Define the default formats that will be used to identify the various data types within the dataset",DataFlow_PreferredIntegralType:"Numerical whole number",DataFlow_PreferredFractionalType:"Numerical fraction",DataFlow_DetectDataTypeStarted:"Detect data type started",DataFlow_DetectDataTypeFailed:"Detect data type failed",DataFlow_DetectDataTypeSucceeded:"Detect data type succeeded",DataFlow_ImportSchemaStarted:"Import schema started",DataFlow_ImportSchemaSucceeded:"Import schema succeeded",DataFlow_ImportSchemaFailed:"Import schema failed.",DataFlow_MoveFromDescription:"All source files under the provided path will be moved. The path starts at the source container root and can point to a folder or a file. Leave this option blank to move all source files.",DataFlow_MoveFromTextPlaceholder:"All source files",DataFlow_MoveToTextPlaceholder:"Specify destination",DataFlow_MoveToDescription:"Specify the folder path to which you would like to move your source files within the source container. The folder path starts at the source container root.",Dataset_ImportProjection_Label:"Import projection",Dataset_ImportProjection_LabelInProgress:"Importing projection",Dataflow_DatasetRootRequired:"{0} is required when dataset is referenced as source/sink in Dataflow.",Dataflow_RequireContainer:"Container needs to be specified",Dataflow_RequireFileSystem:"File system needs to be specified",Dataflow_RequireFolderPath:"Folder path needs to be specified",Dataflow_RequireSheetName:"Sheet name needs to be specified",Dataflow_RequireSheetIndex:"Sheet index needs to be specified",Dataflow_RequireWildcardPath:"At least one wildcard path needs to be specifed",DataFlow_Sink_NoSchema:"No schema defined. Please import your schema.",DataFlow_Sink_Date_Format_Description:"You may select your sink datetime format from the dropdown or enter a custom format",DataFlow_Sink_Timestamp_Format_Description:"You may select your sink timestamp format from the dropdown or enter a custom format",DataFlow_SparkGateway_Disabled:"In order to use the IR for data flows, it must not use managed VNet, must not have TTL, and the compute location must match the factory location.",DataFlow_Twilio_Phone_Description:"Phone number with country code. Eg.+17755425856",Trigger_EventGridNotRegisteredError:'Register Azure Event Grid resource provider to your subscription before creating an event trigger. Learn more <a target="_blank" href="https://docs.microsoft.com/azure/azure-resource-manager/management/resource-providers-and-types#azure-portal">here</a>',Trigger_EventGridResourceProviderAPIFailure:"Failed to get Event Grid rsource provider for the subscription.",Trigger_ActivateDescription:"Activate/Deactivate the trigger, it will take effect after you do the publish.",Trigger_StatusDescription:"Select to start or stop the trigger when you publish your updates.",Trigger_StatusStartDescription:"Select to start the trigger when you publish it.",Trigger_StatusStartLabel:"Start trigger",Trigger_StatusStartOption:"Start trigger on creation",Trigger_Pipeline:"Pipeline triggers",ScheduleLabel:"Schedule",Trigger_AdvancedRecurrenceOptions:"Advanced recurrence options",Trigger_TypeDescription:"Type of this trigger",Trigger_TypeLabel:"Trigger type",Trigger_TypeRequired:"Trigger type is required",Trigger_EndRequired:"Trigger end is required",Trigger_RangeStartRequired:"Rerun range start is required",Trigger_RangeEndRequired:"Rerun range end is required",Trigger_IntervalRequired:"Enter a value for the interval",Trigger_IntervalPositive:"Interval must be a positive number",Trigger_StartDateUTC:"Start Date (UTC)",Trigger_EndDateUTC:"End Date (UTC)",Trigger_StartDescription:"Choose the start time",Trigger_RecurrenceDescription:"Select the recurrence type",Trigger_TimeZoneDescription:"Select the Time Zone",Trigger_TimeZoneTooltip:"Some time zones observe daylight savings. Select 'Coordinated Universal Time (UTC)' to disregard auto adjustment.",Trigger_DaylightSavingsInfo:"This time zone observes daylight savings. Trigger will auto-adjust for one hour difference.",Trigger_NoEnd:"No End",Trigger_Occurences:"Occurences",Trigger_OnDate:"On Date",Trigger_EndOnUTC:"End On (UTC)",Trigger_EndOn:"End On",Trigger_EndOnLabel:"Specify an end date",Trigger_EndOnDescription:"Choose the end time",Trigger_OccurencesNumber:"Number of times",Trigger_Minutes:"Minute(s)",Trigger_Hours:"Hour(s)",Trigger_Days:"Day(s)",Trigger_Weeks:"Week(s)",Trigger_Months:"Month(s)",Trigger_AddTrigger:"Add triggers",Trigger_ChooseTrigger:"Choose trigger...",Trigger_Dependency:"Trigger dependency",Trigger_NewTrigger:"New trigger",Trigger_EditTrigger:"Edit trigger",Trigger_ActiveRerunRanges:"Active rerun ranges",Trigger_AddRerunRange:"Add rerun range",Trigger_AddRerunUpTo5:"Up to 5",Trigger_CanvasButtonFormat:"Trigger ({0})",Trigger_CanvasButtonNoTriggers:"Add trigger",Delete_Confirm_Title:"Delete?",Rerun_Confirm_Title:"Rerun?",Cancel_Confirm_Title:"Cancel?",Yield_Confirm_Title:"Yield?",Stop_Confirm_Title:"Stop?",Start_Confirm_title:"Start?",Cancel_Job_Template:"The job will be cancelled immediately.\n\nAre you sure you want to cancel the job {0}?",Yield_Job_Template:"The job will be yielded immediately.\n\nAre you sure you want to yield the job {0}?",Delete_Confirm_Content_Template:"Are you sure you want to delete:\n{0}?",Detach_Trigger_Confirm_Content_Template:"Are you sure you want to detach {0} from {1}?",Delete_Pipeline_Warning:"Please note that existing pipeline runs will not be cancelled.",TumblingWindow_Delete_Confirm_Title:"Deleting this trigger will remove it from other triggers that are dependent on it.",Trigger_NoParameters:"This pipeline has no parameters",Trigger_TriggerParameters:"Trigger Run Parameters",Trigger_ParametersWarning:"Parameters that are not provided a value will not be included in the trigger.",Trigger_PipelinesLengthError:"Trigger '{0}' can only contain one pipeline",Trigger_TumblingInvalidIntervalError:"Tumbling Window trigger with frequency 'Minutes' must have interval of at least 5.",Trigger_TumblingWindowSelfDependencyMustHaveNegativeOffset:"Negative offset for self dependency not provided",Trigger_TumblingWindowDuplicateNoOverlappingWindow:"Duplicate trigger must not have overlapping windows",Trigger_TumblingWindowMaxDependency:"The maximum number of dependencies allowed is {0}.",Trigger_TumblingWindowMaxDependencyExceeded:"The trigger '{0}' has {1} dependencies, the maximum number of dependencies allowed is {2}.",Trigger_TumblingWindowDependencyTimeTooLong:"The the offset or window size for dependency '{0}' exceeds the maximum allowed dependency length of 45 days.",Trigger_TumblingWindowEditBlocked:"This property cannot be edited once the tumbling window trigger has been published.",Trigger_TumblingWindowInvalidOffset:"Please re-align offset for dependency {0} by -{1} or +{2} to align with upstream trigger runs.",Trigger_TumblingWindowInvalidWindowSize:"Please re-align window size for dependency {0} by -{1} or +{2} to align with upstream trigger runs.",Trigger_InvalidFrequencyError:"Selected Frequency not supported for trigger type",Trigger_DelayDescription:"The delay before data processing of the window starts",Trigger_MaxConcurrencyDescription:"The number of simultaneous trigger runs that are fired",Trigger_RerunConcurrencyDescription:"The number of simultaneous trigger runs that are fired for this specific rerun range",Trigger_RerunRangeWindowEstimate:"This will run approximately {0} windows.",Trigger_RetryCountDescription:'The number of retries before a pipeline run is marked as "Failed"',RetryIntervalDescription:"The number of seconds between each retry attempt",Trigger_DelayFormat:"Delay must be in HH:MM:SS format",Trigger_MaxConcurrencyRequired:"Enter a value for the max concurrency",Trigger_MaxConcurrencyRange:"Max concurrency value not in range 1-50",Trigger_RetryCountRange:"Retry count must be a positive number",Trigger_RetryIntervalRange:"Retry interval must be 30 or greater",Trigger_Interval_Month:"For Frequency type Month, interval has to be in the range 1 - 16 inclusive",Trigger_Interval_Week:"For Frequency type Week, interval has to be in the range 1 - 71 inclusive",Trigger_Interval_Day:"For Frequency type Day, interval has to be in the range 1 - 500 inclusive",Trigger_Interval_Hour:"For Frequency type Hour, interval has to be in the range 1 - 12000 inclusive",Trigger_Interval_Minute:"For Frequency type Minute, interval has to be in the range 1 - 720000 inclusive",Trigger_ParameterValueRequired:"Parameter '{0}' has no default value, must be specified",Trigger_EndTimeAfterStart:"End time cannot be before start time",Trigger_EndTimeInvalid:"End time is invalid",Trigger_StartTimeInvalid:"Start time is invalid",Trigger_Parameterization_ValidIntegerErrorMessage:"Parameter '{0}' should be a valid integer",Trigger_Parameterization_ValidFloatErrorMessage:"Parameter '{0}' should be a valid float",Trigger_Parameterization_ValidBooleanErrorMessage:"Parameter '{0}' should be a valid boolean",Trigger_Parameterization_ValidArrayErrorMessage:"Parameter '{0}' should be a valid array",Trigger_BlobCreated:"Blob created",Trigger_BlobDeleted:"Blob deleted",Trigger_IgnoreEmptyBlobsDescription:"If selected, blobs with zero bytes will be ignored and will not generate a pipeline run",Trigger_Event:"Event",Trigger_EventDescription:"Choose which events are associated with this trigger",Trigger_EventOptionsError:"Select at least one type of event",Trigger_ContainerDescription:"Choose or write the container name. The container name can only have between 3 and 63 lowercase letters, numbers and non consecutive dashes. It must be in the format /containername/",Trigger_ContainerFormat:"The container name is not written in an accepted format.",Trigger_ContainerNameRequired:"Container name is required",Trigger_ContainerNamePlaceholder:"/containername/",Trigger_ListContainersError:"Unable to list containers",Trigger_PreviewBlobResults:"Preview blob results",Trigger_EventFilters:"Event trigger filters",Trigger_EventStartsWith:"Starts with",Trigger_EventEndsWith:"Ends with",Trigger_EventContainerName:"Container name",Trigger_EventAllContainers:"All Containers",Trigger_EventFiltersMatches:'{0} blobs matched in "{1}"',Trigger_EventPageNavigationItemsDisplayed:"{0} - {1} of {2} items",Trigger_EventPreviewAuthenticationFailed:"Unable to list files and show data preview. Please ensure that:\n* Your storage account is not using a firewall\n* You have the 'Storage Blob Data Reader' role, or, permission to list the storage access keys.",Trigger_EventPreviewError:"Unable to list blobs. Please try again",Trigger_EventPreviewFilesGenericError:"Unable to list blobs. Error: {0} Message: {1}",Trigger_EventPreviewFilesystemNotFound:"Unable to list blobs. The filesystem '{0}' does not exist in the storage account '{1}'",Trigger_EventPreviewContainerNotFound:"Unable to list blobs. The container '{0}' does not exist in the storage account '{1}'",Trigger_EventPreviewPathNotFound:"Unable to list blobs. The path '{0}' does not exist in the storage account '{1}'",Trigger_EventInvalidScopeFormat:"Invalid scope format, the storage account name must be in lowercase and the scope must be written in the format: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/{storageaccountname}",Trigger_BeginsWithFormat:"The content of begins with is not an accepted format",Trigger_BeginsWithDescription:"\u2018Blob path\u2019 must begin with a container name enclosed by '/' characters. For example, '/orders/'. Folder and file name are optional but when specified they must appear after a '/blobs/' segment. For example: '/orders/blobs/2018/april/shoes.csv' or '/orders/blobs/2018/'",Trigger_BeginsWithDescription_new:"\u2018Blob path\u2019 must begin with the path. For example: '2018/april/shoes.csv' or '2018/'",Trigger_EndsWithFormat:"The content of ends with is not an accepted format",Trigger_EndsWithDescription:"\u2018Blob path\u2019 must end in a file name or extension. For example, 'shoes.csv' or '.csv'. Container and folder name are optional but when specified they must be separated by a '/blobs/' segment. For example '/orders/blobs/2018/april/shoes.csv'. To specify a folder in any container omit the leading '/' character. For example, 'april/shoes.csv'.",Trigger_BroadFiltersWarning:"Make sure you have specific filters. Configuring filters that are too broad can match a large number of files created/deleted and may significantly impact your cost.",Trigger_NoContainerWarning:"Selecting 'All Containers' will match a larger amount of files created/deleted since it targets all the files in the storage account. Review your filters carefully since this can significantly impact your cost.",Trigger_V2StorageAccountOnly:"Storage event triggers support only version 2 Storage accounts (General purpose).",Trigger_Chaining_DependentPipelines:"Dependent pipelines",Trigger_Recurrence_RunDays:"Run on these days",Trigger_Recurrence_RecurEvery:"Recur every",Trigger_Recurrence_AddRecurrence:"Add new recurrence",Trigger_Recurrence_MonthDays:"Select day(s) of the month to execute",Trigger_Recurrence_ExecuteTimes:"Execute at these times",Trigger_Recurrence_ScheduleTimesUTC:"Schedule execution times (UTC)",Trigger_Recurrence_HoursUTC:"Hours (UTC)",Trigger_Recurrence_MinutesUTC:"Minutes (UTC)",Trigger_Recurrence_ScheduleTimes:"Schedule execution times",Trigger_Recurrence_DailyDescription:"Hours must be in the range 0-23 and minutes in the range 0-59. The time specified follows the timezone setting above.",Trigger_Recurrence_DailyHourRange:"Hour values not in range 0-23",Trigger_Recurrence_DailyMinuteRange:"Minute values not in range 0-59",Trigger_Recurrence_Selector_Aria:"Current recurrence is set to every {0} {1}",Trigger_Recurrence_Day_Input_Aria:"Current {0} values are: {1}",Trigger_Recurrence_Day_Aria:"Current scheduled execution times: {0}",Trigger_Recurrence_Week_Aria:"Currently set to run on these days: {0}",Trigger_Activation_Error:"Trigger activation error",Trigger_Activation_Failed:"Trigger activation failed for {0}. ",Trigger_Deactivation_Error:"Trigger deactivation error",Trigger_Deactivation_Failed:"Trigger deactivation failed for {0}",Trigger_Activation_Failed_Toast_Title:"Failed to activate",Trigger_Activation_Failed_Toast_Desc:"Failed to activate {0} ({1}). ",Trigger_Deactivation_Failed_Toast_Title:"Failed to deactivate",Trigger_Deactivation_Failed_Toast_Desc:"Failed to deactivate {0} ({1}). ",Trigger_EventsSubscribe_Failed_Toast_Title:"Failed to subscribe",Trigger_EventsSubscribe_Failed_Toast_Desc:"Failed to subscribe to events for event trigger {0} ({1}). ",Trigger_EventsUnsubscribe_Failed_Toast_Title:"Failed to unsubscribe",Trigger_EventsUnsubscribe_Failed_Toast_Desc:"Failed to unsubscribe to events for event trigger {0} ({1}). ",Trigger_EventsSubscriptionStatus_Failed_Toast_Title:"Failed to get subscription status",Trigger_EventsSubscriptionStatus_Failed_Toast_Desc:"Failed to get subscription status on storage events for event trigger {0} ({1}). ",Trigger_EventsSubscribe_Failed:"Failed to subscribe to storage events for event trigger: {0}",Trigger_EventsSubscriptionStatus_Failed:"Failed to get subscription status on storage events for event trigger: {0}",Trigger_EventsUnsubscribe_Failed:"Failed to unsubscribe from storage events for event trigger: {0}",Trigger_ActivationErrorNoPipeline:"Trigger '{0}' cannot be activated and contain no pipelines",Trigger_CannotContainDuplicateRecurrenceScheduleOccurence:"Trigger '{0}' cannot contain duplicate monthly occurence types",EventGridTopicName:"Event grid topic name",EventGridTopic_Subscription_Description:"You can select an Azure subscription to filter the Azure event grid topic names.",Trigger_EventGridTopicScope_Description:"The event grid topic scope to which this trigger will subscribe for events",Trigger_SubjectBeginsWith_Description:"Only Events where the 'Subject' begins with this value will be associated with this trigger",Trigger_SubjectEndsWith_Description:"Only Events where the 'Subject' ends with this value will be associated with this trigger",Trigger_CustomEventDescription:"Choose which event types are associated with this trigger",Trigger_CustomEventsScopeRequired:"Scope needs to be defined",Trigger_CustomEventsTypeRequired:"At least one event type is required",Trigger_StorageEventTenantIdDescription:"Select the Azure Active Directory (Azure AD) account to use when connecting to your storage account.",Trigger_CustomEventTenantIdDescription:"Select the Azure Active Directory (Azure AD) account to use when connecting to your event grid topic.",Trigger_EnterValidOffset:"Enter a valid offset as a timespan -D.HH:MM:SS",Trigger_EnterValidWindowSize:"Enter a valid window size as a positive timespan D.HH:MM:SS",SchedulePipeline:"Schedule pipeline",NoSchedule:"No schedule",FrequencyRun:"Frequency-based run",SchedulePipelineConfiguration:"Schedule configuration",SchedulePipelineParameterConfiguration:"Parameter configuration",ScheduleTrigger:"Schedule trigger",TumblingWindowTrigger:"Tumbling window trigger",BlobEventsTrigger:"Storage events trigger",CustomEventsTrigger:"Custom events trigger",ChainingTrigger:"Chaining trigger",RerunTumblingWindowTrigger:"Rerun tumbling window trigger",SharedPipelines:"Share pipeline",Share:"Share",SharedPipelineCreationFailure:"Failed to create shared pipeline {0}",ShareSucceeded:"Succeeded to create shared pipeline {0}",TemplateGallery:"Template gallery",TemplateGalleryCap:"Template Gallery",GalleryCustomTemplates:"My templates",GalleryOfficialTemplates:"Microsoft templates",TransformLabel:"Transform",GalleryAuthorMicrosoft:"Microsoft",GalleryContributorCommunity:"Community",GalleryContributorOrganization:"My organization",GalleryCatagorySSISLabel:"SSIS",GalleryCategoryTransformDataFlowLabel:"Transform with data flow",GalleryCategoryPrepDataFlowLabel:"Prep with data flow",GallerySaveAsTemplate:"Save as template",GallerySaveAsTemplateTooltip:"Save the current resource configuration as a template for sharing or future use",EditTemplate:"Edit template",TemplateName:"Template name",TemplateGitLocation:"Git location",TemplateUserInputsLabel:"User inputs",TemplateNameRequired:"Template name is required",TemplateUserInputsTooltip:"Activities, datasets and the corresponding connections to data stores and computes that require an input from the user for the selected template",SaveTemplate:"Save template",DeleteTemplate:"Delete template",RelatedTemplates:"Related templates",UseThisTemplate:"Use this template",PipelineFromTemplate:"Pipeline from template",TemplateValidationOutput:"Template validation output",SaveTemplateRequireGitRepository_Title:"Git repository required",SaveTemplateRequireGitRepository_Message:"Connect your Azure Data Factory to a Git repository to save custom templates.",SaveTemplateRequireGitRepository_LinkMessage:"Learn more how to connect to a repository",TemplateNameNotUniqueErrorMessage:"This template name already exists. Provide a unique name.",TemplateNameReservedErrorMessage:"This name has been reserved, please use another name.",CustomTemplateLoadFailed:"Failed to load custom templates",UseTemplateResourceCountMessage:"{0} {1} ({2}) created from template",GalleryTemplateCreatedBy:"Created by",GalleryCollapseFilter:"Collapse filter",GalleryExpandFilter:"Expand filter",GalleryTemplateContributor:"Contributor",GalleryNoTagToDisplayMessage:"No tags available",TemplateDocumentationLink:"Documentation link",OpenPipeline:"Open pipeline",PluginWorkSpaceFolder:"plugin workspace folder",ImportFromPipelineTemplate:"Import from pipeline template",ImportPipelineTemplate:"Import pipeline template",ParameterizationTemplateValidationError:"The parameterization template JSON is invalid. Please fix it and validate again.",ValidationLabel:"{0} validation output",ValidationCompleteLabel:"Your {0} has been validated.",PipelineValidationLabel:"Pipeline validation output",PipelineValidationCompleteLabel:"Your pipeline has been validated.",DataFlowValidationLabel:"Data flow validation output",DataFlowValidationCompleteLabel:"Your data flow has been validated.",TemplateValidationCompleteLabel:"Your template has been validated.",NoErrorsValidationLabel:"No errors were found.",RerunValidationLabel:"Rerun validation if you have made additional changes.",CopyRuntimeValidationLabel:"Copy runtime environment validation output",CopyRuntimeValidationCompleteLabel:"Your copy runtime environment has been validated.",SandboxUIResourcesValidationLabel:"Resource validation output",SandboxUIResourcesValidationCompleteLabel:"Your resources have been validated.",Repairing_Desc:"Repairing the integration runtime {0} ({1}).",Repairing_Failed_Title:"Failed to repair",Repairing_Failed_Desc:"Failed to repair the integration runtime {0} ({1}).",Starting_Desc:"Starting {0} ({1}).",Start_Failed_Desc:"Failed to start {0} ({1}).",Completed_Request_Action:"Completed the requested action",Completing_Requested_Action:"Completing the requested action",Complete_Requested_Action_Failed:"Failed to complete the requested action",Enabling_Interactive_Query:"Enabling interactive authoring capability for {0} ({1}).",Disabling_Interactive_Query:"Disabling interactive authoring capability for {0} ({1}).",ADF_Save_LiveCreate_Success_Title:"Successfully created",ADF_Save_LiveCreate_Success_Desc:"Successfully created {0} ({1}).",ADF_Save_LiveUpdate_Success_Title:"Successfully applied settings",ADF_Save_LiveUpdate_Success_Desc:"Successfully applied settings to {0} ({1}).",Save_Success_Title:"Successfully saved",ADF_Save_Git_Success_Desc:"Successfully saved {0} ({1}).",A365_Save_Success_Title:"Successfully committed",A365_Save_Success_Desc:"Successfully committed {0} ({1}).",ADF_Save_LiveCreate_Failed_Title:"Failed to create",ADF_Save_LiveCreate_Failed_Desc:"Failed to create {0} ({1}).",ADF_Save_LiveUpdate_Failed_Title:"Failed to apply settings",ADF_Save_LiveUpdate_Failed_Desc:"Failed to apply settings to {0} ({1}).",ADF_Save_Git_Failed_Title:"Failed to save in the Git repository",ADF_Save_Git_Failed_Desc:"Failed to save {0} ({1}).",A365_Save_Failed_Title:"Failed to commit",A365_Save_Failed_Desc:"Failed to commit {0} ({1}).",ADF_Save_GIT_Title:"Saving in the {0}",A365_Save_GIT_Title:"Committing to the Git repository",ADF_Save_GIT_Desc:"Saving {0} ({1}) in the {2}.",A365_Save_GIT_Desc:"Committing {0} ({1}) to the Git repository.",ADF_Save_All_GIT_Desc:"Saving all changes in the {0}.",A365_Save_All_GIT_Desc:"Committing all changes to the Git repository.",ADF_Saved_All_GIT_Title:"Successfully saved in the {0}",A365_Saved_All_GIT_Title:"Successfully committed to the Git repository",ADF_Saved_All_GIT_Desc:"Successfully saved all changes in the {0}.",A365_Saved_All_GIT_Desc:"Successfully committed all changes to the Git repository.",Resource_Published_Failed_Title:"Failed to publish",Resource_Published_Failed_Desc:"Failed to publish {0} ({1}).",Resource_Publishing_Desc:"Currently publishing {0} ({1}).",Resource_Published_Desc:"Successfully published {0} ({1}).",Resource_Publishing_Delete:"{0} ({1}) will be deleted while publishing.",Resource_Deleted_Success_Title:"Successfully deleted",Resource_Deleted_Success_Desc:"Successfully deleted {0} ({1}).",Resource_Deleted_Success_Git_Desc:"Successfully deleted {0} ({1}) from the {2}.",Resource_Deleted_Failed_Title:"Failed to delete",Resource_Deleted_Failed_Desc:"Failed to delete {0} ({1}).",Resource_Delete_Failed_Desc_Git_Permissions:"Failed to delete {0} ({1}). Either select another branch or resolve the permissions in the Git repository",Resource_Delete_Failed_Desc_Git_Branch_Locked:"Failed to delete {0} ({1}) as current branch is locked. Either select another branch or resolve the permissions in the Git repository",Resource_Delete_Failed_Desc_Git_MisMatch:"Please switch to live mode and check whether this integration runtime is referenced by linked services. Please make sure your Git repository is in-sync with your {0}.",Resource_Saving:"Saving {0}.",Resource_Saved:"{0} has been saved.",Resource_Save_Failed:"Failed to save {0}.",Resource_Save_Failed_WithError:"Failed to save {0}.\nError: {1}",Resource_Provision_Failed:"Failed to provision {0}.",Resource_Publishing:"Publishing {0}.",Resource_Publish_Failed:"Failed to publish {0}.",Resource_Published:"Successfully published {0}.",Resource_ToBeCreatedTitle:"Will be created",Resource_ToBeCreatedDesc:"{0} ({1}) will be created when publishing.",Resource_ToBeUpdatedTitle:"Will be applied",Resource_ToBeUpdatedDesc:"Changes to {0} ({1}) will be applied when publishing.",Resource_ToBeDeletedTitle:"Will be deleted",Resource_ToBeDeleted:"{0} will be deleted upon publishing.",Resource_Deleted:"{0} has been deleted.",Resource_Delete_Failed:"Failed to delete {0}.",Resource_Delete_Failed_WithError:"Failed to delete {0}.\nError: {1}",Resource_Disassociated:"{0} has been disassociated.",Resource_Disassociate_Failed:"Failed to disassociate {0}.",Resource_Disassociate_Failed_WithError:"Failed to disassociate {0}.\nError: {1}",PermissionDeniedToPushToBranchText:"You are not allowed to save to current branch, either select another branch, or resolve the permissions in Azure DevOps.",BranchIsLockedText:"Current branch is locked. Either select another branch, or resolve the permission in Azure DevOps.",UnableToSaveBranchPermissions:"\u200bUnable to save {0} ({1}). Either select another branch or resolve the permissions in the Git repository.",UnableToSaveBranchLocked:"Unable to save {0} ({1}) as current branch is locked. Either select another branch or resolve the permissions in the Git repository.",BranchCreationPermissionRequired:"You need the Git 'CreateBranch' permission to perform this action. Use an existing branch, or resolve the permission in Azure DevOps.",NoProfileForAADUser:"There is no Azure DevOps account associated with the selected Azure Active Directory {0}, either create a Azure DevOps account for the active directory or choose another directory.",Git_MaximumFileSizeExceeded:"The size of the resource exceeds 20 MB.",SapTable_LogonType:"Logon type",SapTable_LogonType_ApplicationServer:"Application server",SapTable_SncQop_Integrity:"Integrity",SapTable_SncQop_Privacy:"Privacy",ModelName_ServerName:"Server name",ModelName_DatabaseName:"Database name",ModelName_UserName:"User name",ModelName_SubscriberName:"Subscriber name",ModelName_X509CertificatePath:"X509 certificate path",ModelName_Password:"Password",ModelName_DisableCertValidation:"Disable certificate validation",ModelName_HttpRequestTimeout:"HTTP request timeout",ModelName_DisableAsync:"Disable async pattern",HttpRequestTimeout_ErrorMessage:"HTTP request timeout can be from 1 to 10 minutes as a TimeSpan (00:01:00 to 00:10:00)",WebActivityDisableAsyncTooltip:"Option to disable invoking HTTP GET on location given in response header of a HTTP 202 Response. If set true, it stops invoking HTTP GET on http location given in response header. If set false then continues to invoke HTTP GET call on location given in http response headers.",WebActivityHttpTimeoutTooltip:"Timeout for the HTTP request to get a response. Format is in TimeSpan (hh:mm:ss). This value is the timeout to get a response, not the activity timeout. The default value is 00:01:00 (1 minute). The range is from 1 to 10 minutes",WebActivityURLTooltip:"Target endpoint and path.",WebActivityRelativeURLTooltip:"Target endpoint and path, relative to base URL path in Linked Service.",WebActivityLinkedServiceToolTip:"Reference to the Rest linked service for authentication purpose.",WebActivityConnectionToolTip:"Reference to the Rest connection for authentication purpose.",WebActivityMethodTooltip:"REST API method for the target endpoint.",WebActivityBodyTooltip:"Represents the payload that is sent to the endpoint.",WebActivityHeadersTooltip:'Headers that are sent to the request. For example, to set the language and type on a request: "headers" : { "Accept-Language": "en-us", "Content-Type": "application/json" }.',WebActivityAuthenticationTooltip:"Authentication method used for calling the endpoint.",infoSentViaUrlWarning:"Information will be sent to the URL specified. Please ensure you trust the URL entered.",WebActivityLinkedServicesTooltip:"The selected linked service(s) will be sent in the request payload to the endpoint. The information will be sent to the URL specified. Please ensure you trust the URL entered.",WebActivityDatasetsTooltip:"The selected dataset(s) will be sent in the request payload to the endpoint. The information will be sent to the URL specified. Please ensure you trust the URL entered.",WebActivityHttpTimeoutError:"HTTP Request Timeout is invalid. The value must be formatted as HH:MM:SS and must be between 1 and 10 minutes",WebActivityUseLSBanner:"Inline way of using web activities will deprecate by Feb 29 2024. Please start using Linked Service to execute your web activities.",ModelName_SecurityToken:"Security token",ModelName_Host:"Host",ModelName_Subdomain:"Subdomain",ModelName_Endpoint:"Endpoint",ModelName_Cluster:"Cluster",ModelName_ContainerUriName:"Container URI",ModelName_StorageAccountName:"Storage account name",ModelName_StorageAccountKey:"Storage account key",ModelName_StorageAccountKind:"Storage account kind",ModelName_EndpointSuffix:"Endpoint suffix",ModelName_SasURI:"Storage SAS URI",ModelName_SasToken:"SAS token",ModelName_SasUrl:"SAS URL",ModelName_Url:"URL",ModelName_BaseUrl:"Base URL",ModelName_ServiceUrl:"Service URL",ModelName_ServiceUri:"Service Uri",ModelName_SessionToken:"Session token",ModelName_SiteUrl:"Site URL",ModelName_AzureCloudType:"Azure cloud type",ModelName_SystemNumber:"System number",ModelName_ClientId:"Client ID",ModelName_ClientKey:"Client Key",ModelName_ClientSecret:"Client secret",Language:"Language",Language_Description:"Choose the language displayed in the {0}",RegionalFormat:"Regional format",RegionalFormat_Description:"Choose the regional format displayed in the {0}",ModelName_ConnectionString:"Connection string",ModelName_Credential:"Credential",ModelName_Authentication_Source:"Authentication source",ModelName_ProjectID:"Project ID",ModelName_AdditionalProjectIDs:"Additional project IDs",ModelName_RequestGoogleDriveScope:"Request access to Google drive",ModelName_RefreshToken:"Refresh token",ModelName_AccountsUrl:"Accounts Url",ModelName_MessageServer:"Message server",ModelName_MessageServerService:"Message server service",ModelName_SystemId:"System ID",ModelName_LogonGroup:"Logon group",ModelName_SncMode:"SNC mode",ModelName_SncMyName:"SNC my name",ModelName_SncPartnerName:"SNC partner name",ModelName_SncLibraryPath:"SNC library path",ModelName_SncQop:"SNC quality of protection",ModelName_IsSSO:"With single sign-on",ModelName_KeyFilePath:"Key file path",ModelName_TrustedCertPath:"Trusted cert path",ModelName_UseSystemTrustStore:"Use system trust store",ModelName_AllowHostNameCNMismatch:"Allow host name CN mismatch",ModelName_AllowSelfSignedServerCert:"Allow self-signed server cert",ModelName_UseEncryptedEndpoints:"Use encrypted endpoints",ModelName_UseHostVerification:"Use host verification",ModelName_UsePeerVerification:"Use peer verification",ModelName_DeploymentType:"Deployment type",ModelName_OrganizationName:"Organization name",ModelName_ServerType:"Server type",ModelName_ThriftTransportProtocol:"Thrift transport protocol",ModelName_ServiceDiscoveryMode:"Service discovery mode",ModelName_ZooKeeperNameSpace:"Zoo keeper namespace",ModelName_UseNativeQuery:"Use native query",ModelName_HttpPath:"Http path",ModelName_ConsumerKey:"Consumer key",ModelName_PrivateKey:"Private key",ModelName_RedirectUri:"Redirect uri",ModelName_ServerVersion:"Server version",ModelName_Catalog:"Catalog",ModelName_CompanyId:"Company ID",ModelName_AccessTokenSecret:"Access token secret",ModelName_CredString:"Cred string",ModelName_AuthMech:"Auth mech",ModelName_SecurityLevel:"Security level",ModelName_CaCertFile:"CA certificate file",ModelName_ServiceEndpoint:"Service endpoint",ModelName_ClientCustomerId:"Client customer ID",ModelName_LoginCustomerId:"Login customer ID",ModelName_DeveloperToken:"Developer token",ModelName_AccountURI:"Account URI",ModelName_PackageCollection:"Package collection",ModelName_CertificateCommonName:"Certificate common name",ModelName_SqlServerLinkedService:"SQL server linked service",ModelName_PreFetch:"Prefetch",ModelName_Socket:"Socket",PreFetch_Description:"The number of rows to cache in memory at once.",Socket_Description:"The size of the socket communications buffer between the data store and the driver in bytes. Allowed value is between 4096 and 131072.",Socket_ValidationMessage:"Allowed value is between 4096 and 131072.",ModelName_ServerHint:"server or server:port",ModelName_ServerName_Description:'Specify the server name and optional port number as "server" or "server:port". The default port is 50000 if not specified.',Disabled_Preview_Title_Label:"Please go to activity page to preview data.",ModelName_Extension:"Extension",ModelName_Extensions:"Extensions",ModelName_CmdKeyCommandExtension:"Cmdkey Command Extension",ModelName_CmdKeyCommandExtensions:"Cmdkey Command Extensions",ModelName_ComponentExtension:"Install 3rd Party Component Extension",ModelName_ComponentExtensions:"Install 3rd Party Component Extensions",ModelName_MsdtcConfigurationExtension:"Configure MSDTC Extension",ModelName_MsdtcConfigurationExtensions:"Configure MSDTC Extensions",ModelName_EnvVariableExtension:"Environment Variable Extension",ModelName_EnvVariableExtensions:"Environment Variable Extensions",ModelName_SqlServerAliasExtension:"SQL Server Alias Extension",ModelName_SqlServerAliasExtensions:"SQL Server Alias Extensions",ModelName_LicensedComponentExtension:"Install Licensed Component Extension",ModelName_LicensedComponentExtensions:"Install Licensed Component Extensions",ModelName_AzPowerShellExtension:"Install Azure PowerShell Extension",ModelName_AzPowerShellExtensions:"Install Azure PowerShell Extensions",ModelName_ExpressCustomSetupError:"Error of express custom setup",ModelName_ExpressCustomSetupErrors:"Errors of express custom setup",ModelName_PackageStore:"Model of Package store",ModelName_PackageStores:"Model of Package stores",ModelName_PackageStoreName:"Package Store Name",ModelName_PackageStoreLinkedService:"Package Store Linked Service",ModelName_Request_Timeout:"Request timeout",ModelName_REST_AdditionalSettings:"Additional settings",ModelName_REST_Source_Request_Interval:"Request interval",ModelName_REST_Sink_Write_Batch_Size:"Write Batch Size defaults to 10000",ModelName_REST_Cache_Description:"Lookup output in cache for repeated input values.",ModelName_Source_QueryTimeout:"Query timeout",ModelName_Source_QueryTimeout_Min:"Query timeout (minutes)",ModelName_IngestionMappingName:"Ingestion mapping name",ModelName_AdditionalPropertiesName:"Additional properties",ModelName_NoTruncation:"No truncation",NoTruncation_Description:"Indicates whether to truncate the returned result set. By default result is truncated after 500,000 records or 64 MB. Truncation is strongly recommended for a proper behavior of the activity.",ModelName_AdditionalPropertiesName_Desc:"A property bag which can be used for specifying any of the ingestion properties which aren't being set already by the Kusto Sink. Specifically, it can be useful for specifying ingestion tags.",ModelName_AMLTrainedModelName:"Trained model name",ModelName_AMLTrainedModelFilePath:"Trained model file path",ModelName_AMLWebServiceInputs:"Web service inputs",ModelName_AMLWebServiceOutputs:"Web service outputs",ConnectToExternalDatastore_Title:"Connect to external data",ConnectToExternalDatastore_SubTitle:"Once a connection is created, the underlying data of that connection will be available for analysis in the Data hub or for pipeline activities in the Integrate hub.",CreateDataset_SubTitle:"In pipeline activities and data flows, reference a dataset to specify the location and structure of your data within a data store.",CreateDatamapper_SubTitle:"Change Data Capture (CDC) automatically detects data changes at the source and then sends the updated data to the destination.",EditNewTables:"Edit new tables",AirflowEntity:"Airflow Entity",AirflowEntities:"Airflow Entities",ModelName_DataSet:"Dataset",OrchestrationDataset:"Integration dataset",OrchestrationDatasets:"Integration datasets",ModelName_DataSets:"Datasets",ModelName_AccountName:"Account name",ModelName_AzureBatchAccessKey:"Access key",ModelName_AzureBatchBatchUrl:"Batch URL",ModelName_AzureBatchPoolName:"Pool name",ModelName_StorageLinkedServiceName:"Storage linked service name",ModelName_Token:"Token",ModelName_APIKey:"API key",ModelName_DatasetId:"Dataset ID",ModelName_AzureMLUpdateResourceEndPoint:"Update resource endpoint",ModelName_AzureMLEnableUpdateResourceEndPoint:"Enable update resource",ModelName_AzureMLDisableUpdateResourceEndPoint:"Disable update resource",ModelName_AzureMLUseARMWebService:"Azure resource manager web service",ModelName_AzureMLUseClassicalWebService:"Classical web service",ModelName_Columns:"Columns",ModelName_CompressionType:"Compression type",ModelName_HttpCompressionType:"Http Compression type",ModelName_Compression:"Compression",ModelName_CompressionLabel:"Compression level",ModelName_RowElement:"Row element",ModelName_CopyBehavior:"Copy behavior",ModelName_FileFormatType:"File format type",ModelName_NumberOfLinesToSkip:"Number of lines to skip",ModelName_FirstRowAsHeader:"First row as header",FirstRowAsHeader_Hint:'When source "first row as header" property is set as an expression, "import schema" now treats it as false. To work around, you can explicitly set it as true, then import schema, and at last set it back to expression.',ModelName_TreatEmptyAsNull:"Treat empty as null",LoggingSettingsLabel:"Logging settings",ModelName_ExpiryDatetime:"Expiry datetime (UTC)",ModelName_ExpiryDatetime_Description:'Specifies the expiry time of the written files. The time is applied to the UTC time zone in the format of "2018-12-01T05:00:00Z". By default it is NULL, which means the written files are never expired.',AzureDataLakeStoreWriteSettings_ExpiryDateTimeError:"The expiry time must be greater than the current date time.",ModelName_Port:"Port",ModelName_Sid:"SID",ModelName_ServiceName:"Service name",ModelName_Service:"Service",ModelName_Domain:"Databrick Workspace URL",ModelName_WorkspaceResourceId:"Workspace resource ID",ModelName_WorkspaceResourceIdLowerCase:"workspace resource ID",ModelName_AccessToken:"Access token",ModelName_ExistingClusterId:"Existing cluster ID",ModelName_InstancePoolId:"Instance pool ID",DataBricks_ChooseFromExistingClusters:"Choose from existing clusters",ModelName_NewClusterVersion:"Cluster version",ModelName_NewClusterNumOfWorker:"Number of worker nodes",ModelName_NewClusterNodeType:"Cluster node type",ModelName_NewClusterSparkConf:"Cluster Spark conf",ModelName_NewClusterCustomTags:"Cluster custom tags",ModelName_NewClusterSparkEnvVars:"Cluster Spark environment variables",ModelName_NewClusterInitScripts:"Cluster init scripts",ModelName_NewClusterDriverNodeType:"Cluster driver node type",ModelName_NewClusterLogDestination:"Cluster Log Dbfs Destination",ModelName_EnableSSL:"Secure Transmission",ModelName_EnableServerCertificateValidation:"Server Certificate Validation",ModelName_SkipHostKeyValidation:"SSH Host Key Validation",ModelName_HostKeyFingerprint:"SSH Host Key Finger-print",ModelName_PrivateKeyPath:"Private key path",ModelName_PrivateKeyContent:"Private key content",ModelName_PassPhrase:"Pass Phrase",ModelName_CertificatePassword:"Certificate Password",ModelName_CertificateThumbprint:"Certificate Thumbprint",ModelName_EmbeddedCertificateData:"Embedded Certificate Data",DataBricks_PythonVersion:"Python Version",ModelName_HDInsightClusterUri:"Cluster Uri",ModelName_HDInsightIsEspEnabled:"Enterprise Security Package enabled",ModelName_HdfsClusterUri:"Cluster Url",ModelName_AccessKeyID:"Access key ID",ModelName_AmazonAccessKey:"Secret access key",ModelName_Policy:"policy",ModelName_Policies:"policies",ModelName_DataLakeStoreUri:"Account Uri",ModelName_FunctionAppUrl:"Function App URL",ModelName_FunctionKey:"Function Key",ModelName_ResourceGroupName:"Resource group name",ModelName_SubscriptionName:"Subscription name",ModelName_Tenant:"Tenant",ModelName_TenantId:"Tenant ID",ModelName_AADResourceId:"AAD resource",ModelName_ServicePrincipalID:"Service principal ID",ModelName_ServicePrincipalKey:"Service principal key",ModelName_ServicePrincipalCert:"Service principal certificate",ModelName_ServicePrincipalCredential:"Service principal credential",ModelName_ServicePrincipalEmbeddedCert:"Service principal embedded cert",ModelName_ServicePrincipalEmbeddedCertPassword:"Embedded cert password",ModelName_ScriptLinkedservice:"Script linked service",ModelName_UsqlScriptPath:"U-SQL script path",ModelName_ScopeScriptPath:"Scope script path",ModelName_NebulaArguments:"Nebula arguments",ModelName_GeneratedFromActivity:"Generated from activity",ModelName_OutputDatasets:"Output Datasets",ModelName_LastCompiledTime:"Last compiled time",ModelName_DateTimeParamName:"dateTimeParamName",ModelName_OutputAdlsLinkedService:"Output datasets ADLS linked service",ModelName_OutputAdlsCosmosStructuredStreamLinkedService:"Output datasets ADLS Cosmos Structured Stream linked service",ModelName_DegreeOfParallelism:"Degree of parallelism",ModelName_Priority:"Priority",ModelName_RuntimeVersion:"Runtime version",ModelName_CompilationMode:"Compilation mode",ModelName_ADLAAccount:"ADLA linked service",ModelName_ADLAAccount_Trident:"ADLA connection",ModelName_OAuthUserName:"OAuth user name",ModelName_OAuthPassword:"OAuth password",ModelName_OAuthTokenEndpoint:"OAuth endpoint",ModelName_EmailRecipients:"Email recipients",ModelName_JobOwner:"Job owner",ModelName_JobName:"Job Name",ModelName_MaxNodes:"Max nodes",ModelName_MaxNodePercent:"Max nodes percentage",ModelName_ScopeScriptInclusionSet:"Scope script inclusion set",ModelName_clusterNamePrefix:"Cluster name prefix",ModelName_SparkVersion:"Spark version",ModelName_OsType:"OS type",ModelName_ScriptAction:"Script action",ModelName_ScriptName:"Script name",ModelName_ScriptUri:"Script URI",ModelName_ScriptNodeTypes:"Node type(s)",ModelName_ScriptParameters:"Script parameters",ModelName_clusterUserName:"Cluster user name",ModelName_clusterPassword:"Cluster password",ModelName_clusterSshUserName:"Cluster SSH user name",ModelName_clusterSshPassword:"Cluster SSH password",ModelName_HeadNodeSize:"Head node size",ModelName_DataNodeSize:"Data node size",ModelName_ZookeeperNodeSize:"Zoopkeeper node size",ModelName_CoreConfig:"Core configuration",ModelName_HBaseConfig:"Hbase configuration",ModelName_HDFSConfig:"HDFS configuration",ModelName_HiveConfig:"Hive configuration",ModelName_MapReduceConfig:"Map reduce configuration",ModelName_OozieConfig:"Oozie configuration",ModelName_StormConfig:"Storm configuration",ModelName_YarnConfig:"Yarn configuration",ModelName_Office365Tenant:"Microsoft 365 Tenant",Office365Tenant_tooltip:"Microsoft 365 tenant must be the same Azure Active Directory (Azure AD) tenant as current {0}.",ModelName_MarketplaceID:"Marketplace ID",ModelName_SellerID:"Seller ID",ModelName_MwsAuthToken:"MWS AuthToken",ModelName_SubnetName:"Subnet name",ModelName_VirtualNetworkId:"Virtual Network ID",ModelName_AuthHeaders:"Auth headers",AuthHeaders_Description:"Specify additional HTTP request headers for authentication.",ModelName_NotebookPath:"Notebook path",ModelName_NotebookLibraries:"Notebook libraries",ModelName_BasePrarmters:"Base parameters",MainClassNameLabel:"Main class name",ModelName_JarLibraries:"Jar libraries",ModelName_PythonFile:"Python File",ModelName_Mapper:"Mapper",ReducerLabel:"Reducer",ModelName_ScriptPath:"Script path",ModelName_Defines:"Defines",VariableLabel:"Variable",ModelName_ClassName:"Class name",ModelName_SPROCParameter:"Stored procedure parameters",ModelName_RootPath:"Root path",ModelName_EntryFilePath:"Entry file path",ModelName_JobLinkedService:"Job linked service",SparkConfiguration:"Spark configuration",ModelName_Arguments:"Arguments",ModelName_WriteBatchSize:"Write batch size",ModelName_WriteBatchSize_Description:"Number of rows to insert into the SQL table per batch. The allowed value is integer (number of rows). By default, the service dynamically determines the appropriate batch size based on the row size.",ModelName_WriteBatchTimeout:"Write batch timeout",ModelName_WriteBatchTimeout_Description:'The wait time for the batch insert operation to finish before it times out. The allowed value is timespan. The default value is "00:30:00" (30 minutes).',ModelName_QuoteAll:"Quote all",ModelName_QuoteAllText:"Quote all text",ModelName_Warehouse:"Warehouse",ModelName_Command:"Command",ModelName_CommandTimeout:"Command timeout",ModelName_CommandType:"Command Type",ModelName_FileExtension:"File extension",ModelName_FileExtension_Description:"The file extension used to name the output files. It will be ignored when file name is configured in the sink dataset.",ModelName_RecordName:"Record name",ModelName_RecordNamesapce:"Record namesapce",ModelName_ForceToLoad:"Force to load all files",ModelName_ForceToLoad_Description:"Specifies to load all files, regardless of whether they\u2019ve been loaded previously and have not changed since they were loaded. Note that this option reloads files, potentially duplicating data in a table",ModelName_FileFormatOptions:"File format options",ModelName_FileFormatOptions_Description:'Additional file format options provided to COPY command. For example, "DATE_FORMAT= YYYY-MM-DD TIME_FORMAT= HH24:MI"',ModelName_LoadUncertainFiles:"Load uncertain files",ModelName_LoadUncertainFiles_Description:"Specifies to load files for which the load status is unknown.",ModelName_FunctionName:"Function name",ModelName_MLWorkspaceName:"Azure Machine Learning workspace name",ModelName_MLPipelineParameters:"Machine Learning pipeline parameters",ModelName_MLPipelineDataPathAssignments:"Machine Learning data path assignments",ModelName_MLPipelineId:"Machine Learning pipeline ID",ModelName_MLPipelineIdType:"Machine Learning pipeline ID type",ModelName_PipelineId:"Pipeline ID",ModelName_MLPipelineEndpointId:"Pipeline endpoint ID",ModelName_MLPipelineEndpointIdVersion:"Pipeline version",ModelName_MLPipelineName:"Machine Learning pipeline name",ModelName_MLPipelineEndpointName:"Machine Learning pipeline endpoint name",ModelName_MLPipelineSelectEndpoint:"Select pipeline endpoint via",ModelName_MLExperimentName:"Experiment name",ModelName_MLParentRunId:"Machine Learning parent run ID",ModelName_EndpointName:"Endpoint name",ModelName_EndpointID:"Endpoint id",ModelName_MLContinueOnStepFailure:"Continue on step failure",Frequency:"Frequency",ModelName_Interval:"Interval",ModelName_ScheduledTriggers:"Schedule triggers",ModelName_ChainingTrigger:"Chaining",ModelName_ChainingTriggers:"Chaining triggers",ModelName_ExtensibleTrigger:"Extensible Trigger",ModelName_Minutes:"Minutes",ModelName_Hours:"Hours",ModelName_Weekdays:"Weekdays",ModelName_MonthDays:"Month days",ModelName_Week_Days:"Week days",ModelName_MonthlyOccurences:"Monthly occurences",ModelName_RecurrenceSchedule:"Recurrence schedule",ModelName_Day:"Day",ModelName_Occurence:"Occurence",ModelName_RecurrenceScheduleOccurence:"Recurrence schedule occurence",ModelName_TumblingWindowTrigger:"Tumbling window",ModelName_TumblingWindowTriggers:"Tumbling window triggers",ModelName_StorageEvents:"Storage events",ModelName_CustomEvents:"Custom events",ModelName_StorageEventTriggers:"Storage event triggers",ModelName_CustomEventTriggers:"Custom event triggers",ModelName_SubjectFilters:"Subject filters",SubjectFiltersTooltip:"Apply filters to the subject of each event. Only events with matching subjects get delivered. Learn more",ModelName_SubjectBeginsWith:"Subject begins with",ModelName_SubjectEndsWith:"Subject ends with",ModelName_BlobPathBeginsWith:"Blob path begins with",ModelName_BlobPathEndsWith:"Blob path ends with",ModelName_IgnoreEmptyBlobs:"Ignore empty blobs",ModelName_Delay:"Delay",ModelName_RetryPolicy:"Retry policy",ModelName_RetryInterval:"Retry policy: interval in seconds",ModelName_RetryCount:"Retry policy: count",ModelName_MaxConcurrency:"Max concurrency",ModelName_RerunConcurrency:"Rerun concurrency",ModelName_ChooseDependencies:"Add dependencies",ModelName_AuthenticationType:"Authentication type",ModelName_ServicePrincipalCredentialType:"Service principal credential type",ModelName_ServiceAuthentication:"Service authentication",ModelName_UserAuthentication:"User authentication (Preview)",ModelName_UserAuthentication_NoPreview:"User authentication",ModelName_RerunTumblingWindowTrigger:"Rerun tumbling window",ModelName_RerunTumblingWindowTriggers:"Rerun tumbling window triggers",ModelName_ParentTrigger:"Parent trigger",ModelName_RequestedStartTime:"Requested start time",ModelName_RequestedEndTime:"Requested end time",ModelName_ReferenceName:"Reference name",ModelName_SynapseWorkspaceName:"Azure Synapse Analytics workspace name",ModelName_SynapseWorkspaceEndpoint:"Azure Synapse Analytics workspace URL",SynapseWorkspaceURLDescription:"Workspace development endpoint can be found in Azure portal under workspace overview.",SynapseWorkspaceIdDescription:"The resource ID of the Synapse workspace. This can be found in the properties of the Synapse workspace, and it should be of the format: /subscriptions/{subscriptionID}/resourceGroups/{resourceGroup}/providers/Microsoft.Synapse/workspaces/{workspaceName}",ModelName_AzureDataExplorerName:"Cluster name",ModelName_Scope:"Storage account",ModelName_Table:"Table",ModelName_View:"View",ModelName_Report:"Report",ModelName_Reports:"Reports",ModelName_Sheets:"Sheets",QueryLabel:"Query",QueryInterpolationTooltip:'Use string interpolation to make use of parameters by enclosing a query string with quotation marks. For example, "Select * from {$tableName}".',QueryNewlineWarningMessage:"You have a newline character '\\n' present in your query. This will be replaced with a new line when viewing this query upon refresh. The result of the query at runtime will be unaffected.",ModelName_BaseRequestId:"Base request ID (>)",ModelName_ExcludeLastRequest:"Exclude last request",ModelName_StorageBlob:"Storage blob",ModelName_StorageQueue:"Storage queue",ModelName_EventhubNamespace:"Event Hubs namespace",Create_All_Label:"Create all",Create_Ingestion_PE_Label:"Create ingestion PEs",ModelName_Container:"Container",ModelName_FileFilter:"File filter",ModelName_ListBefore:"List before",ModelName_ListAfter:"List after",ModelName_ListBeofreAndAfterMode:"Name range",FileNameLabel:"File name",BlobNameLabel:"Blob name",ModelName_WildcardPath:"Wildcard path",ModelName_WildcardFolderPath:"Wildcard folder path",ModelName_WildcardFileName:"Wildcard file name",ModelName_FileListPath:"Path to file list",ModelName_BucketName:"Bucket name",ModelName_ModifiedDatetimeStart:"Modified datetime start (UTC)",ModelName_ModifiedDatetimeEnd:"Modified datetime end (UTC)",SkipLineCountLabel:"Skip line count",ModelName_CommentChar:"Comment char",ModelName_IncludeAttribute:"Include attribute",ModelName_AttributeColumnPrefix:"Attribute column prefix",ModelName_EnableSchemaValidation:"Enable schema validation",ModelName_IgnoreLeadingWhiteSpace:"Ignore leading whitespace",ModelName_IgnoreTrailingWhiteSpace:"Ignore trailing whitespace",ModelName_AddtionalNullValues:"Addtional null values",ModelName_CompletedDateTimeStart:"Completed datetime start (UTC)",ModelName_CompletedDateTimeEnd:"Completed datetime end (UTC)",ModelName_MaxRowsPerFileLabel:"Max rows per file",ModelName_MaxRows_ValidationError:"Max rows per file and {0} cannot be specified together.",FileNamePrefix_ValidationError:"File name prefix is supported only when source is tabular or hierarchical type dataset, and with no partition option enabled.",FileNamePrefix_MaxRows_ValidationError:"File name prefix is not supported when max rows per file is empty.",MaxRowsPerFileDesc:"When writing data into a folder, you can choose to write to multiple files and specify the max rows per file.",MaxRowsPerFileDisabledHint:"This property will be ignored when copying into a file.",FileNamePrefixDesc:"Specify the file name prefix when writing data to multiple files, resulted in this pattern: <fileNamePrefix>_00000.<fileExtension>. If not specified, prefix will be auto generated. Not applicable when source is file-based store or source has partition option enabled.",ModelName_PreserveZipFileNameAsFolder:"Preserve zip file name as folder",ModelName_PreserveCompressionFileNameAsFolder:"Preserve compression file name as folder",PreserveCompressionFileNameAsFolder_Description:'Preserve the source compressed file name as folder structure during copy. When set to true, ADF writes uncompressed files into folder "<path in dataset>/<folder named as source compressed file>/"; when set to false, ADF writes data directly to "<path specified in dataset>".',PreserveCompressionFileNameAsFolder_Tip:"Make sure you don't have duplicated file names in different source compressed files to avoid racing or unexpected behavior.",ModelName_IgnoreTableNotFound:"Ignore table not found",ModelName_RowKeyName:"Row key name",ModelName_PartitionKeyName:"Partition key name",ModelName_SheetName:"Sheet name",ModelName_SheetIndex:"Sheet index",ModelName_SheetMode:"Worksheet mode",SheetName_Description:"The Excel worksheet name. If you point to a folder or multiple files, make sure this particular worksheet exists in all those files.",SheetIndex_Description:"The Excel worksheet index. If there is worksheet added or deleted from excel file, the index of existed worksheets will change automatically.",SheetRange_Description:"Specify a range in the worksheet to locate the data. For example, A3:H5.",ModelName_ColumnDelimiter:"Column delimiter",ModelName_Encoding:"Encoding",ModelName_CompressionCodec:"Compression codec",ModelName_CharToEscapeQuoteEscaping:"Char to escape quote escaping",ModelName_Template:"Template",ModelName_Templates:"Templates",ModelName_EnvironmentUrl:"Environment url",ModelName_SalesforceExternalIdFieldName:"External ID field",ModelName_SalesforceApiVersion:"Salesforce API version",Api_Version_Default_ValueToolTip:"The default value is 45.0 when the linked service is used in copy source, and 40.0 in copy sink.",Api_Version_Description:'You can edit to specify a certain API version. Click <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2170062">here</a> to check the latest available API version.',Api_Version_Invalid_ValueMessage:"Please input a valid API version, e.g. 54.0",Api_Version_Not_SupportDefault:"'Default' is no longer supported. Please specify an explicit API version.",Api_Version_Placeholder:"E.g. 54.0",ModelName_PowerQueryActivities:"Power Query activities",ModelName_AccountID:"Account ID",ModelName_OrgID:"Organization ID",ModelName_MetaScopes:"Metascopes",ModelName_Exp:"Exp",ModelName_JournalEndpoint:"JournalEndpoint",SalesforceExternalIdFieldNameDescription:'The name of the external ID field for upsert operation. The specified field must be defined as "External ID Field" in the Salesforce object, and it cannot have NULL values in the corresponding input data.',SalesforceExternalIdFieldNameMissingError:"The name of the external ID field for upsert operation is required.",ModelName_IgnoreNullValues:"Ignore null values",ModelName_AlternateKeyName:"Alternate key name",IgnoreNullValuesDescription:"Indicates whether to ignore null values from input data during write operation.Allowed values are: true, and false. - true: leave the data in the destination object unchanged when doing upsert/update operation, and insert defined default value when doing insert operation. - false: update the data in the destination object to NULL when doing upsert/update operation, and insert NULL value when doing insert operation.",HttpEnableServerCertificateValidationDescription:"Specify whether to enable server SSL certificate validation when connecting to HTTP endpoint. When your HTTPS server is using self-signed certificate, set this to false.",HttpAuthenticationTypeDescription:"Allowed values are Anonymous, Basic, Digest, Windows, and ClientCertificate. User-based OAuth isn't supported. You can additionally configure authentication headers in authHeader property.",DynamicsAlternateKey_Description:"Specify the alternate key name defined on your entity to upsert records.",ArmTemplateConfiguration:"ARM template configuration",PublishConfigLoadError:"Unable to load publish config file, default values will be used. Please verify that the '{0}' JSON file in your repository is not malformed and try again.",PublishConfigLoadErrorNoDefault:"Unable to load publish config file. Please verify that the JSON file in your repository is not malformed and try again.",PublishAbortValidationFailed:"Validation of model(s) failed. Fix it before publishing",PublishErrorCollaborationUserAreNotSame:"Sync and commit your changes with collaboration branch before publishing.",PublishCommitMessage:"ARM template and parameters deployed on {0}, based on the collaboration branch's commit ID: {1}",PublishCommitMessageA365:"Template and parameters deployed on {0}, based on the collaboration branch's commit ID: {1}",DataFactoryArmTemplateCommitMessage:"Data Factory ARM template deployed on {0}, based on the collaboration branch's commit ID: {1}",SaveAbortValidationFailed:"Validation of model(s) failed. Fix it before saving",Usql_CompilationModeOptions_Semantic:"Semantic",Usql_CompilationModeOptions_Full:"Full",Usql_CompilationModeOptions_SingleBox:"SingleBox",Global:"Global",SourceControl_InvalidProjectState:"Invalid project state encountered",WranglingLimitedMTransformSupportInfoMessage:"Currently not all Power Query M functions are supported for data wrangling despite being available during authoring.",PowerQuerySchemaMappingWarningMessage:"Please select a Power Query and sink to configure sink schema mapping.",PowerQuerySchemaMappingInfoMessage:"Ensure that your source has the latest schema before validating or publishing changes.",PowerQuerySchemaMappingLoadingMessage:"Loading schema mappings...",LearnMore:"Learn more",NoVsoAccount:"Could not find a valid Azure DevOps account.",NoVsoTeamProject:"Could not find a valid Azure DevOps team project for the selected account.",NoVsoTeamProjectErr:"User does not have access to Azure DevOps account or have no projects",NoVsoAccountErr:"Failed to access Azure DevOps account with error {0}",NoVsoAccountWithoutErr:"Failed to access Azure DevOps account",ParameterConfiguration_Error_DuplicateName:"Parameters with duplicate name will be overwritten.",SaveAllLabel:"Save all",SaveAllToolTip:"Save current changes without validation to your git repository",SaveAllWithCommentLabel:"{0} with comment",SaveAllEditComment:"Edit comment here...",CommitAllLabel:"Commit all",Commit:"Commit",CommitLowercase:"commit",Committed:"Committed",Committing:"Committing...",PublishAndSaveAll:"Publish will {0} all pending changes",RefreshConfirmDialogTitleTemplate:"{0} will discard any changes not saved.",RefreshConfirmDialogContent:"If you want to keep these changes, select {0} first.",SwitchSaveAllText:"Are you sure you want to save all pending changes to continue?",SwitchCommitAllText:"Are you sure you want to commit all pending changes to continue?",AracdiaRefreshPrompt:"Discard all pending changes of {0} to continue?",SaveChangesBeforePublish:"If you want to discard these changes, select Discard all first.",SaveLowercase:"save",CommitHistory:"Commit history",PublishAll:"Publish all",PublishAllToolTip:"Validate all resources and publish them",PublishAllChangesText:"Are you sure you want to publish all pending changes to continue?",QuestionMark:"?",UpdateFormatTypeDatasetGeneralMessageTemplate:"Are you sure you want to change the linked service type from {0} to {1}? Note the following pipelines and activities which reference this dataset will be modified to use the new type as well.",UpdateFormatTypeDatasetWarningMessage:"Existing pipelines and activities settings which are incompatible with the new type will be cleared. ",ApplyChanges:"Apply changes?",ToggleFullscreen:"Toggle full screen mode",EnterFullscreen:"Enter full screen mode (Ctrl+Shift+F)",ExitFullscreen:"Exit full screen mode (Ctrl+Shift+F)",UploadWithoutLinkedServiceError:"Please select a linked service to upload file.",UploadWithoutFilePathError:"Please specify a path to upload your local file.",AzurePostgreSQL_WriteBatchSize_Description:"The number of rows loaded into PostgreSQL per batch. By default, Data Factory uses 1,000,000.",SubscriberProcess:"Subscriber process",RecoveryPointer:"Recovery pointer",Recovery:"Recovery",AmazonRedshift_DisplayText:"Amazon Redshift",AzureSearch_DisplayText:"Azure Search",CosmosDb_DisplayText:"Azure Cosmos DB for NoSQL",CosmosDbMongoApi_DisplayText:"Azure Cosmos DB for MongoDB",CosmosMongoServerAbove32:"Above 3.2",CosmosMongoServerDesc:'Select the version of Azure Cosmos DB API for MongoDB. <a href="https://go.microsoft.com/fwlink/?linkid=2168384" target="_blank">Click to learn more.</a>',Cassandra_DisplayText:"Cassandra",Db2_DisplayText:"DB2",FtpServer_DisplayText:"FTP",Hdfs_DisplayText:"HDFS",HttpServer_DisplayText:"HTTP",Lakehouse_DisplayText:"Lakehouse",LakehouseTable_DisplayText:"Lakehouse table",Lakehouse_Name_DisplayText:"Lakehouse name",Lakehouse_Name_Empty_Message:"Name can't be empty",Lakehouse_Name_InvalidFirstChar_Message:"Name must start with a letter.",Lakehouse_Name_InvalidCharacters_Message:"Name can only contain alphanumeric characters and underscores.",Lakehouse_Name_InvalidLength_Message:"Name cannot be more than 123 characters.",Lakehouse_Name_AlreadyInUse_Message:"Lakehouse name already exists",Lakehouse_Creation_Failed_Message:"Failed to create a new lakehouse",Lakehouse_Root_Folder_Type_Table:"Tables",Lakehouse_Root_Folder_Type_Files:"Files",Lakehouse_Table_Action_Append_Description:"Append new values to existing table",RestService_DisplayText:"REST",MongoDb_DisplayText:"MongoDB",MongoDbAtlas_DisplayText:"MongoDB Atlas",MySql_DisplayText:"MySQL",OData_DisplayText:"OData",SharePointOnlineList_DisplayText:"SharePoint Online List",SharePointOnlineFile_DisplayText:"SharePoint Online File",Odbc_DisplayText:"ODBC",PostgreSql_DisplayText:"PostgreSQL",Salesforce_DisplayText:"Salesforce",SapBW_DisplayText:"SAP BW via MDX",SapOpenHub_DisplayText:"SAP BW Open Hub",SapTable_DisplayText:"SAP Table",SapOdp_DisplayText:"SAP CDC",SapHana_DisplayText:"SAP HANA",Sftp_DisplayText:"SFTP",Sybase_DisplayText:"Sybase",Teradata_DisplayText:"Teradata",TeamDesk_DisplayText:"TeamDesk",Quickbase_DisplayText:"Quickbase",Smartsheet_DisplayText:"Smartsheet",Zendesk_DisplayText:"Zendesk",Workday_DisplayText:"Workday",AdobeExperiencePlatform_DisplayText:"Adobe Experience Platform",AdobeIntegration_DisplayText:"Adobe Integration",AdobeExperiencePlatform_ApiKeyDescription:"The API key value from your Adobe I/O Console integration",AdobeExperiencePlatform_AccountIdDescription:"The\u202fTechnical Account ID\u202ffrom your Adobe I/O Console integration",AdobeExperiencePlatform_OrgIdDescription:"The IMS org credentials from your Adobe I/O Console integration ",AdobeExperiencePlatform_ClientSecretDescription:"The client secret generated from your Adobe I/O Console integration.",AdobeExperiencePlatform_PrivateKeyContentDescription:"The private key to generate the JWT token",AdobeExperiencePlatfor_CompletedDateTimeDescription:"The batches with completed time in the range [startTime, endTime) will be filtered for further processing. The time will be applied to UTC timezone in the format of 'yyyy-mm-ddThh:mm:ss.fffZ'.",AdobeExperiencePlatfor_CompletedDateTimeError:"Completed datetime end must be greater than completed datetime start.",AdobeIntegration_ExpDescrption:"Expiration time for JWT token (hour)",AdobeIntegration_MetaScopesDescrption:"Metascopes of JWT token",AdobeIntegration_JournalApiEndpointDescription:"Journaling API Endpoint of Adobe I/O Events",AdobeIntegration_TenantIdDescription:"Adobe standard Tenant ID",AdobeIntegration_AccountIdPlaceholder:"Your technical account ID",AdobeIntegration_OrgIdPlaceholder:"Your organization ID",AdobeIntegration_TenantIdPlaceholder:"Your Adobe standard Tenant ID",AdobeIntegration_APIkeyPlaceholder:"Your API Key/Client Id",AdobeIntegration_ExpPlaceholder:"E.g. 60 (minutes)",AdobeIntegration_JournalEndpointPlaceholder:"E.g. https://xxx.adobe.io/events/organizations/xxx/integrations/xxx/",AdobeIntegration_EventsMetaScope:"I/O Events",AdobeIntegration_IOManagementMetaScope:"I/O Management API",AdobeIntegration_PrivacyMetaScope:"Privacy Service API",AdobeIntegration_FilterColumn:"Filter Column",AdobeIntegration_FilterValue:"Filter Value",AzureKeyVault_DisplayText:"Azure Key Vault",AzureKeyVault_DisplayText_With_Abbr:"Azure Key Vault (AKV)",AzureKeyVault_ServicePrincipalCert_DisplayText:"Service principal certificate from Azure Key Vault",AzureKeyVault_SecretVersion_Latest:"latest",Dynamics_DisplayText:"Dynamics",DynamicsAX_DisplayText:"Dynamics AX",AzureMySql_DisplayText:"Azure Database for MySQL",AzureML_DisplayText:"Azure Machine Learning Studio (classic)",AzureMLService_DisplayText:"Azure Machine Learning",Anonymous_DisplayText:"Anonymous",Basic_DisplayText:"Basic",Digest_DisplayText:"Digest",Windows_DisplayText:"Windows",ClientCertificate_DisplayText:"Client Certificate",SapCloudForCustomer_DisplayText:"SAP Cloud For Customer",AmazonMWS_DisplayText:"Amazon Marketplace Web Service",AzureDataShare_DisplayText:"Azure Data Share",AzureDataShare_Account:"Azure Data Share Account",AzureDataShare_AccountName:"Azure Data Share account name",AzureDataShare_InvalidAccountName:"Data share account name should have length of 3 - 90, and cannot contain <>%&:?/#*$^();,.|-+={}[]!~@",AzureDataShare_InputValidSubscriptionId:"Please input a valid subscription ID",AzureDataShare_Wizard_ShareName:"Share name",AzureDataShare_Wizard_ShareType:"Share type",AzureDataShare_Wizard_Recipients:"Recipients",AzureDataShare_Wizard_NumberOfRecipients:"Number of recipients",AzureDataShare_Wizard_ReviewAndCreate:"Review + create",AzureDataShare_Wizard_ReviewAndCreate_Description:"Confirm Copy activity summary",AzureDataShare_Wizard_AddNewShare:"Add Data Share",AzureDataShare_InPlace:"In place",AzureDataShare_CopyBased:"Copy based",AzureDataShare_Daily:"Daily",AzureDataShare_Hourly:"Hourly",AzureDataShare_ShareContents:"Share contents",AzureDataShare_ShareType:"Data share type",AzureDataShare_SnapshotSettings:"Snapshot settings",AzureDataShare_SnapshotSchedule:"Snapshot schedule",AzureDataShare_AddDatasetsHelpText:"Select datasets to be shared. You must have permission to add role assignment to the data store. This permission exists in the Owner role.",AzureDataShare_AddDatasets:"Add datasets",AzureDataShare_Validation_AtLeastSize2:"Name must be at least 2 characters long",AzureDataShare_Validation_NoMoreThan90:"Name must be 90 characters or less",AzureDataShare_Validation_OnlyContainAlphanumericAndUnderscore:"Name can only contain alphanumeric characters and _",AzurePublicCloudLabel:"Azure Public",AzureChinaCloudLabel:"Azure China",AzureUSGovernmentCloudLabel:"Azure US Government",AzureGermanyCloudLabel:"Azure Germany",AzurePostgreSql_DisplayText:"Azure Database for PostgreSQL",Concur_DisplayText:"Concur",Couchbase_DisplayText:"Couchbase",Drill_DisplayText:"Drill",Eloqua_DisplayText:"Oracle Eloqua",GoogleBigQuery_DisplayText:"Google BigQuery",GoogleAdWords_DisplayText:"Google AdWords",GoogleAdWords_Legacy_DisplayText:"Google AdWords (Legacy)",GoogleAdWords_V2_DisplayText:"Google Ads",GoogleAdWords_ApiType_DisplayText:"API version",GoogleAdWords_VersionUpgrade_WarningText:"Please select Google Ads to upgrade your linked service to newer version before the legacy API is deprecated by Google. Once upgraded, reversion is not available.",GoogleAdwords_Query_Description:'The syntax for Google Ads Query Language is similar to AWQL from the AdWords API, with some minor changes. <a target="_blank" href="https://go.microsoft.com/fwlink/?linkid=2185675">Learn more</a>',Greenplum_DisplayText:"Greenplum",HBase_DisplayText:"HBase",Hive_DisplayText:"Hive",Hubspot_DisplayText:"HubSpot",Impala_DisplayText:"Apache Impala",Jira_DisplayText:"Jira",EventHubs_DisplayText:"EventHubs",Magento_DisplayText:"Magento",MariaDB_DisplayText:"MariaDB",AzureMariaDB_DisplayText:"Azure Database for MariaDB",Marketo_DisplayText:"Marketo",Netezza_DisplayText:"Netezza",Vertica_DisplayText:"Vertica",SapEcc_DisplayText:"SAP ECC",Paypal_DisplayText:"PayPal",Phoenix_DisplayText:"Phoenix",Presto_DisplayText:"Presto",QuickBooks_DisplayText:"QuickBooks",ServiceNow_DisplayText:"ServiceNow",Shopify_DisplayText:"Shopify",Spark_DisplayText:"Spark",Lake_DisplayText:"Lake",Square_DisplayText:"Square",Xero_DisplayText:"Xero",Snowflake_DisplayText:"Snowflake",Zoho_DisplayText:"Zoho",Kusto_DisplayText:"Kusto",KustoDatabase_DisplayText:"KQL Database",DataWarehouse_DisplayText:"Data Warehouse",AzureDataExplorer_DisplayText:"Azure Data Explorer (Kusto)",AzureDataExplorerCommand_DisplayText:"Azure Data Explorer Command",Office365_DisplayText:"Microsoft 365 (Office 365)",Microsoft365_DisplayText:"Microsoft 365",OracleResponsys_DisplayText:"Oracle Responsys",SalesforceMarketingCloud_DisplayText:"Salesforce Marketing Cloud",OracleServiceCloud_DisplayText:"Oracle Service Cloud",OracleCloudStorage:"Oracle Cloud Storage (S3 API)",SalesforceMarketingCloud_Subdomain_Description:'Subdomain is represented by a 28-character string starting with the letters "mc". E.g. mc563885gzs27c5t9-63k636ttgm',Workday_Subdomain_Description:"Subdomain for Workday. You can find it from endpoint: https://${subdomain}.workday.com/ccx/service/${tenant}/${service_name}",Workday_TenantId_Description:"Tenant for Workday. You can find it from endpoint: https://${subdomain}.workday.com/ccx/service/${tenant}/${service_name}",Workday_Username_Description:"Username you use for Workday login",Workday_Service_Description:"Service name to construct Workday Endpoint, only Human Resources service is suppported as of now.",Workday_Operation_Description:"Operation name to construct Workday Endpoint",Workday_Request_DisplayText:"Request Payload",Workday_Request_Description:"SOAP request payload. Modify the sample request payload as per your requirement and make sure to follow the payload structure.",Workday_Request_Cardinality_Optional:"\x3c!--Optional:--\x3e",Workday_Request_Cardinality_0_Or_More:"\x3c!--Zero or more repetitions:--\x3e",Workday_Request_Cardinality_1_Or_More:"\x3c!--1 or more repetitions:--\x3e",Workday_Request_Cardinality_By_Occurs:"\x3c!--Cardinality:[min({0}), max({1})]:--\x3e",Workday_Request_Choice:"\x3c!--You have a CHOICE of the next {0} items at this level--\x3e",Workday_Request_Namespace_Def:'xmlns:{0}="{1}"',Workday_Request_XML_Wrapper:'<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" {0}><soapenv:Header>{1}</soapenv:Header><soapenv:Body>{2}</soapenv:Body></soapenv:Envelope>',Workday_Service_Version_DisplayText:"Service Version",Workday_Service_Version_Description:"Service Version to use to make request to workday endpoint.",Workday_Service_Version_Placeholder:"i.e. v39.1",AzureBlobStorage_DisplayText:"Azure Blob Storage",AzureTableStorage_DisplayText:"Azure Table Storage",AzureFileStorage_DisplayText:"Azure File Storage",AzureQueue_DisplayText:"Azure Queue Storage",Dynamics365_DisplayText:"Dynamics 365",DynamicsCRM_DisplayText:"Dynamics CRM",DynamicsCRMEntity_DisplayText:"Dynamics CRM Entity",CommonDataService_DisplayText:"Dataverse (Common Data Service for Apps)",Informix_DisplayText:"Informix",InformixTable_DisplayText:"Informix Table",MicrosoftAccess_DisplayText:"Microsoft Access",MicrosoftAccessTable_DisplayText:"Microsoft Access Table",SalesforceServiceCloud_DisplayText:"Salesforce Service Cloud",SalesforceServiceCloudObject_DisplayText:"Salesforce Service Cloud Object",DataCatalog_DisplayText:"Data Catalog",AzureEventHubs_DisplayText:"Event Hubs",AzureEventHubs_SelectMethodDescription:"You can select a Azure Event Hubs namespace from the list of available namespaces in your Azure subscriptions.",ModelName_AzureEventHubsNamespace:"Event Hub namespace",ModelName_AzureEventHubsHubName:"Event Hub name",ModelName_AzureEventHubsPolicyName:"Event Hub policy name",ModelName_AzureEventHubsPolicyKey:"Event Hub policy key",ModelName_AzureEventHubsConsumeGroup:"Consumer Group",CognitiveService_DisplayText:"Azure Cognitive Services",CognitiveService_SelectionMethod:"Azure Cognitive Services selection method",CognitiveServices_SelectSubscription_Description:"Only Cognitive Services supporting private endpoints are listed.",CognitiveService_SelectMethodDescription:"You can select an Azure Cognitive Services namespace from the list of available namespaces in your Azure subscriptions.",ModelName_CognitiveServiceName:"Azure Cognitive Services name",ModelName_CognitiveServiceKind:"Azure Cognitive Services kind",ModelName_CognitiveServiceLocation:"Azure Cognitive Services location",ModelName_CognitiveServiceKey:"Azure Cognitive Services key",ModelName_CognitiveServiceEndPoint:"Azure Cognitive Services endpoint",DocumentDbCollection_DisplayText:"DocumentDB Collection (SQL API)",CosmosDbMongoDbApiCollection_DisplayText:"Azure Cosmos DB Collection for MongoDB",MongoDbCollection_DisplayText:"MongoDB Collection",MongoDbV2Collection_DisplayText:"MongoDB Collection V2",ODataResource_DisplayText:"OData Resource",SharePointOnlineListResource_DisplayText:"SharePoint Online List Resource",SharePointOnlineList_SiteURL_Description:"The URL of the SharePoint Online site. For example, https://contoso.sharepoint.com/sites/siteName.",SharePointOnlineList_Tenant_Description:"The tenant ID under which your application resides. You can find it from Azure portal Active Directory overview page.",SharePointOnlineList_ServicePrincipalId_Description:"The application (client) ID of your application registered in Azure Active Directory. Make sure to grant SharePoint site permission to this application.",SharePointOnlineList_ServicePrincipalKey_Description:"The client secret of your application registered in Azure Active Directory.",SharePointOnlineListResource_ListName_Description:"The name of the SharePoint Online list.",SharePointOnlineListResource_Query_Description:'The OData query to filter the data in SharePoint Online list. For example, "$top=1".',SharePointOnlineListSource_HttpRequest_Description:"The wait time to get a response from SharePoint Online. Default value is 5 minutes (00:05:00).",RelationalTable_DisplayText:"Relational Table",WebTable_DisplayText:"Web Table",ErrorMessageForDeleteLinkedService:"Not able to delete linked service, message returned from service - {0} {1}",ErrorMessageForLinkedServiceDependency:"Make sure to publish any changes to dependent resources as they may have been committed but not yet published.",SourceControlService_NullResponse:"Fatal! Response cannot be null for source control request",SourceControlService_Unauthorized:"Unauthorized access! User does not have access to the resource requested. Reason: {0}",SourceControlService_ResourceNotFound:"Requested resource not found. Reason: {0}",SourceControlService_TimeOut:"Time out! Request timed out waiting for response Reason: {0}",SourceControlService_Conflict:"Request could not processed due to conflict with updates. Reason: {0}",SourceControlService_Other:"Resource not found or inaccessible. Reason: {0}",InternalServerError:"Internal server error. Reason: {0}",Validator_ErrorMessage_StringLength:"{0} length is invalid. Length must be within limits {1} - {2}",RepoInvalidNamePattern:"The repository name is invalid",BranchInvalidNamePattern:"The branch name is invalid",FolderInvalidNamePattern:"The folder name is invalid",FolderInvalidDotNamePattern:"The folder name cannot end with '.'",NativeCopyWizard_Title:"Copy Data (Native)",ErrorSavingTabs:"Error while saving entities. Details: {0}",TabsWithInvalidName:"Some of the unsaved entities have invalid names ({0}). Fix the name to continue.",TabWithInvalidName:"Invalid name detected. Fix it to continue",ImportExistingResources:"Import existing resources to repository",ImportExistingResourcesLabel:"Import existing resources",ImportResourcesLabel:"Import resources",ImportResourcesNotificationMessage:"Successfully imported existing resources to repository",ImportResourcesTooltip:"Import existing resources from live environment to a git repository branch",GitDisablePublish:"Disable publish (from ADF Studio)",GitDisablePublishLabel:"Automated publish config",GitDisablePublishTooltip:'Disable publish from ADF studio to avoid overwriting the last automated publish deployment. <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/continuous-integration-delivery-improvements">Learn more</a>',GitPublishLabel:"Publish (from ADF Studio)",GitPublishDisabledTooltip:"{0} from ADF Studio is disabled to avoid overwriting automated deployments. If required you can change publish setting in Git configuration.",Directory:"Directory",SelectGitHubEnterprise:"Use GitHub Enterprise Server",SelectGitHubEnterpriseDescription:"Only GitHub Enterprise Server mode is supported in this environment.",FactoryResources:"Factory Resources",MainMenuLabel:"Main menu",SparkOnCosmos:"SparkOnCosmos",FailedToImportResources:"Configured repository, but failed to import resource from {0}, this could happen if there are invalid resources. Details: {1}",FailedToImportResourcesWithGitHubPermissionDenied:"You don't have permission to import resource to the repository, please resolve the permissions in GitHub",FailedToLoadPostPublish:"Failed to load after publish",FailedToPublish:"Failed to publish change(s) because of the following validation error(s). Fix error(s) to continue publishing.",CanOnlyPublishInCollaborationBranch:"Publish is only allowed from collaboration ('{0}') branch.  Create a pull request to merge your changes into the collaboration branch.",MergeChangesToMasterBranch:"Merge the changes to '{0}'.",NewOrDirtyLiveOnlyResourcesPublishFirstTitle:"Resources to be published first",NewOrDirtyLiveOnlyResourcePublishFirstTitle:"Resource to be published first",NewOrDirtyLiveOnlyResourcesPublishFirstContent:"{0} don't support to be saved into git and need to be published first.",NewOrDirtyLiveOnlyResourcePublishFirstContent:"{0} doesn't support to be saved into git and needs to be published first.",ViolationsFound:"Violations detected resources:",SapCDCSLTPreviewMessage:"Preview data is not supported for SLT system",Others:"Others",Octal:"Octal",CopyWizard_CopyActivityAdditionalSettingsLabel:"Other copy settings",CopyWizard_Target_Type:"Target type",ODICopyLabel:"ODI Copy",DeploymentCompleted:"Deployment completed",DeploymentUnderway:"Deployment is underway",InProgress:"In progress",Executing:"Executing",RunInProgress:"Run in progress",StatusStoppedSessionTimedOut:"Stopped (session timed out)",CopySink:"copy sink",ModelName_WriteBehavior:"Write behavior",WriteBehavior_Option_Insert:"Insert",WriteBehavior_Option_Upsert:"Upsert",WriteBehavior_Option_Upload:"Upload",ModelName_SecretName:"Secret name",ModelName_SecretVersion:"Secret version",ModelName_CertName:"Certificate name",ModelName_CertVersion:"Certificate version",ModelName_AKVLinedService:"AKV linked service",AzureKeyVault_AccountName_SelectionMethod:"Azure key vault selection method",AzureKeyVault_AccountName_SelectionMethod_Description:"You can select an azure key vault account from the list of available accounts in your Azure subscriptions, in which case you don\u2019t need to enter account name in free form text fields.",AzureKeyVault_Subscription_Description:"You can select an azure subscription to filter the azure key vault accounts.",AzureKeyVault_AccountName_Select:"Azure key vault name",AzurKeyVault_BaseUrl:"Azure key vault base URL",AzureKeyVaultKeyName:"Key Name",AzureKeyVaultKeyVersion:"Key Version",AzureKeyVault_EditKeyVault_Text:"Edit key vault",Resource_NotExisting_ErrorTemplate:"Failed to load resource {0}. It may not exist in your {1}.",AzureKeyVault_SecretVersion_Placeholder:"Use the latest version if left blank",AzureKeyVaultSecretName_Tooltip_Template:"Specify the name of the Azure Key Vault secret that stores the destined {0}'s {1}.",AzureKeyVaultCertName_Tooltip_Template:"Specify the name of the Azure Key Vault certificate that stores the destined {0}'s {1}.",AzureKeyVaultSecretName_Tooltip_Template_ConnectionStringWithSample:"Specify the name of the Azure Key Vault secret that stores the destined {0}'s connection string (e.g. {1}).",AzureKeyVault_linkedService_Tooltip:"The linked service for Azure Key Vault that stores your secrets and certificates.",AzureKeyVault_secretName_Tooltip:"The name of your Azure Key Vault secret that holds the value.",AzureKeyVault_secretName_Tooltip_Template:"The name of your Azure Key Vault secret that holds the value to be assigned to {0}.",AzureKeyVault_secretVersion_Tooltip:"Specify the key vault secret version. 'Latest version' is recommended as you don't need to update the version number in the linked service even when you update the secret in key vault.",AzureKeyVault_CertVersion_Tooltip:"Specify the key vault certificate version. 'Latest version' is recommended as you don't need to update the version number in the linked service even when you update the secret in key vault.",AzureKeyVault_InvalidUrl:"Invalid key vault url",AzureKeyVault_RequiredValue_Tooltip:"Please provide the necessary parameter values in above linked service and then click on refresh button to get the {0} list.",AzureKeyVault_SwitchTOEdit_Tooltip:"Please switch to edit mode",AzureKeyVault_ParameterNotSupport_Tooltip:"We cannot list secret versions if the value of the Azure Key Vault linked service parameter is an expression. If you want to specify a specific secret version, please switch to edit mode",AzureKeyVault_PE_Status:"It depends on the linked service referencing this linked service.",UnpublishedResource:"This resource has not been published to your {0}",ModelName_DatasetTemplate:"{0} dataset",ModelName_DatasetsTemplate:"{0} datasets",ModelName_LinkedServiceTemplate:"{0} linked service",ModelName_LinkedServicesTemplate:"{0} linked services",ModelName_ActivityTemplate:"{0} activity",ModelName_ActivitiesTemplate:"{0} activities",ModelName_DisplayTextPreviewTemplate:"{0} (Preview)",ModelName_CopySourceTemplate:"{0} copy source",ModelName_CopySourcesTemplate:"{0} copy sources",ModelName_LocationTemplate:"{0} location",ModelName_LocationsTemplate:"{0} locations",ModelName_StoreReadSettingTemplate:"{0} store read setting",ModelName_StoreReadSettingsTemplate:"{0} store read settings",ModelName_AzureSearchAdminKey:"Service admin key",ModelName_BatchSize:"Batch size",ModelName_BatchSizeInBytes:"Batch size in bytes",ModelName_CursorMethods:"Cursor methods",ModelName_RfcTableOptions:"RFC table options",ModelName_RfcTableFields:"RFC table fields",ModelName_RowSkips:"Row skips",ModelName_CustomRfcReadTableFunctionModule:"Custom function module",CustomRfcReadTableFunctionModule_DescriptionTemplate:"A custom RFC function module that can be used to read data from an {0}. You can use a custom RFC function module to define how the data is retrieved from your SAP system and returned to Data Factory. ",ModelName_ParquetDataset:"Parquet",ModelName_DelaFormatDataset:"Delta",ModelName_DelimitedTextDataset:"DelimitedText",ModelName_JsonDataset:"JSON",ModelName_XmlDataset:"XML",ModelName_AvroDataset:"Avro",ModelName_OrcDataset:"ORC",ModelName_BinaryDataset:"Binary",ModelName_StoreReadSettings:"Store settings",ModelName_FormatReadSettings:"Format settings",ModelName_ExcelDataset:"Excel",ModelName_AzureCosmosAccountName:"Azure Cosmos DB account name",ModelName_AzureCosmosAccountURI:"Azure Cosmos DB account URI",ModelName_AzureCosmosAccountKey:"Azure Cosmos DB access key",AzureCosmos_SelectMethodDescription:"You can select a Azure Cosmos DB account from the list of available accounts in your Azure subscriptions.",AzureCosmos_NestingSeparatorDescription_ForSink:"A special character in the source column name to indicate that nested document is needed.",AzureCosmos_NestingSeparatorDescription_ForSource:"Special character to indicate that the document is nested and how to flattern the result set.",AzureCosmos_PageSizeDescription:'The number of documents per page of the query result. Default is "-1" which means uses the service side dynamic page size up to 1000.',CosmosMongoDBApi_CursorMethods_Key_Title:"Method type",CosmosMongoDBApi_CursorMethods_Value_Title:"Method value",PowerBI:"Power BI",CosmosMongoDBApi_CursorMethods_description:"Specify the way that the underlying query is executed.",MongoDB_BatchSize_Description:"Specifies the number of documents to return in each batch of the response from MongoDB instance.",SapTable_BatchSize_Description:"Specifies the maximum number of rows that will be retrieved at a time when retrieving data from SAP Table.",SapTable_RowCount_Description:"The number of rows to be retrieved.",SapTable_RowSkips_Description:"The number of rows that will be skipped.\u200b",SapTable_RfcTableOptions_Description:"The options for the filtering of the SAP Table. For example, \"COLUMN0 EQ 'SOME VALUE'\".",SapTable_RfcTableFields_Description:'The fields of the SAP table that will be retrieved. For example, "column0, column1".',Sap_ColumnDelimiter_Description:"The single character that will be used as delimiter passed to SAP RFC to split the output data. Use this property to keep leading/trailing spaces in your data.",Sap_ColumnDelimiter_ValidationError:"The column delimiter can only be single character.",Sap_RowCount_Partition_Confilict_ValidationError:"The row count and partition option cannot be enabled at the same time.",CassandraLinkedService_HostDescription:"One or more IP addresses or host names of Cassandra servers. Specify a comma-separated list of IP addresses or host names to connect to all servers concurrently.",CassandraLinkedService_PortDescription:"The TCP port that the Cassandra server uses to listen for client connections. (default is 9042)",ModelName_KeySpace:"Key space",ModelName_Cassandra_ConsistencyLevel:"Consistency level",RequiredTemplate:"{0} is required",RequiredTemplateWhen:"{0} is required when {1} is {2}",RequiredValidatorMessage:"{0} {1} requires a {2}",RequiredValidatorMessageNoVowel:"{0} is required in {1} {2}",PatternValidatorMessage:"{0} {1} cannot be '{2}'. It should be in the form of {3}.",AllowedValidatorMessage:"{0} {1} cannot be '{2}'. It should be one of the values in [{3}].",WholeNumberValidatorMessage:"{0} {1} cannot be '{2}'. It should be a whole number.",MinMaxValidatorMessageBetween:"{0} {1} cannot be '{2}'. It has to be between {3} and {4}.",MinMaxValidatorMessageMinimum:"{0} {1} cannot be '{2}'. It has to be a minimum value of {3}.",MinMaxValidatorMessageMaximum:"{0} {1} cannot be '{2}'. It can be a maximum value of {3}.",OccursValidatorMessageMinOne:"{0} '{1}' should have at least one {2}.",OccursValidatorMessageMinOneSwitch:"{0} '{1}' should have at least one {2} for each case.",OccursValidatorMessageMin:"{0} '{1}' should have at least {2} {3}.",OccursValidatorMessageMaxOne:"{0} '{1}' can have at most one {3}.",OccursValidatorMessageMax:"{0} '{1}' can have at most {2} {3}.",LinkCriteriaValidationMessage:"Link from '{0}' to '{1}' activity has invalid criteria, remove and put link again.",PipelineValidationMessage_Cyclic:"Pipeline '{0}' cannot have cycles. Activity '{0}' is in a cycle.",FilePathValidationMessage:"{0} {1} requires a {2}.",EventTriggerValidationMessage:"Blob events trigger requires a blob path input.",EventHubNamePatternValidation:"Either {0} or {1} should be defined",EventHubFormatValidation:"EventHub Format is invalid. It should be one of {0}, {1} or {2}",EventHubPayloadSchemaValidation:"EventHub payload schema is required when format is {0} or {1}",Error_SourceDatasetRequired:"Source dataset is required.",Error_SinkDatasetRequired:"Sink dataset is required.",Error_BinaryCopyNotSupportImportSchema:"Binary copy does not need import schemas.",Error_FileNameAndFilter:"FileName and FileFilter can't be specified together.",Error_ManagedIRNotSupport:"This feature is not supported by Azure integration runtime.",Error_SelfhostedIRNotSupport:"This feature is not supported by Self-hosted integration runtime.",Error_SelfhostIRNotValid:"Your Self-hosted integration runtime is offline or invalid.",Error_SelfhostedIRLowVersion:"The version of your self-hosted integration runtime is lower than required version. please upgrade to latest. http://go.microsoft.com/fwlink/?LinkId=734875",Error_ScriptUploadNotSupportAKV:"The script upload and preview function does not support the Azure Storage with Azure key vault authentication",Error_InteractiveQueryNotEnabled:"The interactive authoring capability is not enabled on the integration runtime '{0}'. Please enable interactive authoring first and retry the operation.",Error_InteractiveQueryNotEnabled_Link:"Click to enable interactive authoring capability",Error_InteractiveQueryNotEnabled_Button:"Enable interactive authoring",Error_IRLowVersionForV2MappingPrefix:'The version "{0}" of self-hosted integration runtime "{1}" is lower than {2}, which is not supported for the latest column mapping. Please ',Error_IRLowVersionForV2DatasetPrefix:'The version "{0}" of self-hosted integration runtime "{1}" is lower than {2}, which is not supported for your dataset. Please ',Error_IRLowVersionForSapCDCProjectionSelectionPrefix:'The version "{0}" of self-hosted integration runtime "{1}" is lower than {2}, which is not supported for {3} projection and selection. Please ',Error_IRLowVersionForSapResetActivityPrefix:'The version "{0}" of self-hosted integration runtime "{1}" is lower than {2}, which is not supported for Sap reset activity. Please ',Error_IRLowVersionForEncodingPrefix:'The version "{0}" of self-hosted integration runtime "{1}" is lower than {2}, which is not supported for your "{3}" encoding. Please ',Error_IRLowVersionForCopyCommandInStaging:'The version "{0}" of self-hosted integration runtime "{1}" is lower than {2}, which does not support staged copy data to Azure Synapse Analytics with Copy command. Please upgrade to the latest version.',Error_IRUpgradeClickEdit:" Click edit button to ",Error_IRUpgradeLink:"upgrade",Error_IRLowVersionSuffix:" to the latest version.",Error_LinkedServiceMissing_ErrorMessage:"Linked service of the dataset {0} is required",Hint_SelfhostedIRVersion:"Please make sure your self-hosted integration runtime is higher than version {0} if connecting via self-hosted integration runtime.",ValidationError_EmptyFilePathTemplate:"{0} value is required under {1} file path mode.",Error_Invalid_Request_Payload:"Invalid Request payload \n Found {0} error{1} \n {2}",Error_Invalid_XML_Element:"Error on line {0} at column {1}: XML element '{2}' has invalid value '?'",Error_Invalid_XML_Attribute:"Error on line {0} at column {1}: XML attribute '{2}' has invalid value '?'",DoNotShowAgain:"Don't show again",NPSSurvey_worst:"Not at all likely",NPSSurvey_best:"Extremely likely",NPSSurvey_followup:"What is the reason for your score?",FeatureResearch_Feedback_InitQuestion_opt1:"Overall, how useful is {0} in your work?",FeatureResearch_Feedback_InitQuestion_opt2:"Overall, how easy is it to use {0}?",FeatureResearch_Feedback_InitQuestion_opt3:"Overall, how satisfied or dissatisfied are you with {0}?",FeatureResearch_Feedback_Followup:"In the last month, which of the following tasks have you tried to accomplish with the {0}?",FeatureResearch_Feedback_A365_feat1:"Setting up a workspace",FeatureResearch_Feedback_A365_feat2:"Creating/managing Apache Spark pools",FeatureResearch_Feedback_A365_feat3:"Orchestrating pipelines",FeatureResearch_Feedback_A365_feat4:"Managing user access",FeatureResearch_Feedback_A365_feat5:"Monitoring workspace activity",FeatureResearch_Feedback_A365_feat6:"Ingesting data",FeatureResearch_Feedback_A365_feat7:"Using Apache Spark pools",FeatureResearch_Feedback_A365_feat8:"Creating/managing SQL pools",FeatureResearch_Feedback_A365_feat9:"Using dedicated SQL pools",FeatureResearch_Feedback_A365_feat10:"Using serverless SQL pool",FeatureResearch_Feedback_A365_feat11:"Using notebooks",FeatureResearch_Feedback_A365_feat12:"Authoring data flows",FeatureResearch_Feedback_A365_feat13:"Generating machine learning models",FeatureResearch_Feedback_A365_feat14:"Configuring workspace firewall settings",FeatureResearch_Feedback_A365_feat15:"Configuring managed private endpoints",FeatureResearch_Feedback_A365_feat16:"Configuring linked services",FeatureResearch_Feedback_A365_feat17:"Authoring PowerBI reports",FeatureResearch_Feedback_A365_feat18:"Accessing the knowledge center",FeatureResearch_Feedback_A365_feat19:"Reading Azure Synapse documentation",FeatureResearch_Feedback_A365_feat20:"SQL Data Warehouse (dedicated and serverless)",FeatureResearch_Feedback_ADF_feat1:"Authoring pipelines",FeatureResearch_Feedback_ADF_feat2:"CI/CD and Git integration",FeatureResearch_Feedback_ADF_feat3:"Debugging functional and performance issues",FeatureResearch_Feedback_ADF_feat4:"Triggering and scheduling pipeline execution",FeatureResearch_Feedback_ADF_feat5:"Monitoring and alerting",FeatureResearch_Feedback_ADF_feat6:"Lifting & shifting/migrating SSIS packages",FeatureResearch_Feedback_MoreQuestions:"Do you have a moment to answer a few more questions regarding your experience?",FeatureResearch_Feedback_TaskSatisfaction:"How satisfied or dissatisfied are you with each of the task?",ErrorMessage_Feedback_IsHelpfulOrNot:"How helpful or unhelpful was this error message?",ErrorMessage_Feedback_Thanks:"Thank you for your feedback!",ErrorMessage_Feedback_ImproveQuestion:"How can we improve?",ErrorMessage_Feedback_Content_Placeholder:'Tell us why you rated "{0}" (optional)',ErrorMessage_Feedback_CommentsOrSuggestion:"Comments or suggestions?",ErrorMessage_Feedback_TellUsHere:"Tell us here.",ErrorMessage_Feedback_Helpful_NotHelpful:"Not helpful",ErrorMessage_Feedback_Result_Poor:"Poor",ErrorMessage_Feedback_Result_Good:"Good",ErrorMessage_Feedback_Result_Excellent:"Excellent",Performance_Feedback_Question:"How satisfied or dissatisfied are you with the performance of this copy activity?",Performance_Feedback_InvitationProvide:"Provide ",Performance_Feedback_InvitationOnPerformance:" on performance.",Performance_Feedback_Result_VeryDissatisfied:"Very dissatisfied",Performance_Feedback_Result_Dissatisfied:"Dissatisfied",Performance_Feedback_Result_Neutral:"Neutral",Performance_Feedback_Result_Satisfied:"Satisfied",Performance_Feedback_Result_VerySatisfied:"Very satisfied",Performance_Feedback_Result_NotUseful:"Not useful",Performance_Feedback_Result_NotEasy:"Not easy",Performance_Feedback_Result_Slightly:"Slightly",Performance_Feedback_Result_Somewhat:"Somewhat",Performance_Feedback_Result_Mostly:"Mostly",Performance_Feedback_Result_Very:"Very",CopyWizard_Properties_PageDescription:"Enter name and description for the copy data task",CopyWizard_Properties_PageNewPathDescription:"Select copy data task type and configure task schedule",CopyWizard_Properties_NavKeyword_OneTimeCopy:"One time copy",CopyWizard_Properties_NavKeyword_RecurringCopy:"Recurring copy",CopyWizard_Properties_Name:"Task name",CopyWizard_Properties_NamePrefix:"Task prefix name",CopyWizard_Properties_Description:"Task description",CopyWizard_Properties_PipelineMode:"Task cadence or task schedule",CopyWizard_Properties_PipelineMode_Once:"Run once now",CopyWizard_Properties_PipelineMode_Schedule:"Run regularly on schedule",CopyWizard_Properties_TaskMode:"Task type",CopyWizard_Properties_PipelineTask_Typical:"Built-in copy task",CopyWizard_Properties_PipelineTask_TypicalShortDesc:"You will get single pipeline to copy data from 90+ data source easily.",CopyWizard_Properties_PipelineTask_TypicalLongDesc:"You will get single pipeline to quickly copy objects from data source store to destination in a very intuitive manner.",CopyWizard_Properties_PipelineTask_Metadata:"Metadata-driven copy task",CopyWizard_Properties_Metadata_ShortDescription:"You will get parameterized pipelines which can read metadata from an external store to load data at a large scale.",CopyWizard_Properties_Metadata_LongDescription:"You will get parameterized pipelines which can read metadata (tables name etc.) from the control table and copy them dynamically. You can update the control table to adjust the copy jobs without redeploying the pipelines.",CopyWizard_Properties_Metadata_ControlStoreDescription:"The metadata will be stored in the control table so that the generated parameterized pipelines can always get value from control table before running.  The metadata in control table contains list of table names, DB names, column mapping etc.",CopyWizard_Properties_PipelineTask_Metadata_ControlTableTemplate:"Control table {0}",CopyWizard_Properties_PipelineTask_Metadata_ErrorMessage:"Metadata-driven copy task requies a control table connection and a table name.",CopyWizard_Properties_PipelineTask_Metadata_EditMode_ErrorMessage:"Editing control table requies a query value.",CopyWizard_Properties_MetadataSensitiveData_HintMessage:"Mask credential with dummy code in SQL script",CopyWizard_Properties_MetadataEdit_AddCondition:"Add condition",CopyWizard_Properties_MetadataEdit_FailedToRecoverDataTemp:"Failed to fetch control metadata: {0}",CopyWizard_Properties_MetadataEdit_InvalidParameter:"please make sure the top-level pipeline contains the parameter 'MainControlTableName'.",CopyWizard_Properties_MetadataEdit_EmptyData:"no control metadata",CopyWizard_Properties_Metadata_InvalidSchemaTemplate:"Invalid control table schema: {0}.",CopyWizard_Properties_Metadata_TypeCanOnlyBeTemplate:"Column type of {0} should be {1}",CopyWizard_Metadata_TopLevel_Error:"top-level pipeline or inner activities.",CopyWizard_Metadata_MiddleLevel_Error:"middle-level pipeline or inner activities.",CopyWizard_Metadata_BottomLevel_Error:"bottom-level pipeline or inner activities.",CopyWizard_Metadata_ForeachActivity_Error:"ForEach activity in the bottom-level pipeline.",CopyWizard_Metadata_SwitchActivity_Error:"Switch activity in the bottom-level pipeline.",CopyWizard_Metadata_DefaultActvity_Error:"default activity in the bottom-level pipeline.",CopyWizard_Metadata_SourceDataset_Error:"source or sink dataset configuration in the bottom-level pipeline.",CopyWizard_Metadata_ControlTableLookup_Error:"Lookup activity for control table.",CopyWizard_Summary_SaveAndRun:"Save + Run",CopyWizard_Summary_StartDataTransferImmediately:"Start data transfer immediately",CopyWizard_Summary_StartDataTransferImmediately_Desc:"This will start running the current pipeline immediately after it's saved successfully.",AddPipelineActivityLabel:"Add pipeline activity",StartFromPipelineLabel:"Choose a task to start",StartBuildingPipelineHeader:"Start building your data pipeline",ConnectionLabel:"Connection",DataSourcesLabel:"Data sources",DataDestinationLabel:"Data destinations",ChooseDataSourceLabel:"Choose data source",ChooseDataSourceDescription:"Select a connector. Then enter the connection information.",ConnectToDataSourceLabel:"Connect to data source",ChooseDataDestinationLabel:"Choose data destination",ChooseDataDestinationDescription:"Define the data store as destination.",ChooseDataDescription:"Select, preview, and choose the data.",ConnectToDataDestinationLabel:"Connect to data destination",MapToDestinationDescription:"Select and map to {0}.",DataStoreTypeLabel:"Data store type",Connection_Settings:"Connection settings",CopyWizard_Connection_Search_No_Result:"Your search doesn't match any data sources.",CopyWizard_Connection_ErrorMessage_No_Type_Selected:"Please select a data store type.",CopyWizard_Connection_ErrorMessage_Invalid_Dataset_Selected:"Please select a valid dataset.",CopyWizard_Connection_ErrorMessage_None_Selected:"Please select a {0}.",CopyWizard_Connection_Existing_ErrorMessage_No_LinkedService_Selected:"At least one existing connection should be selected.",CopyWizard_Connection_TestingConnectionTemplate:"Testing connection to {0}. Please wait.",CopyWizard_Connection_FetchingMetadata:"Fetching metadata from control table. Please wait.",CopyWizard_Connection_TestingConnection:"Testing connection. Please wait.",CopyWizard_Connection_Properties_PageHeader:"Connection properties",CopyWizard_Connection_Properties_ErrorMessage_Metadata_NotFound:"Unsupported data store type.",DifferentSelfhostedIRError:"{0} and {1} in the copy activity must be connected via the same self-hosted integration runtime.",CopyWizard_Connection_SetParameter:"Set parameter values",CopyWizard_Connection_Source_DataStore_PageHeader:"Source data store",CopyWizard_Connection_Source_DataStore_PageDescription:"Specify the source data store for the copy task. You can use an existing data store connection or specify a new data store.",CopyWizard_Connection_Destination_DataStore_PageHeader:"Destination data store",CopyWizard_Connection_Destination_DataStore_PageDescription:"Specify the destination data store for the copy task. You can use an existing data store connection or specify a new data store.",CopyWizard_Dataset_Source_ExportJsonAsIs:"Export as-is to JSON files or Azure Cosmos DB collection",CopyWizard_Dataset_File_Source_PageHeader:"Choose the input file or folder",CopyWizard_Dataset_File_Http_PageHeader:"Specify HTTP dataset properties",CopyWizard_Dataset_File_Http_PageDescription:"",CopyWizard_Dataset_File_Source_PageDescription:"Select a source {0} to be copied to the destination data store.",CopyWizard_Dataset_File_Cosmos_StructuredStream_PageHeader:"Choose the input file",CopyWizard_Dataset_File_Cosmos_StructuredStream_PageDescription:"Select a source file to be copied to the destination data store.",CopyWizard_Dataset_File_Or_Folder_Label:"File or folder",CopyWizard_Dataset_File_Description:"Specify the path to the file.",CopyWizard_Dataset_File_Sink_PageHeader:"Choose the output file or folder",CopyWizard_Dataset_File_Sink_PageDescription:"Specify a folder that will contain output files or a specific output file in the destination data store.",CopyWizard_Dataset_File_Source_NotChooseFileError:"Please input or choose a {0}",CopyWizard_Dataset_File_Source_NotChooseFileError_1:"Please choose a {0}",CopyWizard_Dataset_File_PartitionInfo:"You can use variables in the folder path to copy data from/to a folder or a file that is determined at runtime. The supported variables are: {0}. Example: {1}.",CopyWizard_Dataset_File_PartitionStartTimeLabel:"Time to preview generated file path",CopyWizard_Dataset_File_Source_GeneratedFilePath:"Generated file path",CopyWizard_Dataset_File_Source_PrevieWSchemaPath:"Generated file path for preview and schema",CopyWizard_Dataset_File_Sink_TableNameEmpty_Error:"Table name cannot be empty.",CopyWizard_Dataset_File_Sink_NotSupportRootFolder:"Folder path cannot be empty - copying to root folder is not supported",CopyWizard_Dataset_File_Sink_FileNameBySource:"Filenames are defined by source",CopyWizard_Dataset_File_Sink_FileNameBySourceTable:"File name is defined by source table name",CopyWizard_Dataset_File_Sink_SubfolderNameBySourceTable:"Subfolder name is defined by source table name",CopyWizard_Dataset_File_Sink_EditFileNameOnebyOne:"Edit file names one by one",CopyWizard_Dataset_File_Sink_EditSubfolderNameOnebyOne:"Edit subfolder names one by one",CopyWizard_Dataset_File_Sink_FileNameSuffix:"File name suffix",CopyWizard_Dataset_File_Sink_FileNameSuffixDescription:"Suffix that will be appended to filenames. Only string interpolation expression (@{ ... }) is supported",CopyWizard_Dataset_File_Format_NotDetectDataTypeLabel:"Handle all of the data as strings",CopyWizard_Dataset_File_Format_SchemaEmptyError:"File schema is not defined",CopyWizard_Dataset_File_Filenames_Source:"Source entity name",CopyWizard_Dataset_File_Filenames_Destination:"Destination file name",CopyWizard_Dataset_File_SubfolderNames_Destination:"Destination subfolder name",CopyWizard_Dataset_File_Sink_FileName_ExpressionNotSupported:"File name only supports string interpolation expression (@{ ... })",CopyWizard_Dataset_Blob_Sink_ContainerValidationError:"Container name may only contain lowercase letters, numbers, and hyphens, and must begin with a letter or a number. Each hyphen must be preceded and followed by a non-hyphen character. The name must also be between 3 and 63 characters long.",CopyWizard_Dataset_ADLSGen2_Sink_ContainerValidationError:"File system name may only contain lowercase letters, numbers, and hyphens, and must begin with a letter or a number. Each hyphen must be preceded and followed by a non-hyphen character. The name must also be between 3 and 63 characters long.",CopyWizard_Dataset_File_Sink_FileSharePathValidationError:'Fileshare path is not valid. "\\", ":", "*", "?", ""","<", ">" and "|" are not allowed. Please use / for folder separation.',CopyWizard_Dataset_Tabular_PageHeader:"Select tables from which to copy the data or use a custom query.",CopyWizard_Dataset_Tabular_PageHeader_TableOnlyTemplate:"Select {0}s from which to copy the data",CopyWizard_Dataset_Tabular_PageHeader_QueryOnly:"Specify a custom query to copy the data",CopyWizard_Dataset_Tabular_WebTable_PageHeader:"Specify Web Table dataset properties",CopyWizard_Dataset_Tabular_NoTableAndQuery_PageHeader:"Specify dataset and copy source properties",CopyWizard_Dataset_Tabular_PageDescription:"You can select multiple tables, or you can provide single custom query.",CopyWizard_Dataset_Tabular_PageDescription_TableOnlyTemplate:"You can select multiple {0}s.",CopyWizard_Dataset_Tabular_Mode_Table:"Existing tables",CopyWizard_Dataset_Tabular_Mode_Label:"Source tables",LoadingLabel:"Loading",CopyWizard_Dataset_Tabular_TableFilter_Placeholder:"Filter by name...",CopyWizard_Dataset_Tabular_ViewFilter_Label:"Show views",CopyWizard_Dataset_Tabular_SelectedFilter_Label:"Show all selected items",CopyWizard_Dataset_Tabular_Showing_TableTemplate:"Showing {0} out of {1} items ({2} selected)",CopyWizard_Dataset_Tabular_Showing_TableViewTemplate:"Showing {0} out of {1} {2}s, {3} out of {4} views ({5} selected)",CopyWizard_Dataset_Tabular_ErrorMessage_NoTableSelected:"At least one table should be selected",CopyWizard_Dataset_Tabular_ErrorMessage_TableStructureRequired:"Table structure is required: {0}",CopyWizard_Dataset_Tabular_WebTable_Index_Required_Error:"Please specify index",CopyWizard_Dataset_Tabular_NotSupportPreviewMessage:"This connector does not support preview data.",CopyWizard_Dataset_Lakehouse_RootFolderIsDisabled_Tooltip:'Copy to "Tables" is not supported when schema agnostic (binary copy) is enabled.',Dataset_FileName_SpaceSlashValidation:'{0} name cannot contain "/" nor spaces.',MissingConnection_BannerMessage:"You cannot access one or more connections used in this data pipeline. You can either ask the data pipeline owner to grant permissions to all connections used in the data pipeline or choose other connections that you can access.",View_Template:"View {0}",Umask_Validation_Error:"Umask must be composed of three octal numbers.",CopyWizard_Dataset_Tabular_Filter_PageHeader:"Apply filter",CopyWizard_Dataset_Tabular_Filter_MetadataPageHeader:"Choose loading behavior",CopyWizard_Dataset_Tabular_Filter_MetadataPageDesc:"Select full load vs. delta load. Once selecting delta load, you need to specify the watermark column for each table used to identify the changes.",CopyWizard_Dataset_Tabular_Filter_BulkLoadAll:"Full load all tables",CopyWizard_Dataset_Tabular_Filter_ConfigueIndividual:"Configure for each table individually",CopyWizard_Dataset_Tabular_Filter_PageDescription:"Choose date time column that will be used to filter data or use a custom query.",CopyWizard_Dataset_Tabular_Filter_PageDescriptionTooltip:"If table that has been selected for copy contains columns of type 'DateTime' you can specify that column and time-filter query will be generated. Or you can write custom query using macro expressions.",CopyWizard_Dataset_Tabular_Filter_HowToFilter:"How to filter",CopyWizard_Dataset_Tabular_Filter_NoFilter:"No filter",CopyWizard_Dataset_Tabular_Filter_FilterByCustomQuery:"Filter by custom query",CopyWizard_Dataset_Tabular_Filter_FilterByDateTimeColumn:"Filter by datetime column",CopyWizard_Dataset_Tabular_Filter_GeneratedQuery:"Generated query",CopyWizard_Dataset_Tabular_Filter_DateTimeColumn:"DateTime column",CopyWizard_Dataset_Tabular_Filter_NoDateTimeColumn:"No datetime column",CopyWizard_Dataset_Tabular_Filter_NoDateTimeIdColumn:"No datetime or ID column",CopyWizard_Dataset_Tabular_Filter_ErrorMessage_FilterInvalid:"One or more configured filter is invalid",CopyWizard_Dataset_Tabular_Loading_Behavior:"Loading behavior",CopyWizard_Dataset_Tabular_Delta_Loading:"Delta load",CopyWizard_Dataset_Tabular_Delta_WatermarkName:"Watermark column name",CopyWizard_Dataset_Tabular_Delta_WatermarkStartValue:"Watermark column value start",CopyWizard_MetadataSolution_ConcurrentRuns:"Number of concurrent copy tasks",CopyWizard_MetadataSolution_ConcurrentRunsDesc:"You can decide the max number of concurrent copy activity run in order to control the load impact on your source store. The default value is 20.",CopyWizard_Dataset_Tabular_Delta_LoadErrormessage:"Delta load for table {0} requires a watermark column name and watermark start value.",CopyWizard_Dataset_Tabular_NonExistedErrormessageTemplate:"Tables don't exist in the source data store: '{0}'.",QueryBuilderLabel:"Query builder",UseQueryBuilderLabel:"Use query builder",ManuallyWriteQueryLabel:"Write query manually",QueryOptionDesc:"Use the query builder below to filter control table items for this editing session or write your query manually.",CopyWizard_HasParameterInSecretFieldsErrorMessage:"Parameterizing passwords or secrets in {0} store is not supported in metadata-driven copy tasks. Please store all connection strings in Azure Key Vault instead, and parameterize the Secret Name.",CopyWizard_ViewPipelineText:"View pipeline + debug pipeline run",CopyWizard_RunScriptTextTemplate:"Please run the following SQL script in your SQL server to {0} a control table. Then view your pipeline to execute a debug run.",CopyWizard_GeneratedSqlScriptForControlTable:"Generated SQL script for control table",CopyWizard_GeneratedSqlScriptForSP:"Generated SQL script for stored procedure",CopyWizard_Dataset_Preview_NoInput:"Select a table or input a query to see its preview.",CopyWizard_Dataset_Preview_NoInput_1:"Select the data from the side panel to preview it",CopyWizard_Dataset_Preview_NoInput_TableOnlyTemplate:"Select a {0} to see its preview",CopyWizard_Dataset_Preview_NoInput_QueryOnly:"Input a query to see its preview",CopyWizard_Dataset_Preview_ClickPreviewDataButton:"Click Preview data button to see its preview and schema",CopyWizard_Dataset_Preview_Loading:"Loading preview",CopyWizard_Dataset_Schema_NoInput:"Select a table or input a query to see its schema.",CopyWizard_Dataset_Schema_NoInput_TableOnlyTemplate:"Select a {0} to see its schema",CopyWizard_Dataset_Schema_NoInput_QueryOnly:"Input a query to see its schema.",CopyWizard_Dataset_WebTable_Schema_NoInput:"Specify an index to see its schema.",CopyWizard_Dataset_WebTable_Preview_NoInput:"Specify an index to see its preview.",CopyWizard_Dataset_Schema_Loading:"Loading schema",CopyWizard_Dataset_PreviewSchema_Loading:"Loading preview and schema. Please wait",DestinationType:"Destination type",CopyWizard_Parameters_Definition_Title:"Pipeline parameters definition",CopyWizard_Parameters_SampleValue_Title:"Trigger system variables sample value",CopyWizard_Parameters_SampleValue_Description:"Values here are only used to validate query. The runtime value will be generated by trigger.",Table_name_Label:"Table name",Table_count:"Number of tables",CopyWizard_Dataset_Query_SelectTableText:"-Select table-",CopyWizard_Dataset_Query_ErrorMessage_NoQuery:"Please input a query to validate",CopyWizard_Dataset_Query_ErrorMessage_NotExistingTableName:"Destination table doesn't exist in the data store, please update table name or select ",CopyWizard_Dataset_Query_ErrorMessage_ParameterNotReplaced:"Some parameters in query template have not been replaced: {0}",CopyWizard_Dataset_Query_InvalidExpression:"Invalid expression for query",CopyWizard_Dataset_ParseExpressionException:"Error happened when parsing expression:",CopyWizard_Dataset_TableMapping_PageHeader:"Table mapping",CopyWizard_Dataset_TableMapping_PageDescription:"For each table you have selected to copy in the source data store, select a corresponding table in the destination data store or specify the stored procedure to run at the destination.",CopyWizard_Dataset_TableMapping_Source_CustomQuery:"<Custom query>",CopyWizard_Dataset_TableMapping_DynamicProperties:"<Dynamic properties>",CopyWizard_Dataset_TableMapping_SkipColumnMapping:"Skip column mapping",CopyWizard_Dataset_TableMapping_SkipColumnMappingForAllTables:"Skip column mapping for all tables",CopyWizard_Dataset_TableMapping_SkipSchemaMappingForAllTables:"Skip schema mapping for all tables",CopyWizard_Dataset_TableMapping_UseExistingTable:"Use existing table",CopyWizard_Dataset_TableMapping_AutoCreateTable:"Auto-create a destination table with the source schema",CopyWizard_Dataset_TableMapping_AutoCreateLabel:"auto-create",CopyWizard_Dataset_TableMapping_SelectText:"-Select-",CopyWizard_Dataset_TableMapping_UseStoredProcedure:"Use stored procedure",CopyWizard_Dataset_TableMapping_ErrorMessage_Template:"Table mapping for {0} has not been properly configured: {1}",CopyWizard_Dataset_ErrorMessage_NoMappingSelected:"At least one table or view should be selected",CopyWizard_Dataset_TableMapping_ErrorMessage_AutoCreateTableExists:"Table '{0}' already exists",CopyWizard_Dataset_TableMapping_ErrorMessage_AutoCreateTableNameInvalid:"Table name '{0}' is invalid",CopyWizard_Dataset_TableMapping_ErrorMessage_AutoCreateTableNameReserved:"Table name '{0}' is reserved",CopyWizard_Dataset_TableMapping_ErrorMessage_AutoCreateTableRequired:"Table name is required",CopyWizard_Dataset_TableMapping_ErrorMessage_StoredProcedureNoTableType:"Invalid stored procedure for sink. No user defined table type.",CopyWizard_Dataset_TableMapping_ErrorMessage_NotSupportMultipleItems:"{0} only supports single table mapping, please remove extra table selection in source data store",CopyWizard_Dataset_TableMapping_ErrorMessage_NotSupportSqlDwSinkView:"Only table is supported, view is not supported.",CopyWizard_Dataset_ColumnMapping_PageHeader:"Column mapping",CopyWizard_Dataset_SchemaMapping_PageHeader:"Schema mapping",CopyWizard_Dataset_SchemaMapping_PageDescription:"Choose how source and destination columns are mapped",CopyWizard_Dataset_SchemaMapping_TableMappings:"Table mappings",CopyWizard_Dataset_SchemaMapping_ColumnMappings:"Column mappings",CopyWizard_Dataset_SchemaMapping_SourceColumnName:"Source column name",CopyWizard_Dataset_SchemaMapping_DestinationColumnType:"Destination column type",CopyWizard_Dataset_SchemaMapping_IncludeThisColumn:"Include this column",CopyWizard_Dataset_SchemaMapping_SelectColumn:"-Select a column-",CopyWizard_Dataset_SchemaMapping_ColumnMappingSkipped:"You have already selected '{0}' in previous page, so that the column mapping disappeared here.",CopyWizard_Dataset_SchemaMapping_RepeatabilitySettings:"Repeatability settings",CopyWizard_Dataset_SchemaMapping_RepeatabilitySettings_Description:"Repeatability is needed to prevent data duplication during copy operation",CopyWizard_Dataset_SchemaMapping_SinkProperties_Template:"{0} sink properties",CopyWizard_Dataset_SchemaMapping_WarningMessage_ColumnHasWarning:"Column mapping has warning",CopyWizard_Dataset_SchemaMapping_ErrorMessage_ColumnMappingNotInitialized:"Column mapping is not initialized",CopyWizard_Dataset_SchemaMapping_ErrorMessage_ColumnMappingsNotInitialized:"One or more column mappings are not initialized",CopyWizard_Dataset_SchemaMapping_ErrorMessage_IncompatibleColumnTypes:"Incompatible or unsupported column types",CopyWizard_Dataset_SchemaMapping_ErrorMessage_ColumnNotMapped:"Column is not mapped",CopyWizard_Dataset_SchemaMapping_ErrorMessage_InvalidColumnName:"Invalid column name",CopyWizard_Dataset_SchemaMapping_ErrorMessage_ErrorNeedFix:"Please fix the errors",CopyWizard_Dataset_SchemaMapping_ErrorMessage_DynamicErroFormat:"Please fix the errors in ",CopyWizard_Dataset_SchemaMapping_ErrorMessage_NoColumnMappingSelected:"At least one column mapping should be selected",CopyWizard_Dataset_SchemaMapping_ErrorMessage_ColumnDuplicatelyMapped:"One or more columns are duplicately mapped",CopyWizard_Dataset_SchemaMapping_ErrorMessage_AzureTableNoPartitionKeyName:"Column name of partition key is not specified or doesn't exist in sink or source columns.",CopyWizard_Dataset_SchemaMapping_ErrorMessage_AzureTableNoRowKeyName:"Column name of row key is not specified or doesn't exist in sink or source columns.",CopyWizard_Dataset_SchemaMapping_PreCopyScriptDescription:"You could use the following parameters in your Pre-copy script. Leave them alone if you don't need them.",CopyWizard_Dataset_SchemaMapping_UnderTableMappingSection:"Under table mapping section",CopyWizard_Dataset_SchemaMapping_ErrorMessage_ErrorNeedFix_From:"Please fix the errors for source table '{0}': {1}",CopyWizard_Settings_FaultToleranceSettings:"Fault tolerance settings",CopyWizard_Settings_PerformanceSettings:"Performance settings",CopyWizard_Settings_PolybaseError:"Please correct PolyBase error",CopyWizard_Settings_ParallelCopyError:"Please correct parallel copy error",CopyWizard_Settings_DataMovementUnitError:"Please correct data movement unit error",CopyWizard_Settings_PageDescription:"More options for data movement",CopyWizard_Settings_UnloadStagingEnabledValidation_Template:"Unload and staging can be enabled at the same time only when copying data from Amazon Redshift to {0} with PolyBase.",CopyWizard_Settings_ResourceManagerEndpointMissing:"The ResourceManager endpoint is required",CopyWizard_Settings_TempScriptMissing:"The Temp script path is required",CopyWizard_Settings_TestConnection_ForStaging:"Test connection for staging. Please wait",CopyWizard_Settings_TestConnection_ForLogging:"Test connection for logging. Please wait",CopyWizard_Settings_TestConnnection_ForFaultTolerance:"Test connection for fault tolerance. Please wait",CopyWizard_Settings_TestConnectionFailed_ForLsTemp:"Test connection failed for {0} linked service: ",CopyWizard_Settings_TestConnectionFailed_ForFaultTolerance:"Test connection failed for fault tolerance storage connection: ",CopyWizard_Summary_DeployModeLabel:"Deploy mode",CopyWizard_Summary_DeployModeOption_DeployAndRun:"Deploy and run",CopyWizard_Summary_DeployModeOption_DeployOnly:"Deploy only",CopyWizard_Summary_PageDescriptionTemplate:"You are running pipeline to copy data from {0} to {1}.",CopyWizard_Summary_UpdateControlTable_PageDescriptionTemplate:"You are updating control table for pipeline {0}.",CopyWizard_Review_NavTitle:"Review and finish",CopyWizard_ReviewLabel:"Review",CopyWizard_TypeConversion_PolybaseError:"Polybase cannot be used together with type conversion, disable polybase or go back to the column mapping page to disable type conversion.",CopyWizard_TypeConversion_CopyCommandError:"Copy command cannot be used together with type conversion, disable copy command or go back to the column mapping page to disable type conversion.",CopyWizard_TypeConversion_StagingIRError:"Staging self-hosted integration runtime (IR) version doesn't meet the requirement of type conversion, upgrade IR or disable staging or go back to the column mapping page to disable type conversion.",CopyWizard_Deployment_NavTitle:"Deployment",CopyWizard_Deployment_StepLabel:"Deployment step",CopyWizard_Deployment_Deploying:"Deploying",CopyWizard_Operation_Complete_Template:"{0} complete",CopyWizard_Operation_Failed_Template:"{0} failed",CopyWizard_Operation_Skipped_Template:"{0} skipped",CopyWizard_Deployment_CreatingLinkedServices:"Creating connections",CopyWizard_Deployment_CreatingDatasets:"Creating datasets",CopyWizard_Deployment_CreatingPipelines:"Creating pipelines",CopyWizard_Deployment_CreatingRuns:"Running pipelines",CopyWizard_Deployment_CreatingTriggers:"Creating triggers",CopyWizard_Deployment_StartingTriggers:"Starting triggers",CopyWizard_Deployment_GeneratingSQLScript:"Generating SQL scripts",CopyWizard_Deployment_ResourceCreatedDescription:"Datasets and pipelines have been created. You can now monitor and edit the copy pipelines or click finish to close Copy Data Tool.",CopyWizard_Deployment_EditPipeline:"Edit pipeline",CopyWizard_Validation_ValidateRuntime:"Validating copy runtime environment",CopyWizard_Validation_SkipError:"Skip validation",CopyWizard_DisabledInSynapseGit:"Copy data tool is only available in Synapse live mode.",CopyWizard_Description_Short:"Use Copy Data Tool to author a one-time or scheduled pipeline to load data from 90+ data sources.",CopyWizard_Description_Long:"Use Copy Data Tool to perform a one-time or scheduled data load from 90+ data sources.\nFollow the wizard experience to specify your data loading settings, and let the Copy Data Tool generate the artifacts for you, including pipelines, datasets, and linked services.",CopyWizard_EditModeDescription_Long:"Use Copy Data Tool to perform controle table updating. \nFollow the wizard experience to modify items in your control table, and let the Copy Data Tool generate the SQL script for you.",CopyWizard_Summary_Setting:"Copy settings",CopyWizard_ConnectionName:"Connection name",ScriptFlow_Diagram_Heading:"Diagram",ScriptFlow_NodeHighlighting_Label:"Node highlighting",ScriptFlow_NodeHighlighting_Tooltip:"When a line in the script is clicked on, highlight its corresponding node in the diagram.",ScriptFlow_HideLabels_Label:"Hide labels",ScriptFlow_HideLabels_Tooltip:"To reduce clutter, hide statement labels.",ScriptFlow_RedrawDiagram:"Redraw diagram",ScriptFlow_APIError:"Parse API call failed with error code",Discard:"Discard",DiscardAll:"Discard all",DiscardAllTooltip:"Discard all changes to resources",Discarding:"Discarding",GitHubRepoNotAuthorized:"You are not authorized to access this repo",GitHubNotAuthorizedDescription:"The data factory is configured to use GitHub repository: {0}. Please provide GitHub personal access token for authorization. The token should have repo scope access. If your repository belongs to an organization that uses single sign-on, you must authorize the token.",GitHubLogInNotAuthorizedDescription:"The {0} is configured to use GitHub repository: {1}. If you want to use your GitHub repository, please log in.",GitHubNotAuthorizedNoRepoDescription:"Please provide GitHub personal access token for authorization. The token should have repo scope access. If your repository belongs to an organization that uses single sign-on, you must authorize the token.",GitHubLogInNotAuthorizedNoRepoDescription:"If you want to use your GitHub repository, please log in.",UseAdfMode:"Use ADF mode",UseWorkspaceMode:"Use workspace mode",GitHubLogIn:"Log into GitHub",RetrieveGitHubAccessToken:"Retrieve GitHub Access Token",ConfigureGitHubToken:"Configure GitHub token",GitHubPersonalAccessToken:"Personal access token",GitHubTokenSavingLabel:"Save token",GitHubTokenDoNotSave:"Don't save",GitHubTokenSaveInSessionStorage:"Save until browser close",GitHubTokenSaveInLocalStorage:"Save until cache is cleared",GitHubTokenInvalidPattern:"The personal access token is invalid",GitHubTokenInvalid:"The personal access token is invalid. Please provide a valid token.",GitHubTokenInvalidByoa:"Failed to fetch personal access token. Please confirm the BYOA client id and secret are valid and that ADF has been granted access to the Azure Key Vault.",GitHubHostLabel:"GitHub Enterprise Server URL",GitHubHostDescription:"Select the GitHub Enterprise Server root URL, for example, https://github.mydomain.com. Do not use this option with GitHub Enterprise Cloud.",GitHubHostInvalidPattern:"The GitHub Enterprise Server URL is invalid, URL must start with http or https. For example: https://github.mydomain.com",DevopsLinkInvalidPattern:"The Dev Ops repository link is invalid, URL must start with http or https and be in DevOps format. For example: https://account.visualstudio.com/project/_git/repository",GitHubAccountLabel:"GitHub account",GitHubRepositoryOwner:"GitHub repository owner",GitHubAccountDescription:"GitHub organization or account that owns the repository. Ex. for the repository https://github.com/contoso/contoso-ads, the owner is contoso.",GitImportResourcesDescription:"Select the branch where you will import existing resources from live environment. This can be the main branch or an existing branch.",GithubValidateTokenHeader:"GitHub Token Validation",GithubRepoAccessValidationHeader:"GitHub Repo Access Validation",UnknownError:"Unknown Error",GitHubCodeIncorrectOrExpired:"The access code is incorrect or expired. Please provide a valid GitHub access code",ServiceUnauthorized:"Permission denied",GitHubServerError:"GitHub Server Error",GitHubListRepoValidationFailure:"Failed to list GitHub repositories. Please make sure account name is correct and you have permission to perform the action.",GitHubCreateBranchFailedWithConflict:"Failed to create new branch. Please make sure the repository is initialized in GitHub. Reason: {0}",GitHubResourceNotFoundOrUnauthorized:"Requested GitHub resource not found or you don't have permission to perform action: {0}.",GitHubRepoNotFoundOrUnauthorized:"GitHub repository not found or you don't have read permission to it",GitHubByoaDescription:"To use GitHub in this region, you must create your own GitHub application and provide the client ID and secret to ADF.",GitHubByoaClientIdLabel:"GitHub OAuth app client ID",GitHubClientId:"GitHub client ID",GitHubByoaClientId:"BYOA client ID",GitHubByoaClientSecretAkvUrl:"BYOA client secret AKV URL",GitHubByoaClientSecretName:" BYOA client secret name",GitHubByoaAkvClientSecretUrl:"GitHub client secret AKV URL",GitHubByoaAkvClientSecretUrlLabel:"AKV URL for secret containing GitHub OAuth app client secret",GitHubByoaAkvClientSecretNameLabel:"AKV secret name",GovernmentCloudByoa:"Government Cloud BYOA",KeyVaultSecretUrlValidationError:"Key vault secret URL must be of the form https://<vault-name>.vault.azure.net/secrets/<secret-name>",Verify:"Verify",DataConsistencyNotSupported:'Data consistency verification is not supported for copy "{0}"',SkipMissingFileNotSupported:'Skip missing file is not supported for copy "{0}"',SkipForbiddenFileNotSupported:'Skip forbidden file is not supported for copy "{0}"',AzureKeyVaultValidatorErrorMessage:"Secret name and linked service are required for Azure Key Vault.",AuthenticationType_OAuth2ClientCredential:"OAuth2 Client Credential",AuthenticationType_ServicePrincipal:"Service Principal",AuthenticationType_AccountKey:"Account Key",AuthenticationType_ManagedServiceIdentity:"System Assigned Managed Identity",AuthenticationType_UserAssignedManagedIdentity:"User Assigned Managed Identity",AuthenticationType_SqlOrManagedServiceIdentity:"Sql Authentication or Managed Identity",AuthenticationType_SAS:"Shared access signature (SAS)",ServiceIdentityApplicationId_DisplayText:"Managed identity application ID: ",ServiceIdentityObjectId_DisplayText:"Managed identity object ID: ",ServiceIdentityName_DisplayText:"Managed identity name: ",ServiceIdentityApplicationId_GrantAccessText:"Grant {0} service managed identity access to your {1}.",UserAssignedIdentityApplicationId_GrantAccessText:"Grant {0} user assigned managed identity access to your {1}.",Auth:"Auth",Timezone_DST:"International Date Line West (UTC-12)",Timezone_UTC11:"Coordinated Universal Time-11 (UTC-11)",Timezone_Hawaii:"Hawaii (UTC-10)",Timezone_Alaska:"Alaska (UTC-9)",Timezone_Baja:"Baja California (UTC-8)",Timezone_PST:"Pacific Time (US & Canada) (UTC-8)",Timezone_Arizona:"Arizona (UTC-7)",Timezone_Chihuahua:"Chihuahua, La Paz, Mazatlan (UTC-7)",Timezone_Mountain:"Mountain Time (US & Canada) (UTC-7)",Timezone_CentralAmerica:"Central America (UTC-6)",Timezone_CST:"Central Time (US & Canada) (UTC-6)",Timezone_Gaudalajara:"Guadalajara, Mexico City, Monterrey (UTC-6)",Timezone_Saskatchewan:"Saskatchewan (UTC-6)",Timezone_SPST:"Bogota, Lima, Quito (UTC-5)",Timezone_EST:"Eastern Time (US & Canada) (UTC-5)",Timezone_VST:"Caracas (UTC-4:30)",Timezone_PYT:"Asuncion (UTC-4)",Timezone_ADT:"Atlantic Time (Canada) (UTC-4)",Timezone_CBST:"Cuiaba (UTC-4)",Timezone_SWST:"Georgetown, La Paz, Manaus, San Juan (UTC-4)",Timezone_PSST:"Santiago (UTC-4)",Timezone_NDT:"Newfoundland (UTC-3:30)",Timezone_ESAST:"Brasilia (UTC-3)",Timezone_AST:"Buenos Aires (UTC-3)",Timezone_SEST:"Cayenne, Fortaleza (UTC-3)",Timezone_GDT:"Greenland (UTC-3)",Timezone_MST:"Montevideo (UTC-3)",Timezone_BST:"Salvador (UTC-3)",Timezone_UTC2:"Coordinated Universal Time-02 (UTC-2)",Timezone_MDT:"Mid-Atlantic (UTC-2)",Timezone_Azores:"Azores (UTC-1)",Timezone_CVST:"Cape Verde Is. (UTC-1)",Timezone_UTC:"Coordinated Universal Time (UTC)",Timezone_Casablanca:"Casablanca (UTC+1)",Timezone_GMT:"Dublin, Edinburgh, Lisbon, London (UTC+0)",Timezone_GST:"Monrovia, Reykjavik (UTC+0)",Timezone_WEDT:"Amsterdam, Berlin, Bern, Rome, Stockholm, Vienna (UTC+1)",Timezone_CEDT:"Belgrade, Bratislava, Budapest, Ljubljana, Prague (UTC+1)",Timezone_RDT:"Brussels, Copenhagen, Madrid, Paris (UTC+1)",Timezone_Sarajevo:"Sarajevo, Skopje, Warsaw, Zagreb (UTC+1)",Timezone_WCAST:"West Central Africa (UTC+1)",Timezone_Namibia:"Windhoek (UTC+1)",Timezone_GTB:"Athens, Bucharest (UTC+2)",Timezone_MEDT:"Beirut (UTC+2)",Timezone_Egypt:"Cairo (UTC+2)",Timezone_Syria:"Damascus (UTC+2)",Timezone_EEDT:"E. Europe (UTC+2)",Timezone_SAST:"Harare, Pretoria (UTC+2)",Timezone_FDT:"Helsinki, Kyiv, Riga, Sofia, Tallinn, Vilnius (UTC+2)",Timezone_Turkey:"Istanbul (UTC+3)",Timezone_Israel:"Jerusalem (UTC+2)",Timezone_Libya:"Tripoli (UTC+2)",Timezone_Jordan:"Amman (UTC+3)",Timezone_Arabic:"Baghdad (UTC+3)",Timezone_Kalinigrad:"Kaliningrad (UTC+2)",Timezone_Arab:"Kuwait, Riyadh (UTC+3)",Timezone_EAST:"Nairobi (UTC+3)",Timezone_Minsk:"Minsk (UTC+3)",Timezone_MSK:"Moscow, St. Petersburg, Volgograd (UTC+3)",Timezone_SAMT:"Samara, Ulyanovsk, Saratov (UTC+4)",Timezone_Iran:"Tehran (UTC+3:30)",Timezone_Arabian:"Abu Dhabi, Muscat (UTC+4)",Timezone_Azerbaijan:"Baku (UTC+4)",Timezone_Mauritius:"Port Louis (UTC+4)",Timezone_Georgian:"Tbilisi (UTC+4)",Timezone_Caucasus:"Yerevan (UTC+4)",Timezone_Afghan:"Kabul (UTC+4:30)",Timezone_WAST:"Ashgabat, Tashkent (UTC+5)",Timezone_Pakistan:"Islamabad, Karachi (UTC+5)",Timezone_IST:"Chennai, Kolkata, Mumbai, New Delhi (UTC+5:30)",Timezone_SLST:"Sri Jayawardenepura (UTC+5:30)",Timezone_Nepal:"Kathmandu (UTC+5:45)",Timezone_CAST:"Astana (UTC+6)",Timezone_Bangladesh:"Dhaka (UTC+6)",Timezone_Ekaterinburg:"Ekaterinburg (UTC+5)",Timezone_Myanmar:"Yangon (Rangoon) (UTC+6:30)",Timezone_SEAsia:"Bangkok, Hanoi, Jakarta (UTC+7)",Timezone_NCAST:"Novosibirsk (UTC+7)",Timezone_NAST:"Krasnoyarsk (UTC+7)",Timezone_China:"Beijing, Chongqing, Hong Kong, Urumqi (UTC+8)",Timezone_MPST:"Kuala Lumpur, Singapore (UTC+8)",Timezone_Perth:"Perth (UTC+8)",Timezone_Taipei:"Taipei (UTC+8)",Timezone_Ulaanbaatar:"Ulaanbaatar (UTC+8)",Timezone_NAEST:"Irkutsk (UTC+8)",Timezone_Tokyo:"Osaka, Sapporo, Tokyo (UTC+9)",Timezone_KST:"Seoul (UTC+9)",Timezone_Adelaide:"Adelaide (UTC+9:30)",Timezone_ACST:"Darwin (UTC+9:30)",Timezone_Brisbane:"Brisbane (UTC+10)",Timezone_AEST:"Canberra, Melbourne, Sydney (UTC+10)",Timezone_WPST:"Guam, Port Moresby (UTC+10)",Timezone_TST:"Hobart (UTC+10)",Timezone_YST:"Yakutsk (UTC+9)",Timezone_CPST:"Solomon Is., New Caledonia (UTC+11)",Timezone_Vladivostok:"Vladivostok (UTC+10)",Timezone_NZST:"Auckland, Wellington (UTC+12)",Timezone_UTC12:"Coordinated Universal Time+12 (UTC+12)",Timezone_Fiji:"Fiji (UTC+12)",Timezone_Magadan:"Magadan (UTC+11)",Timezone_KDT:"Petropavlovsk-Kamchatsky (UTC+12)",Timezone_Tonga:"Nuku'alofa (UTC+13)",Timezone_SST:"Samoa (UTC+13)",Time_Year:"year",Time_Day:"day",Time_Hour:"hour",Time_Minute:"minute",Time_Second:"second",Time_Ms:"ms",PipelineExpressionBuilderTitle:"Pipeline expression builder",DataflowExpressionBuilderTitle:"Dataflow expression builder",DataFlowExpressionBuilderExpressionHeader:'Expression for field "{0}"',DataFlowExpressionBuilderSaveBeforeSubcolumnsAvailable:"Subcolumn expressions cannot be edited if there are unsaved changes in a parent expression",DataFlowExpressionBuilderDiscardExpression:"Discard Expression",DataFlowExpressionBuilderDiscardExpressionDetails:"Adding a subcolumn will delete the current expression. Do you want to continue?",DataFlowExpressionBuilderFullScreen:"Full screen",DataFlowExpressionBuilder_CachedLookupWarning:"Cache sinks connected to this stream cannot be referenced here.",DataFlowExpressionBuilder_DeleteConfirmation:'Are you sure you want to remove: "{0}"?',DataFlowExpressionBuilder_ColumnPatternMatchingConditionDescription:"A boolean expression that matches columns based on the name, type, stream and position of the column. The column pattern will derive new columns based upon each column that is matched.",DataFlowExpressionBuilder_ColumnPatternNameExpressionDescription:"The expression that determines the output column name for each column that is derived. '$$' references the string column name value for the matched column.",DataFlowExpressionBuilder_ColumnPatternValueExpressionDescription:"The expression that determines the output column value for each column that is derived. '$$' references the existing column value for the matched column.",DataFlowExpressionReferenceDocumentation:"Expression reference documentation",DataFlowComputeSizeDescription:"Size of compute used in the spark cluster",DataFlowComputeTypeDescription:"Type of compute used in the spark cluster",DataFlowCoreCountDescription:"Number of cores used in the spark cluster",DataFlowCustomPropertiesLabel:"Compute Custom Properties",DataFlowCustomPropertiesLabelDescription:'Compute custom properties as property name or value. Click here to know more,  <a target="_blank" href="{0}">Learn more</a>',DataFlowLoggingLevelDescription:"Toggle the amount of monitoring output by the data flow activity. Verbose is the default setting that shows the partitioning information and rows output for every transformation. Basic doesn't return partitioning information. None only returns the source rows read and sink rows loaded. Less monitoring output can lead to improved performance",TransformationLabel:"Transformation",AddColumnLabel:"Add column",AddPartitionColumnLabel:"Add partition column",AddColumnPatternLabel:"Add column pattern",AddSubcolumnLabel:"Add subcolumn",AddColumnExpressionLabel:"Add column expression",DeleteColumnExpressionLabel:"Delete column expression",AddMappingLabel:"Add mapping",FixedMappingLabel:"Fixed mapping",RuleBasedMappingLabel:"Rule-based mapping",ValidatorErrorMessageRegularExpression:"Invalid regular expression",ValidatorErrorMessageEmptyField:"Value cannot be empty",AddPatternMatchingConditionLabel:"Add pattern matching condition",NameMatchesLabel:"Name matches",NameMatchesLabelDescription:"Provide a regex to match column names.",DeepHierarchyLevel:"Deep column traversal",DeepHierarchyLevelDescription:"Handle all subcolumns of a complex object individually instead of handling the complex object as a whole column.",RemoveMappingLabel:"Remove mapping",AddSchema:"Add schema",AddSchemaTooltip:"Add source/target table schema and enable column mappings for the selected CDC mapping(s)",RemoveSchema:"Remove schema",RemoveSchemaTooltip:"Remove source/target table schema and disable column mappings for the selected CDC mapping(s)",InputStreamLabel:"Input stream",OutputStreamLabel:"Output stream",DataFlowSourceLabel:"Data flow source",DataFlowSourcesLabel:"Data flow sources",DataFlowSinkLabel:"Data flow sink",DataFlowSinksLabel:"Data flow sinks",DataFlowWildcardPaths:"Wildcard paths",DataFlowMoveFiles:"Move from/to",DataFlowWildcardPathsDescription:"All files matching the wildcard path will be processed. This setting will override the folder path set in the dataset.\n\n* - any set of characters\n** - recursive directory nesting",DataFlowSettingsUseCustomSinkOrderingLabel:"Custom sink ordering",DataFlowSettingsUseCustomSinkOrderingDescription:"Designate sinks to be written sequentially in the specified order. Sinks with the same order value will still be written sequentially. Sinks without a write order will be written last, and their orders may vary.",DataFlowSettingsSinkOrderingEmptyMessage:"Add data flow sinks to enable write ordering",DataFlowDynamicWriteOrderLabel:"Dynamic write order",DataFlowWriteOrderLabel:"Write order",DataFlowSinkGroupLabel:"Sink Group",DataFlowSinkOrderingWarning:"Some sinks are not ordered correctly in their group. Found an issue with the ordering for {0}. Ensure that a sink order does not overlap a different sink group",DataFlowSinkOrderingCacheSinkWarning:"{0} is a cache sink and must be ordered first in a group",DeriveTransformationUILabel:"Derived Column",DeriveTransformationDescription:"Compute new columns based on existing ones",DeriveTransformationTabName:"Derived column's settings",DeriveTransformationDerivedColumn:"derived column",CallTransformationUILabel:"External Call",CallTransformationDescription:"Make a query or call procedures against an external data store",CallTransformationTabName:"Call transformation's settings",CallTransformationRESTOutputHeader:"Header",CallOutputBodyNameAs:"Name body as",CallOutputHeadersNameAs:"Name headers as",CallOutputStatusNameAs:"Name status as",CallTransformationSQLOutputHeader:"Output columns definition",CallTransformationRESTIncludeHeaderDesc:"Check to provide response body name and type",CallTransformationRESTIncludeBodyDesc:"Check to provide response header name",CallTransformationRESTIncludeStatusDesc:"Check to provide response status name",CallTransformationAdvancedSettingsBanner:"Additional headers, Query parameters and Pagination rules have recently moved into Advanced.",CallTransformationRequestSetting:"Request",CallTransformationResponseSetting:"Response",AggregateTransformationAggregateColumn:"aggregate column",WindowTransformationWindowColumn:"window column",PivotTransformationExpressionColumn:"pivot expression",AggregateTransformationGroupByColumn:"group by column",FoldDownTransformationUnrollBy:"Unroll by",FoldDownTransformationUnrollByDescription:"Choose the array in your data to flatten",FoldDownTransformationUnrollRoot:"Unroll root",FoldDownTransformationUnrollRootDescription:"Choose the level of the hierarchy to include in the flatten operation. Default is to unroll all arrays up to your chosen root",FoldDownTransformationUnrollRootPlaceholder:"Unroll root (optional)",PivotTransformationGroupByTabName:"1. Group by",PivotTransformationPivotKeyTabName:"2. Pivot key",PivotTransformationPivotedColumnsTabName:"3. Pivoted columns",PivotTransformationPivotKey:"Pivot key",PivotTransformationValuePlaceholder:"Enter value",PivotTransformationValuePlaceholderOptional:"Enter value (optional)...",PivotTransformationColumnNamePattern:"Column name pattern",PivotTransformationColumnNamePatternColumnFirst:"prefix{expression prefix}middle{Pivot key value}suffix",PivotTransformationColumnNamePatternValueFirst:"prefix{Pivot key value}middle{expression prefix}suffix",PivotTransformationColumnArrangement:"Column arrangement",PivotTransformationColumnArrangementNormal:"Normal",PivotTransformationColumnArrangementNormalDescription:"All new columns generated from a single value will be together",PivotTransformationColumnArrangementLateral:"Lateral",PivotTransformationColumnArrangementLateralDescription:"All new columns generated from an existing column will be together",PivotTransformationEnterColumnPrefix:"Enter a column prefix...",PivotTransformationEnterColumnPrefixOptional:"Enter a column prefix (optional)...",PivotTransformationExpression:"Pivot expression...",PivotTransformationMiddle:"Middle",PivotTransformationSuffix:"Suffix",PivotTransformationPivotKeyColumnRequired:"{0} key column is required",PivotTransformationPivotKeyValueRequired:"{0} key should have at least one value",UnpivotTransformationGroupByTabName:"1. Ungroup by",UnpivotTransformationPivotKeyTabName:"2. Unpivot key",UnpivotTransformationPivotedColumnsTabName:"3. Unpivoted columns",UnpivotTransformationPivotedColumns:"Unpivoted columns",UnpivotTransformationUnpivotColumnName:"Unpivot column name",UnpivotTransformationUnpivotColumnType:"Unpivot column type",UnpivotTransformationValuesOption:"Option",UnpivotTransformationPickColumnsAsValues:"Pick column names as values",UnpivotTransformationEnterValues:"Enter values",UnpivotTransformationDropNullRows:"Drop rows with null",UnpivotTransformationDropNullRowsDescription:"Rows with null value in all unpivot columns for a specific value will be dropped",UnpivotTransformationUPIV012:"Unpivoting will leave {0} column(s) orphaned",UnpivotTransformationUPIV08:"{0} is not of the same unpivot datatype",UnpivotTransformationUPIV011:"Duplicate column name(s) {0} found",WindowTransformationOverTabName:"1. Over",WindowTransformationSortTabName:"2. Sort",WindowTransformationRangeByTabName:"3. Range by",WindowTransformationColumnsTabName:"4. Window columns",WindowTransformationRange:"Range",WindowTransformationRangeByOffset:"Range by current row offset",WindowTransformationRangeByValue:"Range by column value",WindowTransformationRangeStartAt:"Start {0}",WindowTransformationRangeEndAt:"End {0}",WindowTransformationRangeEnterPlaceholder:"Enter {0}",WindowTransformationRangeOffset:"offset",WindowTransformationRangeRow:"row",WindowTransformationRangeValue:"value",WindowTransformationRangeInFront:"In front",WindowTransformationRangeBehind:"Behind",WindowTransformationRangeCurrent:"Current {0}",WindowTransformationRangeBehindCurrent:"Behind the current {0}",WindowTransformationRangeInFrontCurrent:"In front of the current {0}",WindowTransformationRangeUnbounded:"Unbounded",WindowTransformationRangeWeeks:"Weeks...",WindowTransformationRangeDays:"Days...",WindowTransformationRangeHours:"Hours...",WindowTransformationRangeMins:"Mins...",WindowTransformationRangeSecs:"Secs...",WindowTransformationRangeMiliSecs:"mSecs...",WindowTransformationNoRange:"Add start and end for range",WindowTransformationInvalidRange:"Start range/offset can not be greater than end range/offset",WindowTransformationDuplicateColumns:"Duplicate column name {0} found in Window transformation",WindowTransformationOverColumn:"over column",UnpivotTransformationRangeDescription:"Range by value is only available for numeric values. Range option is disabled when unbounded.",TransformationSetting_Label_UnionWith:"Union with",TransformationSetting_Label_AdditionalStreams:"Additional streams",TransformationInputTransformationTabName:"Input settings",TransformationOutputTransformationTabName:"Output settings",TransformationInspectTabName:"Inspect",TransformationOptimizeTabName:"Optimize",TransformationInspect_Streams:"Streams",TransformationLeftStream:"Left stream",TransformationLeftSource:"Left source",TransformationLeftSourceColumn:"Left source column",TransformationRightStream:"Right stream",TransformationRightSource:"Right source",TransformationRightSourceColumn:"Right source column",TransformationInspect_Update:"Updated",TransformationInspect_Dropped:"Dropped",TransformationInspect_Drifted:"Drifted",TransformationInspect_Unchanged:"Unchanged",TransformationInspect_UpdatedColumns:"Updated columns",TransformationInspect_NewColumns:"New columns",TransformationInspect_DroppedColumns:"Dropped columns",TransformationInspect_DriftedColumns:"Drifted columns",TransformationInspect_InputSchema:"Input schema",TransformationInspect_OutputSchema:"Output schema",TransformationInspect_ColumnCount:"Total columns",TransformationInspect_NumberOfSplit:"Number of streams",TransformationInspect_OutputStreams:"Output streams",TransformationInspect_SummaryTitle:"Number of columns",TransformationInspect_NumberOfRows:"Number of rows",TransformationInspect_Column_InputColumns:"Input column",TransformationInspect_Column_AggregatedAs:"Aggregated as",TransformationInspect_Column_BasedOn:"Based on",GroupbyLabel:"Group by",HelpGraphic:"Help graphic",TransformationInspect_Column_FedBy:"Fed by",TransformationInspect_Column_Ordered:"Ordered",TransformationInspect_Column_UsedByColumns:"Used by",TransformationInspect_Column_Pattern:"creates {0} column(s)",TransformationInspect_Column_NameAs:"Name as",ConditionalSplitTransformationUILabel:"Conditional Split",ConditionalSplitTransformationDescription:"Route data into different streams based on conditions",ConditionalSplitTransformationTabName:"Conditional split settings",ConditionalSplitTransformationConditionSplitLabel:"Split condition",ExecuteFlowletTransformationTabName:"Flowlet settings",TransformationInputTransformationColumnName:"Input transformation column",SelectTransformationUILabel:"Select",FoldDownTransformationUILabel:"Flatten",CastTransformationUILabel:"Cast",AssertTransformationUILabel:"Assert",AssertTransformationAllAssertsLabel:"All asserts",AssertTransformationAbortDescription:"Fail the Data Flow if assert conditions are not met",AssertTransformationAssertsDescription:"Assert that the following conditions are met",SelectTransformationDescription:"Choose columns to flow to the next stream",ExecuteFlowletTransformationDescription:"Choose a flowlet to execute.",FlowletLoadFailure:"Failed to update flowlet, it does not have correct numbers of inputs and outputs",FlowletInputMappingMissingInfo:"In order to map inputs, select a flowlet with input transformations configured.",ExecuteFlowletSelectedTransformationDescription:"Execute flowlet {0}",ExecuteFlowletNewTransformation:"Create a new flowlet",ExecuteFlowletOpenTransformation:"Edit the selected flowlet",InputTransformationDescription:"Input for the flowlet.",OutputTransformationDescription:"Output for the flowlet.",FoldDownTransformationDescription:"Choose columns to flatten and flow to the next stream",AssertTransformationDescription:"Assert that condition on columns is met",ExtendTransformationUILable:"Extend",ExtendTransformationDescription:"Use any custom logic from an external library",PivotTransformationUILabel:"Pivot",PivotTransformationDescription:"Pivots row values into columns, groups columns and aggregates data",UnpivotTransformationUILabel:"Unpivot",UnpivotTransformationDescription:"Unpivots columns into row values and ungroups columns",WindowTransformationDescription:"Aggregates data based on a window and joins with original data",FilterTransformationDescription:"Filter rows in the stream based on a condition",AlterRowTransformationDescription:"Insert, update, delete, or upsert rows in the stream based on conditions",AggregateTransformationDescription:"Calculate aggregations on the stream",AggregateTransformationTabName:"Aggregate settings",SortTransformationUILabel:"Sort",SortTransformationDescription:"Order data in the stream based on column(s)",RankTransformationUILabel:"Rank",RankTransformationDescription:"Add a rank column to your data based on column(s)",ParseTransformationDescription:"Parses your string data into JSON or CSV",ParseTransformationUILabel:"Parse",StringifyTransformationUILabel:"Stringify",StringifyTransformationDescription:"Converts your column data into JSON or CSV",CastTransformationTabName:"Cast settings",CastTransformationDescription:"Cast columns to different types",SortTransformationTabName:"Sort settings",RankTransformationTabName:"Rank settings",ParseTransformationTabName:"Parse settings",ParseTransformationColumnNameDescription:"The name of a new or existing column to store your parsed data.",ParseTransformationExpressionDescription:"The name of the source column that contains the data you wish to parse. This can also be a complex expression.",ParseTransformationOutputTypeDescription:"Configure the target output schema for the parsed data. E.g. (trade as boolean, customers as string[]).",ParseTransformationTypeDetectionLabel:"Detect type",UnionTransformationUILabel:"Union",UnionTransformationDescription:"Collect data from multiple streams",SourceTransformationSettingsTabName:"Source settings",SourceTransformationOptionsTabName:"Source options",SourceTransformationSchemaTabName:"Define schema",forcePathStyle:"Force path style",forcePathStyleDesc:"Enable this if using path-style access or Oracle data.",UnionTransformationTabName:"Union settings",SelectTransformationTabName:"Select settings",FoldDownTransformationTabName:"Flatten settings",AssertTransformationTabName:"Assert settings",StringifyTransformationTabName:"Stringify settings",ExtendTransformationTabName:"Extend settings",PivotTransformationTabName:"Pivot settings",UnpivotTransformationTabName:"Unpivot settings",WindowTransformationTabName:"Window settings",FilterTransformationTabName:"Filter settings",AlterRowTransformationTabName:"Alter row settings",SurrogateKeyTransformationTabName:"Surrogate key settings",SurrogateKeyTransformationUILabel:"Surrogate Key",JoinTransformationConditionLabel:"Join conditions",JoinTransformationNewConditionLabel:"New join condition",FilterTransformationConditionLabel:"Filter on",AlterRowTransformationInsertConditionLabel:"Insert if",AlterRowTransformationUpdateConditionLabel:"Update if",AlterRowTransformationDeletetConditionLabel:"Delete if",AlterRowTransformationUpsertConditionLabel:"Upsert if",AlterRowConditionsLabel:"Alter row conditions",JoinTransformationTabName:"Join settings",JoinTransformationUILabel:"Join",JoinTransformationDescription:"Join data from two streams based on a condition",SurrogateKeyTransformationDescription:"Adds a surrogate key column to output stream from a specific value",ExistsTransformationConditionLabel:"Exists conditions",ExistsTransformationCustomExpressionDescription:"Click here to use a custom expression for your exists condition",ExistsTransformationTabName:"Exists settings",ExistsTransformationDescription:"Check the existence of data in another stream",NewBranchDescription:"Create a new flow branch with the same data",SinkOptionDescription:"Destination for your data flow",ConditionalSplitTransformationConditionLabel:"Split on",ConditionalSplitTransformationCondition_All:"All matching conditions",ConditionalSplitTransformationCondition_First:"First matching condition",SinkTransformationSavePolicy:"Save policy",SinkTransformationSavePolicyAppend:"Append",SinkTransformationSavePolicySkip:"Skip",SurrogateKeyStartValueLabel:"Start value",SurrogateKeyStepValueLabel:"Step value",SinkTransformationAutoMapDescription:"Auto map columns to sink dataset",AllInputColumnsComment:"All input columns",InputNameComment:"Input name",TransformationAllInputsMappedMessage:"{0} mappings: All inputs mapped",TransformationInputsPartlyMappedMessage:"{0} mappings: {1} column(s) from the inputs left unmapped",TransformationAllOutputsMappedMessage:"{0} mappings: All outputs mapped",TransformationOutputsPartlyMappedMessage:"{0} mappings: {1} column(s) from the output schema left unmapped",TransformationUnmappedInputsMessage:"The following input column(s) are unused: {0}",TransformationUnmappedOutputsMessage:"The following output column(s) are unmapped: {0}",PreviewMappingOutputs:"Preview the results of this mapping",NoInputColumnsMatched:"No input columns matched",AutoMapAllInputsMappedWithoutSchemaDrift:"All defined inputs mapped by name excluding drifted columns",AutoMapAllInputsMappedWithSchemaDrift:"All inputs mapped by name including drifted columns",MapMismatchedInput:"At least one incoming column is mapped to a column in the sink dataset schema with a conflicting type, which can cause NULL values or runtime errors.",ConvertToFixedMappings:"Convert to fixed mappings",FromInitialRuleBasedMappingConvertToFixedMappingsDescription:"Due to a large number of input columns, a rule-based mapping is used. Click here to convert to fixed mappings",SinkTransformationReMap:"Re-map",SinkTransformationLink:"Link",SinkTransformationDelink:"Delink",DataFlowSchemaInputColumnHeader:"Input columns",SinkTransformationOutputColumnHeader:"Column in dataset",DataflowSchemaOutputColumnHeader:"Output columns",DataFlowSelectTransformationLabel:"Select transformation",DataFlowSelectTransformationsLabel:"Select transformations",DataFlowFoldDownTransformationLabel:"Flatten transformation",DataFlowFoldDownTransformationsLabel:"Flatten transformations",DataFlowDeriveTransformationLabel:"Derived column transformation",DataFlowDeriveTransformationsLabel:"Derived column transformations",DataFlowCallTransformationLabel:"External call transformation",DataFlowCallTransformationsLabel:"External call transformations",DataFlowCastTransformationLabel:"Cast transformation",DataFlowCastTransformationsLabel:"Cast transformations",DataFlowSurrogateKeyTransformationLabel:"Surrogate key transformation",DataFlowSurrogateKeyTransformationsLabel:"Surrogate key transformations",DataFlowSortTransformationLabel:"Sort transformation",DataFlowSortTransformationsLabel:"Sort transformations",DataFlowRankTransformationLabel:"Rank transformation",DataFlowRankTransformationsLabel:"Rank transformations",DataFlowParseTransformationLabel:"Parse transformation",DataFlowParseTransformationsLabel:"Parse transformations",DataFlowStringifyTransformationLabel:"Stringify transformation",DataFlowStringifyTransformationsLabel:"Stringify transformations",DataFlowJoinTransformationLabel:"Join transformation",DataFlowJoinTransformationsLabel:"Join transformations",DataFlowExistsTransformationLabel:"Exists transformation",DataFlowExistsTransformationsLabel:"Exists transformations",DataFlowUnionTransformationLabel:"Union transformation",DataFlowUnionTransformationsLabel:"Union transformations",DataFlowConditionalSplitTransformationLabel:"Conditional Split transformation",DataFlowConditionalSplitTransformationsLabel:"Conditional Split transformations",DataFlowInputTransformationLabel:"Input transformation",DataFlowInputTransformationsLabel:"Input transformations",DataFlowOutputTransformationLabel:"Output transformation",DataFlowOutputTransformationsLabel:"Output transformations",DataFlowInputTransformationAddColumnLabel:"Add input column",DataFlowOutputTransformationAddColumnLabel:"Add output column",DataFlowBrowseTransformationLabel:"Browse transformation",DataFlowBrowseTransformationsLabel:"Browse transformations",DataFlowConnectTransformationLabel:"Connect transformation",DataFlowConnectTransformationsLabel:"Connect transformations",DataFlowExecuteFlowletTransformationLabel:"Execute flowlet transformation",DataFlowExecuteFlowletTransformationsLabel:"Execute flowlet transformations",DataFlowAssertTransformationLabel:"Assert transformation",DataFlowAssertTransformationsLabel:"Assert transformations",DataFlowGenericTransformationLabel:"{0} transformation",DataFlowExtendTransformationMethod:"Method name",DataFlowDataPreviewElapsedTime:"Elapsed time:",DataFlowDataPreviewRefreshingData:"Refreshing data",DataFlowDataPreviewHint_StartCluster:"Please turn on debug mode and wait until the cluster is ready to preview data...",DataFlowDataPreviewHint_SessionStarting:"Please wait while the debug session starts...",DataFlowDataPreviewEmptyOutputMessage:"No output data.",DataFlowDataPreviewRefreshPreviewTooltip:"Refresh the data preview with any changes from the data flow",DataFlowDataPreviewRefreshPreviewFromSourceLabel:"Refetch from sources",DataFlowDataPreviewRefreshPreviewFromSourceTooltip:"When your source data changes, use this refresh option to reread the new source data for your data preview",DataFlowDataPreviewFetchingDataLabel:"Fetching data...",DataFlowDataPreviewFetchingStatisticsLabel:"Fetching statistics...",DataFlowDataPreviewDriftedColumn:"This drifted column is not in the source schema and therefore can only be referenced with pattern matching expressions",DataFlowDataPreviewSystemColumn:"This system column is not part of the schema and only displayed during data preview",DataFlowDataPreviewActionsTooltip:"Your data flow has changed since this data was fetched. Please refresh to enable data preview actions.",DataFlowDataPreviewExportCsv:"Export to CSV",ExportCsvTooltip:"Export CSV - only rows on the current page will be exported",DataFlowDataPreviewCopyRequestPayload:"Copy request payload",DataFlowConditionalSplitOtherText:"Rows that do not meet any condition will use this output stream",DataFlowAddWithoutOutputStreamWarning:"Add an output stream before adding a transformation after '{0}'",DataFlowDeleteOutputStreamWithConsumersWarning:"This stream is already in use, delete downstream consumers before deleting this stream",DataFlowFailedToAddTransformationTitle:"Failed to add a transformation",DataFlowFailedToAddTransformationDesc:"Add an output stream before adding a transformation after {0} ({1}).",DataFlowSearchReultsAlert:"Showing {0} results in {1} categories",DataFlowMenuItemAriaLabel:"{0} with description: {1}: option {2} of {3} for {4}: category {5} of {6}",DataFlowSortConditions:"Sort conditions",CantModifySecureString:"This property is marked as secure string and can't be modified through portal.",NameRequired:"Name is required.",SurrogateKeyColumnRequired:"Key column is required.",RankColumnRequired:"Rank column is required.",SelectTransformation:"Select transform",NewBranch:"New branch",CustomExpression:"Custom expression",DataFlowCdmMetadataFile:"Metadata file",DataFlowCdmMetadataFormat:"Metadata format",DataFlowCdmRootLocation:"Root location",DataFlowColumnType_Integer:"Integer",DataFlowColumnType_Float:"Float",DataFlowColumnType_Long:"Long",DataFlowColumnType_Double:"Double",DataFlowColumnType_Short:"Short",DataFlowColumnType_Timestamp:"Timestamp",DataFlowColumnType_Decimal:"Decimal",DataFlowColumnType_Byte:"Byte",DataFlowColumnType_DecimalPrecision:"Precision",DataFlowColumnType_Scale:"Scale",DataFlowDebugModifyColumns:"ModifyColumns",DataFlowDebugTransformationDescription:"Autogenerated by data preview actions",DataFlowDebugRemoveColumns:"RemoveColumns",DataFlowDebugTransformationConfirmTooltip:"Update the canvas with these changes",DataFlowDebugTransformationCancelTooltip:"Discard changes and refresh preview data",DataFlowDebugTypecastTooltip:"Change the type of a column",DataFlowDebugModifyTooltip:"Modify a column with simple functions",DataFlowDebugStatisticsTooltip:"View statistics for a column",DataFlowDebugMapDriftedTooltip:"Make all drifted columns explicit",DataFlowDebugMapDrifted:"Map drifted",DataFlowMapDriftedTransformation:"MapDrifted",DataFlowMapDriftedTransformationDescription:"Creates an explicit mapping for each drifted column",DataFlowDebugManagement_Cores:"Cores",DataFlowDebugManagement_LastActivityTime:"Last Activity Time",DataFlowDebugManagement_TimeoutMinutes:"Timeout (mins)",DataFlowDebugManagement_Loading:"Loading debug sessions...",DataFlowDebugManagement_CurrentSessionId:"Your current session ID:",DataFlowDebugManagement_MySessionIdLabel:"My current session ID:",DataFlowDebugManagement_MoveLocationTitle:"Debug sessions have moved",DataFlowDebugManagement_MoveLocation:"Pipeline debug run and data flow debug session information are now located in the Monitoring tab",DebugManagement_GoToPipeline:"View pipeline debug runs",DebugManagement_GoToDataflow:"View data flow debug sessions",DebugManagement_ViewDebugRun:"View debug run",DebugManagement_ViewPipelineRun:"View pipeline run",RemoveColumnLabel:"Remove column",DataFlowJoinTypeLabel:"Join type",DataFlowJoinType_Inner:"Inner",DataFlowJoinLeftStream:"left stream",DataFlowJoinRightStream:"right stream",DataFlowJoinAdditionalStream:"Additional stream",DataFlowJoinType_Inner_Description:"Only matching rows from {0} and {1}",DataFlowJoinType_Outer:"Full outer",DataFlowJoinType_Outer_Description:"All matched and unmatched rows",DataFlowJoinType_LeftOuter:"Left outer",MatchedAndUnmatchedRowsFromLabel:"Matched rows and unmatched rows from {0}",DataFlowJoinType_RightOuter:"Right outer",DataFlowJoinType_Cross:"Custom (cross)",DataFlowJoinType_Cross_Description:"Cross product of rows from {0} and {1} matching a criteria",DataFlowJoinFuzzyDisabled:"Fuzzy join doesn't support {0}",DataFlowJoinMatchTypeLabel:"Use fuzzy matching",DataFlowJoinMatchType_Exact:"Exact",DataFlowJoinMatchType_Exact_Description:"Join based on an exact matching of column values",DataFlowJoinMatchType_Fuzzy:"Fuzzy",DataFlowJoinMatchType_FuzzySettings_Header:"Fuzzy matching options",DataFlowJoinMatchType_Fuzzy_Description:"Join based on similarity of column values. Only applicable for string type column comparisons.",DataFlowJoinSimilarityPercentageLabel:"Similarity percentage",DataFlowJoinSimilarityThresholdPlaceholder:"Enter similarity threshold value...",DataFlowJoinOverrideSimilarityThresholdLabel:"Similarity Threshold",DataFlowJoinMatchByCombiningTextParts:"Combine text parts",DataFlowJoinMatchByCombiningTextParts_Description:"Allows combining text parts to find matches. For example, Data Factory is matched with DataFactory if this option is enabled.",DataFlowJoinScoreColumn:"Similarity score column",DataFlowJoinScoreColumn_Description:"Adds the similarity scores with the given column name to the schema",DataFlowExistsTypeLabel:"Exist type",DataFlowExistsType_Exists_Description:"{0} exists in {1}",DataFlowExistsType_DoesNotExist:"Doesn't exist",DataFlowExistsType_DoesNotExist_Description:"{0} doesn't exist in {1}",DataFlowUnionByLabel:"Union by",ImportFromDataset:"Import from dataset",ImportingSchema:"Importing schema...",ImportingSchemaDesc:"Successfully started importing the schema for {0} ({1}).\u200b",ImportingSuccessTitle:"Successfully imported\u200b",ImportingSchemaSuccessDesc:"Successfully imported the schema for {0} ({1}).\u200b",ImportingFailedTitle:"Failed to import",ImportingSchemaFailedDesc:"Failed to import the schema for {0} ({1}).",DataFlowAddSource:"Add Source",AddTarget:"Add target",AddSource:"Add source",RemoveTarget:"Remove target",RemoveSource:"Remove source",DataFlowAddInputMapping:"Add Input",DataFlowAddFlowlet:"Add Flowlet",DataFlowConditionalSplitsLabel:"Conditional splits",DataFlowLeftExpressionLabel:"Left Expression",DataFlowRightExpressionLabel:"Right Expression",DataFlowLeftHeaderLabel:"Left",DataFlowRightHeaderLabel:"Right",DataFlowStreamColumnHeader:"{0}'s column",DataFlowNullsFirstLabel:"Nulls first",DataFlowIgnoreNullsLabel:"Ignore nulls",DataFlowDenseRankLabel:"Dense",DataFlowDenseRankDescription:"If true, the rank count will be consecutive numbers. Rank values will not be skipped in the event of a tie",DataFlowSort_Ascending:"Ascending",DataFlowSort_Descending:"Descending",DataFlowCaseInsensitiveLabel:"Case insensitive",DataFlowBroadcastOptionSetOff:"Turn broadcast off",DataFlowPartitionLevelLabel:"Sort only within partition",DataFlowPartitionTypeLabel:"Partition type",DataFlowPartitionOptionCurrent:"Use current partitioning",DataFlowPartitionOptionNoPartition:"Single partition",DataFlowPartitionOptionSetSinglePartition:"Set single partition",DataFlowPartitionOptionPartitioning:"Set partitioning",DataFlowPartitionOptionBySourceDescription:"Describe how rows will be partitioned, either based on columns or based on queries.",DataFlowPartitionOptionBySourceQuery:"Query condition",DataFlowPartitionMethod:"Partition using",DataFlowNumberOfPartitions:"Number of partitions",DataFlowInvalidNumberOfPartitions:"There must be at least 2 partitions",DataFlowPartitionMethodRoundRobin:"Round Robin",DataFlowPartitionMethodRoundRobinDescription:"Simple distribution of data across partitions equally. Use this partitioning when you do not have a good key candidate for partitioning.",DataFlowPartitionMethodHashDescription:"The columns (or computed) value is used to form a uniform hash to distribute values. Rows with similar values are assured to fall in the same partition.",DataFlowPartitionMethodDynamicRange:"Dynamic Range",DataFlowPartitionMethodDynamicRangeDescription:"Dynamic range is similar to Fixed except that system figures out the ranges for the columns (or computed) columns you supply.",DataFlowPartitionMethodFixedRange:"Fixed Range",DataFlowPartitionMethodFixedRangeDescription:"Fixed Range partitioning will allow you set ranges for your key values to provide balanced partitions. Only use this option if you have an understanding of the range of values of your data.",DataFlowPartitionMethodKeyDescription:"Every distinct Column (no computed columns) value becomes a new partition. Use this partitioning if the number of distinct values are not huge.",DataFlowPartitionConditionLabelHash:"Column values to hash on",DataFlowPartitionConditionLabelDynamicRange:"Sorted ranges in columns",DataFlowPartitionConditionLabelFixedRange:"Condition to partition",DataFlowPartitionConditionLabelKey:"Unique value per partition",DataFlowPartitionConditions:"Partition conditions",DataFlowAddConditionalStreamLabel:"Add conditional stream",DataFlowAddDefaultStreamLabel:"Add default stream",DataFlowStreamNamesLabel:"Stream names",DataFlowNoConditionExpressionMessage:"No Condition Expression",DataFlowEditorErrorColumnNotFound:"Column not found",DataFlowEditorErrorParameterNotFound:"Parameter not found",DataFlowEditorErrorExpression:"Expression should be of '{0}' type",DataFlowEditorErrorWrongTypeExpression:"Expression should not be of '{0}' type",DataFlowEditorErrorTypeMismatch:"Expression's type '{0}' is not matching with required type '{1}'",DataFlowEditorErrorComparisonTypeMismatch:"Argument 1 of type '{0}' and argument 2 of type '{1}' must match and be both numeric, date, timestamp or string",DataFlowEditorErrorTypeUncompatibleWithLeftSide:"Expression's type '{0}' is not compatible with left side expression's type '{1}'",DataFlowEditorErrorComplexType:"Condition expression doesn't support complex or array type",DataFlowEditorErrorWrongNumberOfMinimumParameters:"'{0}' expects minimum '{1}' number of parameters",DataFlowEditorErrorWrongNumberOfMaximumParameters:"'{0}' expects maximum '{1}' number of parameters",DataFlowEditorErrorWrongInvalidFunction:"Please {0} key columns in sink settings to use '{1}'",DataFlowEditorErrorWrongNumberOfParameters:"'{0}' expects '{1}' parameters",DataFlowEditorErrorWrongTypeOfParameter:"'{0}' at parameter '{1}' expects '{2}' type of argument",DataFlowEditorErrorWrongTypeParameter:"'{0}' expects '{1}' type of argument",DataflowEditorErrorWrongTypeParameter_Generic:"'{0}' does not expect a parameter of type '{1}'",DataflowEditorErrorDoesNotAcceptFormat:"Format for {0} is not accepted",DataflowEditorEscapeFunctionFormatNotAccepted:"Format {0} is not accepted. Acceptable formats are 'json', 'xml', 'ecmascript', 'html', 'java'",DataFlowSurrogateKeyNoColumnNameError:"Column name is required for Key column in {0}",DataFlowSurrogateKeyDuplicateColumnNameError:"Duplicate columns found in the surrogate key transformation '{0}', add a select transformation before '{0}' to resolve the error.",DataFlowCallTransformDuplicateColumnNameError:"Duplicate output columns found in the Call transformation '{0}', Please provide unique column names.",DataFlowColumnTemplateConditionRequiredError:"Column template condition requires an expression",DataFlowColumnNameRequiredError:"{0} requires a name",DataFlowColumnExpressionRequiredError:"Column requires an expression",DataFlowRequiresAnExpressionError:"{0} requires an expression",DataFlowRegexInvalidError:"Regex is invalid",DataFlowColumnPatternColumnInvalid:"Expression could not be evaluated for matched column '{0}', error detail: {1}",DataFlowEditorErrorUnknownFunction:"Unknown function name",DataFlowEditorErrorArrayNotSameType:"Array elements must all be of the same type",DataFlowEditorErrorArrayIndexerNoArray:"Array indexer must be used with an array or map value",DataFlowEditorErrorArrayIndexerTypeDoesNotMatch:"Unable to use type '{0}' to access the map. The map key is of type '{1}'",DataFlowEditorErrorFieldSelectorNoComplexType:"Dot operator should be used for the hierarchical type",DataFlowEditorErrorFieldSelectorNoFieldWithName:"No field named {0} in the hierarchical structure",DataFlowEditorErrorColumnTypeNotMatchingExpression:"Column type is not matching with expression",DataFlowEditorMapMissingArguments:"Some map elements are missing either a value or a key",DataFlowEditorMapKeyMissingValue:"You need to provide a value for every key in a map",DataFlowEditorMapValueMissingKey:"You need to provide a key for every value in a map",DataFlowEditorErrorUnableToAssignValues:"Assignments are not supported. Did you mean to use the equality operator '==' instead?",DataFlowEditorErrorUnableToParse:"Unable to parse: {0}",DataFlowEditorErrorUnableToParse_Generic:"Unable to parse the expression. Please make sure it is valid.",DataFlowEditorErrorNullValue:"Expression type is evaluated to null, wrap it in functions like toInteger(expression), toString(expression).",DataFlowEditorErrorAnyValue:"Expression type could not be evaluated, correct the expression.",DataFlowErrorDecimalValueFormat:"'{0}' should be in range 0 - 38",DataFlowErrorInputStreamsNotSet:"One or more input streams are missing.",DataFlowSourceErrorMoveToEmpty:"You must specify a destination path to move your source file(s).",DataFlowErrorNoSource:"No source transformation found in the data flow '{0}'.",DataFlowErrorNoSink:"No sink transformation found in the data flow '{0}'.",FlowletErrorNoSource:"No source or input transformation found in the flowlet '{0}'.",FlowletErrorNoSink:"No output transformation found in the flowlet '{0}'.",DataFlowErrorDuplicatedName:"The name '{0}' is used in multiple transformations in this data flow",DataFlowErrorMissingInput:"The transformation '{0}' is missing an input stream.",DataFlowErrorAlphanumericOnly:"The transformation name '{0}' in the data flow '{1}' contains invalid characters, only alphanumeric characters are supported",DataFlowErrorDuplicateColumn_RowUrl:"Column to store file name '{0}' is a duplicate. Please specify a unique column name.",DataFlowErrorNotSupportedColumnType:"ColumnType '{0}' in column '{1}' is not supported.",DataFlowErrorMultipleCacheSinkOutput:"Multiple cache sinks found with output set to 'true'. Output for only one sink is supported",DataFlowErrorGroupOrderingMismatch:"You must either specify group ordering for all sinks or no sinks.",DataFlowErrorFilePattern_Sink:"You must specify a file name matching pattern to use for your file name(s)",DataFlowErrorPartitionFileNames_Sink:"You must specify a file name to use for your partition",DataFlowErrorSingleFile_Sink:"You must specify the file name for the output content",DataFlowErrorSingleFile_SinkPartitioning:"File name option 'Output to single file' requires 'Single partition' to be the selected partition type.",DataFlowWarning_OutputToSingleFile:"'Output to single file' requires 'Single partition' to be the selected partition type. This setting may impact performance and should only be used for smaller datasets.",DataFlowErrorSpecifyRowUrl_Sink:"You must specify a column to use for your file name(s).",DataFlowErrorSpecifyRowFolderUrl_Sink:"You must specify a column to use for your folder name.",DataFlowErrorFileNameAndKeyPartition_Sink:"Key partitioning is not allowed when 'Per partition' file name is set",DataFlowErrorRowUrlAndPartition_Sink:"You cannot specify file name in 'Name file as column data' when using custom partitioning in 'Optimize' tab",DataFlowErrorRowFolderUrlAndPartition_Sink:"You cannot specify folder name in 'Name folder as column data' when using custom partitioning in 'Optimize' tab",DataFlowErrorConditionExpressionIsEmpty:"Conditional expression in transformation '{0}' cannot be empty",TransformationColumnsLabel:"Columns:",DeriveTransformationColumnsGridDescription:"Create new derived columns or update existing columns",AggregatorTransformationGroupedByLabel:"Grouped by: ",AggregatorTransformationNoGroupedByLabel:"No group by columns",DataFlowTemplateCondition:"Template condition",DataFlowTemplateColumnMatch:"Each {0} that matches",TableNameDisplay_IndexName:"Index name",TableNameDisplay_CollectionName:"Collection name",TableNameDisplay_EntityName:"Entity name",TableNameDisplay_EntityType:"Entity type",TableNameDispaly_OpenHubDestination:"Open Hub Destination",TableNameDisplay_ObjectApiName:"Object api name",FilterTransformationPluralLabel:"Filters",AlterRowTransformationLabel:"Alter Row",AlterRowTransformationPluralLabel:"Alter Rows",AlterRowTransformationColumnsGridDescription:"Enter an expression to match rows for insert, delete, update, or upsert. Expressions are represented in order of priority, and each row will be marked with the policy corresponding to the first-matching condition.",AggregateTransformationLabel:"Aggregate",AggregateTransformationPluralLabel:"Aggregates",DataFlowExpressionBuilderInsertExplicitStructure:"Insert explicit structure",DataFlowExpressionBuilderSaveChangesContent:"Do you want to save changes made to the expression?",DataFlowExpressionBuilderSaveChangesTitle:"Save changes",DataFlowExpressionBuilderSubtract:"Subtract",DataFlowExpressionBuilderMultiply:"Multiply",DataFlowExpressionBuilderDivide:"Divide",DataFlowExpressionBuilderOr:"Or",DataFlowExpressionBuilderAnd:"And",DataFlowExpressionBuilderNot:"Not",DataFlowExpressionBuilderExclusiveOr:"Exclusive or",DataFlowExpressionBuilderEqual:"Equal",DataFlowExpressionBuilderNullEqual:"Null inclusive equal",DataFlowExpressionBuilderCaseInsensitiveEqual:"Case insensitive equal",DataFlowExpressionBuilderNotEqual:"Not equal",DataFlowExpressionBuilderGreaterOrEqual:"Greater than or equal",DataFlowExpressionBuilderLessOrEqual:"Less than or equal",DataFlowExpressionBuilderSimilarity:"Similar",DataFlowExpressionBuilderArrayIndexer:"New array (e.g. [1, 2, 5]) or array indexer (1-based index, e.g. [1, 2, 3][1] -> 1)",DataFlowExpressionBuilderOutputColumnNameExpression:"Output column name expression",DataFlowExpressionBuildeAggregateOnly:"This function is only available in aggregate, pivot, unpivot, and window transformations",DataFlowExpressionBuildeWindowOnly:"This function is only available in window transformations",DataFlowExpressionBuildeUnknownExpressionUnavailable:"This function is not available for the current transformation",DataFlowIncomingStreamLabel:"Incoming stream",DataFlowSelectColumnPlaceholder:"Select column...",DataFlowTypeSelectorPlaceholder:"Select type...",DataFlowAddSelectColumnPlaceholder:"Add or select a column...",DataFlowSurrogateKeyStartValuePlaceholder:"Enter start value...",DataFlowSurrogateKeyStepValuePlaceholder:"Enter step value...",KeyColumnName:"Key column",RankColumnName:"Rank column",DropdownSelectionButtonLabel:"Selection dropdown button",DataFlowEnterExpressionPlaceholder:"Enter expression...",DataFlowEnterConditionPlaceHolder:"Enter condition...",DataFlowEnterMatchingConditionPlaceholder:"Enter matching condition...",DataFlowEnterMatchingConditionOptionalPlaceholder:"Enter matching condition (optional)...",DataFlowEnterConditionOrNoChangePlaceHolder:"Enter condition or leave blank for no change...",DataFlowEnterRegexToMatchOnPlaceholder:"Enter regex...",DataFlowEnterRegularExpression:"Enter regular expression",DataFlowUseRegularExpression:"Use regular expression",DataFlowEnterRegexToMatchOnOptionalPlaceholder:"Enter regex (optional)...",DataFlowEnterFilterPlaceHolder:"Enter filter...",DataFlowEnterStreamNamePlaceHolder:"Enter output stream name...",DataFlowEnterDefaultStreamNamePlaceHolder:"Enter stream name for all other rows...",DataFlowEnterOutputNamePlaceHolder:"Enter output name...",DataFlowUseExpressionBuilder:"Expression builder",DataFlowOpenExpressionBuilder:"Open expression builder",DataFlowOpenExpressionBuilderForComplexType:"Define complex type",DataFlowUseExpressionBuilderExamples:"Examples",DataFlowDatasetUpgradeWarning:"We've made new updates to datasets for features such as data flow. Click to create a duplicate of this dataset with the updates.",DataFlowDatasetExtensionWarning:"The provided file extension does not match the dataset format, which can cause runtime errors. Please verify the file path",DataFlowDatasetUpgradeError:"Dataset {0} should be upgraded to be used in data flow",DataFlowDatasetReplaceWithV2Error:"Dataset {0} should be replaced with a new version dataset (e.g. Parquet, delimited text) to be used in data flow",DataFlowDatasetUpdateMethod:"Update method",DataFlowDatasetUpdateMethodDescription:"Determines what operations are allowed on your destination. The default is to only allow inserts. To update, upsert, or delete rows, an alter-row transformation is required to tag rows for those actions. For updates, upserts and deletes, a key column or columns must be set to determine which row to alter.",DataFlowDatasetKeyColumns:"Key columns",DataFlowDatasetAdditionalConfigs:"Additional configs",DataFlowDatsetListKeyColumns:"List of columns",DataFlowDatasetCustomExpressionDescription:"Custom expression must evaluate to array of strings representing the column names",DataFlowDatsetArrayOfKeyColumns:"Array of columns",DataFlowDatsetArrayOfKeyColumnsDescription:"An array of strings containing key columns",DataFlowHiveLinkedServiceContainerName:"Storage container name",DataFlowHiveLinkedServiceStoragePath:"Storage folder path",DataFlowHiveLinkedServiceAddScript:"Add new script",DataFlowLinkedServiceSelectSchemaPlaceholder:"Click refresh to load schema options",DataFlowSchemaTableSelectionSchemaPlaceholder:"Click refresh to load {0} options",DataFlowHiveLinkedServiceRefreshTablePlaceholder:"Click refresh to load table options",DataFlowLinkedServiceSelectSubTablePlaceholder:"Refresh and select schema for options",DataFlowLinkedServiceSelectSubViewPlaceholder:"Refresh and select table for options",DataFlowSchemaTableSelectionTablePlaceholder:"Refresh and select {0} for options",DataFlowLinkedServiceSelectTablePlaceholder:"Refresh and select a table",DataFlowLinkedServiceSelectContainerPlaceholder:"Refresh and select a container",DataFlowLinkedServiceSelectEntityPlaceholder:"Refresh and select an entity",DataFlowHiveLinkedServiceStagingContainerDescription:"Storage container used to stage data before reading from Hive or writing to Hive",DataFlowHiveLinkedServiceStagingDatabaseDescription:"Hive database used for external and temporary tables while staging data",DataFlowStoredProcedureSchema:"Procedure schema",DataFlowStoredProcedureName:"Procedure name",DataFlowStoredProcedureResultSet:"Result set",DataFlowStoredProcedureOutputParameter:"Output parameter",DataFlowStoredProcedureOutputData:"Output data",DataFlowStoredProcedureOutputDataDescription:"Choose whether the stored procedure should return the result set or output parameter",DataFlowStoredProcedureInputsDescription:"Provide any input parameters for the stored procedure in the order set in the procedure",DataFlowStoredProcedureRefreshInputs:"Load parameters",DataFlowStoredProcedureInputs:"Procedure parameters",DataFlowDatasetKeyColumnDescription:"Choose which column is used to determine if a row from the source matches a row from the sink",DataFlowDatasetKeyColumnDropdownPlaceholder:"Import schema to get the column list",DataFlowLookupTransformationLabel:"Lookup transformation",DataFlowLookupTransformationsLabel:"Lookup transformations",DataFlowExtendTransformationLabel:"Extend transformation",DataFlowExtendTransformationsLabel:"Extend transformations",DataFlowPivotTransformationLabel:"Pivot transformation",DataFlowPivotTransformationsLabel:"Pivot transformations",DataFlowUnpivotTransformationLabel:"Unpivot transformation",DataFlowUnpivotTransformationsLabel:"Unpivot transformations",DataFlowWindowTransformationLabel:"Window transformation",DataFlowWindowTransformationsLabel:"Window transformations",LookupTransformationDescription:"Lookup additional data from another stream",LookupTransformationTabName:"Lookup settings",LookupTransformationConditionLabel:"Lookup conditions",LookupTransformationMultipleRowLabel:"Match multiple rows",LookupTransformationMultipleRowDescription:"Return all matched rows from lookup stream",LookupTransformationMissingPickupRow:"Please select a 'Match on' row",LookupTransformationRowToChooseLabel:"Match on",LookupTransformationFirstRowLabel:"First row",LookupTransformationLastRowLabel:"Last row",LookupTransformationAnyRowLabel:"Any row",LookupNoSecondStreamDescription:"Add second stream to the Lookup from settings",LookupDescription:"Lookup on '{0}' from '{1}'",PrimaryStream:"Primary stream",LookupStream:"Lookup stream",UsedInTransformationLabel:"Used in {0}",DataFlowSurrogateKeyStartValueModel:"Start at",DataFlowSurrogateKeyStepValueModel:"Step",DataFlowFilePaths:"File paths",DataFlowDetectNoSchemaWarning:"You must define a schema for your source dataset in order to detect data types.",DataFlowDetectFormatWarning:"Please start a debug session in order to discover source schemas.",DataFlowTestConnectionWarning:"Please start a debug session in order to test connection.",DataFlowTestConnectionContainer:"Please specify the {0} under '{1}' to test connection",DataFlowRefreshOptionsWarning:"Please start a debug session in order to refresh the schema and {0} options.",DataFlowRefreshParameterizedWarning:"Refreshing the schema and table options is not available for parameterized schema names.",ImportSchemaInProgressWarning:"Please wait for your import schema to complete.",DataFlowDetectDataTypeTooltip_Disable_DetectData:"Please wait for any other data flow actions to complete",DataFlowDetectDataTypeTooltip_Disable_PreviewData:"Please wait for any data previews to complete",DataFlowRefreshDataPreviewTooltip_Disable:"Please wait for all data type detections and data preview operations to complete",DataFlowBrowseDisableTooltip:"Please start a debug session in order to browse an entity.",DataFlowClearFolder:"Clear the folder",DataFlowQuoteAll:"Quote All",DataFlowQuoteAllDescription:"Enclose all values with quotes",DataFlowHeaderDescription:"Enter an array of string values to add as the header of your output file. E.g. ['Microsoft' , 'All rights reserved', '===============']",DataFlowTableAction:"Table action",DataFlowContainerAction:"Container action",DataFlowTableActionDescription:"Determines whether to recreate or remove all rows from the destination table prior to writing.",DataFlowTruncate:"Truncate",DataFlowTruncateSink:"Truncate table",DataFlowTruncateSinkDescription:"All rows from the target table will get removed.",DataFlowMergeSchema:"Merge schema",DataFlowRecreate:"Recreate",DataFlowRecreateTable:"Recreate table",DataFlowRecreateContainer:"Recreate container",DataFlowRecreateTableDescription:"The table will get dropped and recreated. Required if creating a new table dynamically.",DataFlowCreateDataset:"Create dataset",DataFlowEditDataset:"Edit dataset",DataFlowAddAlterRow:"Add Alter Row",DataFlowAddAssert:"Add Assert",DataFlowListOfExpression:"List of expression",DataFlowListOfScripts:"List of scripts",DataFlowCustomExpressionOfSQLListDescription:"Custom expression must evaluate to array of strings representing the sql scripts",SkipDuplicateMappingOutputs:"Skip duplicate output columns",SkipDuplicateMappingOutputsDescription:"If a mapping will result in the same output as a previous mapping, then that mapping output will be skipped",SkipDuplicateMappingInputs:"Skip duplicate input columns",SkipDuplicateMappingInputsDescription:"If the same input column is used in more than one mapping, then that input column will be skipped in mappings after the first",ColumnPatternSubHeading:"The first matching rule will be applied",NewColumnPattern:"New Rule to transform",CastFromType:"From Type",CastToType:"To Type",ColumnPatternMatchesCondition:"Matches",ColumnPatternStartsWithCondition:"Starts With",ColumnPatternEndsWithCondition:"Ends With",ColumnPatternHasValueCondition:"Has Value",ColumnPatternMatchingCondition:"If source schema column",ColumnPatternExceptCondition:"except that column which",NewLinkedserviceTitleLabel:"New linked service ({0})",NewConnectionTitleLabel:"New connection ({0})",EditLinkedserviceTitleLabel:"Edit linked service ({0})",DataFlowReferenceLabel:"Reference:",UnnamedColumnLabel:"<not named>",NoExpressionLabel:"<no expression>",SinkDescription:"Export data to {0}",SinkNoDatasetDescription:"Add sink dataset",SinkCacheDescription:"Export data to cache",SourceDescription:"Import data from {0}",SourceNoDatasetDescription:"Add source dataset",JoinNoSecondStreamDescription:"Add second stream to the join from settings",JoinDescription:"{0} join on '{1}' and '{2}'",SortNoExpressionDescription:"Add column or expression for sort",RankNoExpressionDescription:"Add column or expression for rank",ParseNoSchemaDescription:"Add a columns to be parsed",SurrogateKeyNoColumnDescription:"Specify column name for the key",SurrogateKeyDescription:"Adding new key {0} starting from {1} with step {2}",SortDescription:"Sorting rows on columns '{0}'",RankDescription:"Ranking rows on columns '{0}'",ParseDescription:"Parsing data to '{0}'",FilterNoExpressionDescription:"Add expression for the filter",FilterDescription:"Filtering rows using expressions on columns '{0}'",AssertNoExpression:"Add expression for all asserts",AssertNoId:"Add assert Id for all asserts",AlterRowNoExpressionDescription:"Add expressions to alter rows",AlterRowDescription:"Inserting, updating, deleting, and/or upserting using expressions on columns '{0}'",DeriveTransformationAddColumnsDescription:"Add columns for the derive transformation",DeriveDescription:"Creating/updating the columns '{0}'",SelectDescription:"Renaming {0} to {1} with columns '{2}'",FoldDownDescription:"Unrolling arrays from {0}{1} with columns '{2}'",SelectDescriptionNoColumns:"Add columns to rename",SelectDescriptionNoInputStream:"Choose an incoming stream from settings",FoldDownDescriptionNoColumns:"Add columns to map",AggregateDescription:"Aggregating data by '{0}' producing columns '{1}'",AggregateNoGroupDescription:"Aggregating data by producing columns '{0}'",AggregateNoColumnsDescription:"Add aggregate columns",SplitDescription:"Conditionally distributing the data in {0} groups, based on columns '{1}'",SplitNoConditionDescription:"Add cases to distribute the data in multiple groups",ExistsDescriptionWithNegate:"Filtering rows from {0} which are not matching in {1} based on columns '{2}'",ExistsDescriptionWithoutNegate:"Filtering rows from {0} which are matching in {1} based on columns '{2}'",ExistNoSecondStreamDescription:"Add second stream to exists from settings",ExistNoColumnDescription:"Add column to match data between sources",ExistsNoCustomExpression:"Exists transformation should have exists condition",UnionDescription:"Combining rows from transformation '{0}'",DataFlowExpressionHasError:"Data flow expression has error",DataFlowExpressionUnexpectedReturnTypeGeneric:"Expression should return {0} type",DataFlowExpressionUnexpectedReturnType:"{0} expression should return {1}",DataFlowInvalidParameterName:"Parameter {0} has invalid name, '@', ' ' and '.' are not allowed",DataFlowInvalidParameterValueSelfReferenced:"Parameter {0} has invalid value, parameters cannot be self referenced",DataFlowExpressionUsesBadReferences:"Data flow expression uses functions/parameters/columns which are not present in current context",DataFlowExpressionByNamesColumnsError:"The '{0}' function must be enclosed in another function",DataFlowExpressionColumnUsedError:"Columns cannot be referenced within the 'byPath' function, 'byName' function, 'byNames' function, or 'byPosition' function.",DataFlowExpressionTypecastWarning:"When using the 'byName' function, 'byOrigin' function, or 'byPosition' function, ensure that it is wrapped in a typecasting function, otherwise you may run into runtime errors. For example: toString(byName('column1'))",AllAnnotations:"All annotations",NoAnnotations:"No annotations",FilterResources:"Filter resources",StringifyColumnTypeError:"Stringify expressions must be a complex type or an array of complex types",JoinNoCrossConditionDescription:"Cross join condition cannot be empty",JoinCrossConditionMisingStreamReference:"Cross join condition must have at least one column from each stream, '{0}' stream is not being used",FuzzyJoinBroadcastDescription:"Broadcast must be set to off to use fuzzy matching",FuzzyJoinStringReturnTypeDescription:"Columns used in Fuzzy matching comparisons must be of string type",FuzzyJoinWarning:"Broadcast must be set to off to use fuzzy matching. Fuzzy matching currently only support inner and outer joins",FuzzyJoinInfo:"Fuzzy matching currently only support inner, outer, and left outer joins",RuleBasedColumnNoColumnError:"Pattern based column should have at least one column",NoAggregateFunctionUsedError:"Aggregate columns should use at least one aggregate function",NoAggregateFunctionWrappedError:"Columns should be wrapped with aggregate function",NoWindowFunctionUsedError:"Window columns should use at least one window/aggregate function",NoWindowFunctionWrappedError:"Columns should be wrapped with window/aggregate function",AmbiguousColumnError:"Ambiguous column {0} from input stream {1}",IncompatibleColumnError:"Incompatible types of column {0}: {1} and {2}",IncompatibleColumnTypeError:"Incompatible data type mapping from column {0} to column {1}",TypeConversionDataLossWarning:"Copying from column {0} to column {1} may have data truncation.",TypeConversionDefiniteDataLossWarning:"Copying from column {0} to column {1} will have data truncation.",TypeConversionDataTruncationEnableError:'Copying from column {0} to column {1} will have data truncation.  Please check "Allow data truncation".',ConditionalSplitMissingStreamName:"Stream requires a name",ConditionalSplitDuplicateStreamName:"Duplicate stream name '{0}'",ConditionalSplitMissingExpression:"Stream '{0}' requires an expression",ConditionalSplitMissingExpressionNoName:"Stream requires an expression",ConditionalSplitDuplicateInputColumnName_Warning:"Duplicate input column names found. Consider adding a Select transformation before this transformation.",NonEquiCondition_BroadcastError:"Specify a broadcast side for a non-equality comparison condition.",NonEquiCondition_WarningMessage:"Non-equality comparison logic requires a minimum of 1 stream to be fully broadcast. Please ensure that your Integration Runtime is sized appropriately.",NonEquiCondition_CustomExpressionWarning:"If you plan on using non-equality comparisons in your custom expression, you should utilize the 'Fixed' broadcast setting and specify a minimum of 1 stream to be broadcast. If broadcasting, ensure that your Integration Runtime is sized appropriately.",RemoveBranch:"Remove branch",UnionAlreadyConsumedAllIncomingStreams:"No viable incoming stream can be added to this {0} transformation, all avaialble stream are already attached.",UnionAlreadyConsumedAllIncomingStreamsTitle:"No incoming stream feasible for {0}",JoinDuplicateColumnsTitle:"Duplicate columns found in Join",JoinDuplicateColumnsErrorFormat:"Duplicate columns found in both sides. Consider adding a Select transformation before either the left or right stream. Invalid columns: {0} from {1}",EnterColumnNamePlaceHolder:"Enter column name...",EnterColumnNameExpressionPlaceHolder:"Enter column name expression...",EnterOutputColumnNameExpressionPlaceholder:"Enter output column name expression...",EnterColumnTypePlaceHolder:"Enter type...",AddSourceTourDescription_Title:"Add a source",AddSourceTourDescription:"Start by adding source to data flow",NodeInformationTourDescription_Title:"Transformation information",NodeInformationTourDescription:"The left side of the node shows the type of transformation.\n\nThe right side of the node shows the name and description of the data stream produced by the transformation",RightClickActionTourDescription_Title:"Right click for more actions",RightClickActionTourDescription:"Click on the node to configure.\nRight Click to see more actions.",AddMoreTransformationsTourDescription_Title:"Add a transformation",AddMoreTransformationsTourDescription:"Click on '+' to add new branch or transformation.",SinkBlobLimitationTourDescription_Title:"Sink your data",SinkBlobLimitationTourDescription:"Now, sink your data into a destination dataset. If the dataset type is not available in data flow, then stage your data as Parquet or CSV and use the Copy Activity to load the data into your destination.",DataFlowSinkDatasetContainsFilePath:"Sink dataset filepaths cannot contain a file name. Please remove the file name from '{0}'",DataFlowSinkRequiresKeyColumn:"Allow delete, allow upsert, and allow update options require a key column to be specified. Please specify one or more columns.",DataFlowSinkNoUpdateMethodSelected:"No update method selected.",RequiresKeyColumn:"Upsert require a key column to be specified. Please specify one or more columns for row matching in '{0}'",DupKeyColumn:"Key column '{0}' already listed.",DataFlowSinkRequiresKeyColumnSkipKeyWrites:"Skip key writes requires a key column to be specified. Please specify one or more columns in '{0}'",DataFlowSinkRequiresErrorLogging:"A linked service to store the logged errors must be specified.",DataFlowSinkRequiresContainerErrorLogging:"A location within the linked service to store the errors must be specified.",DataFlowSinkTruncateNotSupport:"Truncate collection is not supported for Azure Cosmos Db.",DataFlowSinkSkipKeyWrites:"Skip writing key columns",DataFlowSinkSkipKeyWritesDescription:"Not write the value to the key column.",DataFlowSinkSkipKeyWrite:"Skip writing key column",DataFlowSinkRequiresAlterRow:"Allowing delete, upsert, or update requires an Alter Row transformation to set row policies.",DataFlowSinkRequiresAssert:"Adding an assert failure output storage requires Assert transformation.",DataFlowSinkKeyColumnsNotMapped:"Key column '{0}' does not exist in mapped columns.",DataFlowSinkRequiresAlterRow_BannerAddOn:"Click to insert an Alter Row transformation.",DataFlowSinkRequiresAssert_BannerAddOn:"Click to insert an Assert transformation.",DataFlowSinkEmptySchema:"Sink dataset {0} doesn't have any schema, please import schema from dataset or turn on 'Allow schema drift' in sink '{1}'",DataFlowSinkColumnReferenceMismatch:"An incoming column reference does not match your current sink schema. Please reset or correct the mapping of '{0}'",DataFlowSinkSinglePartitionBanner:"This sink currently has Single partition set in Optimize. This will make your data flow execution longer. The recommended setting is Use current partitioning.",DataFlowTransformationNumberOfPartitionConditionsError:"Number of 'Fixed Range' conditions must be between {0} and {1} in '{2}'",DataFlowTransformationNumberOfPartitionsError:"Number of partitions must be 2 or greater in '{0}'",DataFlowTransformationPartitionConditionsTypeError:"Partition condition must return {0} type in '{1}'",DataFlowEmptySchemaError:"Columns cannot be empty  in '{0}', please import schema from dataset or turn on 'Allow schema drift'.",DataFlowSchemaEmptyColumnNameError:"Column must have a name  in '{0}'",DataFlowSourceSpecialCharactersError:"Column name cannot contain line breakers like \\n",DataFlowSinkSpecialCharactersError:"Column name cannot contain special characters or spaces when using Azure Synapse Analytics or Parquet format in '{0}'",DataFlowNTileValidationError:"nTile cannot accept a column reference as an argument",DataFlowSinkAllowInsert:"Allow insert",DataFlowSinkAllowDelete:"Allow delete",DataFlowSinkAllowDeleteTooltip:"Delete is not available when recreating or truncating a table.",DataFlowSinkAllowUpsert:"Allow upsert",DataFlowSinkAllowUpsertTooltip:"Upsert is not available when recreating or truncating a table.",DataFlowSinkAllowUpdate:"Allow update",DataFlowSinkAllowUpdateTooltip:"Update is not available when recreating or truncating a table.",DataFlowSinkCosmosDBASAllowUpsertValidationError:"'Allow upsert' is required when the dataflow source is Cosmos DB Analytical Store and '{0}' is checked.",DataFlowSinkSchemaOutputColumnMismatch:"The following column(s) could not be found in the table schema: '{0}'. To add new columns to your table schema, please select the 'Recreate table' option; if the column already exists in the table please import the dataset schema again.",DataFlowSinkSchemaOutputColumnComplex:"The following column(s) have a complex structure which can only be written to Snowflake, REST, ORC, JSON, AVRO, and Azure Cosmos DB: '{0}'. Please remove the columns or update the sink to Snowflake, REST, ORC, JSON, AVRO, or Azure Cosmos DB.",DataFlowSinkSchemaOutputColumnComplexA365:"The following column(s) have a complex structure which can only be written to Snowflake, REST, ORC, JSON, AVRO, Azure Cosmos DB and spark databases: '{0}'. Please remove the columns or update the sink to Snowflake, REST, ORC, JSON, AVRO, Azure Cosmos DB or spark databases.",DataFlowSinkSchemaJsonDataset:"Json dataset in sink cannot have schema.",DataFlowSinkExcludeColumnsDescription:"In upsert mode, the columns which will be included when inserting but excluded when updating.",DataFlow_BatchSize_Description:"Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.",DataFlowSchemaEmptyColumnError:"Input column in mapping cannot be found",DataFlowSchemaEmptyColumnErrorWithName:"Input column '{0}' in mapping cannot be found",DataFlowParseWithNoDelimiterError:"Column delimiter cannot be empty",FoldDownUnrollValuesNotArray:"The inner value type of the 'Unroll by' structure '{0}' must be array type (e.g. structue of arrays, array of arrays)",FoldDowMappingStructWithArray:"Input column '{0}' in mapping '{1}' cannot contain an array that is being unrolled",FoldDownUnrollValuesInvalidColumnd:"Unroll value '{0}' must be a valid column",FoldDownUnrollRootIsInvalid:"Unroll root '{0}' must be in the path from root (empty) to unroll by '{1}'",DataFlowSchemaEmptyColumnTypeError:"Column must have a type  in '{0}'",MultiInputStreamTransformationEnableBroadcastLabel:"Enable broadcast",MultiInputStreamTransformationBroadcastLabel:"Broadcast",MultiInputStreamTransformationBroadcastOptionsLabel:"Broadcast options",MultiInputStreamTransformationBroadcastLeft:"Left: '{0}'",MultiInputStreamTransformationBroadcastRight:"Right: '{0}'",MultiInputStreamTransformationBroadcastStreamDescription:"If your worker node can hold the entire stream in memory choose this option",MultiInputStreamTransformationEnableBroadcastDescription:"Enable broadcast join and choose the stream(s) you want to hold in the memory.",MultiInputStreamTransformationBroadcast_Auto_Description:"The Data Flow engine will automatically decide when to broadcast",MultiInputStreamTransformationBroadcast_Fixed_Description:"Manually specify which streams to broadcast. This option should only be selected if an entire data stream can be held in memory. A memory-optimized cluster is recommended.",MultiInputStreamTransformationBroadcast_Off_Description:"Disables broadcasts. This may negatively impact performance.",MultiInputStreamTransformationBroadcastError:"Specify a broadcast stream",ReferenceNodeTourDescription_Title:"Reference nodes",ReferenceNodeTourDescription:"Reference nodes are created automatically when needed to improve legibility of your data flow design.\n\nHovering over a reference node will highlight node it is referencing.",Dataflow_SearchPlaceholder:"Search data flow...",Pipeline_SearchPlaceholder:"Search pipeline...",Insert_Method:"Insert method",Delete_Method:"Delete method",Upsert_Method:"Upsert method",DataFlowSQLIsolationLevel:"Isolation level",DataFlowSQLIsolationLevelDescription:"Sets the SQL read isolation level - read uncommitted is the default",DataFlowSQLIsolationLevelReadCommittedDescription:"Can only read committed data modifications (no dirty reads)",DataFlowSQLIsolationLevelReadUncommittedDescription:"Can read uncommitted data modifications (dirty reads)",DataFlowSQLIsolationLevelRepeatableReadDescription:"Read committed, plus other transactions cannot read modifications from this transaction until it is complete",DataFlowSQLIsolationLevelSerializableDescription:"Repeatable read, plus other transactions cannot insert new rows with key values in the range of values read by this transaction until it is complete",DataFlowSQLIsolationLevelNoneDescription:"Do not explicitly set an isolation level",DataFlowSQLIsolationLevelReadCommitted:"Read committed",DataFlowSQLIsolationLevelReadUncommitted:"Read uncommitted",DataFlowSQLIsolationLevelRepeatableRead:"Repeatable read",DataFlowSQLIsolationLevelSerializable:"Serializable",GlobalSearch_Placeholder_ADF:"Search factory",GlobalSearch_Placeholder_ADF_Docs:"Search factory and documentation",GlobalSearch_Placeholder_Synapse:"Search workspace",GlobalSearch_Placeholder_Babylon:"Search your organization data",GlobalSearch_AndMore:"and {0} more...",GlobalSearch_ItemCount:"{0} items",GlobalSearch_Results:"Results",GlobalSearch_AdditionalMatches:"{0} additional matches...",ResourceGroup:"Resource group",ResourceGroupNameRequired:"Resource group name is required.",ResourceGroupNameInvalid:"Resource group name is invalid.",InvalidResourceGroupNamePattern:"{0} name is not valid, only alphanumeric characters, dash, underscore, opening parenthesis, closing parenthesis, and period are allowed. The name cannot end with a period.",FilterDescriptionForNativeMongoDBAndCosmosDB:"The selection filter using query operators. To return all documents in a collection, omit this parameter or pass an empty document ({}).",ProjectDescriptionForCosmosDB:"The fields to return in the documents that match the query filter. To return all fields in the matching documents, omit this parameter.",SkipDescriptionForCosmosDB:"The number of documents to skip in the results set.",LimitDescriptionForCosmosDB:"The maximum number of documents the cursor will return.",SortDescriptionForCosmosDB:"Orders the documents in the result set.",AzureCosmos_AccountDescription:"You can select an Azure CosmosDB ({0} API) account.",ConnectionStringDescription:"Click to check the standard connection string format.",Annotations:"Annotations",PropertyName:"Property name",AdditionalConnectionStringFields:"Additional connection properties",AdditionalConnectionString_InvalidPropertyName:"Invalid property name",AdditionalConnectionString_DuplicatePropertyName:"Duplicate property name",ModelName_MaxConcurrentConnections:"Max concurrent connections",ModelName_OperationTimeout:"Operation timeout (minutes)",ModelName_DisableChunking:"Disable chunking",OperationTimeout_Description:"Specify the timeout for writing each chunk to SFTP server.",MaxConcurrentConnections_Description:"The upper limit of concurrent connections established to the data store during the activity run. Specify a value only when you want to limit concurrent connections.",MaxConcurrentConnections_ValidatorErrorMessage:"The max concurrent connections cannot be less than 0.",ModelName_Enable_Sink_Partition:"Enable partition",ModelName_BlockSizeInMB:"Block size (MB)",BlockSizeInMB_Description:"Specify the block size in MB when writing data to {0}. Allowed value is between 4 and 100 MB.",BlockSizeInMB_ErrorMessage:"Block size value must be a number in 4-100.",NeedsAttention:"NEEDS ATTENTION",SucceededRuns:"SUCCEEDED RUNS",FailedRuns:"FAILED RUNS",InProgressRuns:"IN PROGRESS RUNS",CancelledRuns:"CANCELLED RUNS",MonitoringOverview:"Monitoring Overview",OutOfMaxMetrics:"Time range is out of the Max metrics retention period",VariablesLabel:"Variables",SetVariables:"Set variable",AppendVariables:"Append variable",VariableNameAlreadyExists:"This variable name already exists",No_Variable_In_Pipeline:"No variables defined in pipeline",Set_Variable_Undefined:"Set Variable Activity Variable Undefined",Set_Variable_Value_Undefined:"Set Variable Activity Variable Value Undefined",Variable_Does_Not_Exists:"Variable in Set Variable Does Not Exists",Variable_NotValidType:"Variable does not have a valid type, please choose one.",Variable_String:"String",Variable_Boolean:"Boolean",Variable_Array:"Array",Int_Label:"Int",Object_Label:"Object",Bool_Label:"Bool",SecureString_Label:"SecureString",Append_Variable_Undefined:"Append Variable Activity Variable Undefined",Append_Variable_Value_Undefined:"Append Variable Activity Variable Value Undefined",Append_Variable_Value_Not_Of_Type:"Append Variable Activity Variable Type Is Not Of Type Array",Append_Variable_Does_Not_Exists:"Variable in Append Variable Does Not Exists",Invalid_Variable_Array:'Variable value should be a valid array; e.g. ["1","2","3"]',AppendVariableDisclaimer:"Disclaimer: Append Variables Only Supports Adding To 'Array' Type Variables",DashboardNoData:"There are no {0} runs in this time range",InternalHttpClientError:"Internal http error. Unknown response type.",ActiveDebugRunsAndSessions:"Active debug runs and sessions",ActivePipelineRuns:"Active pipeline runs",ActiveDataFlowSessions:"Active data flow sessions",ActiveDataFlowDebugSessions:"Active data flow debug sessions",InvokedBy:"Invoked By",PipelineInvokedBy_Manual:"User",DbPythonVersion2:"2",DbPythonVersion3:"3",Accessibility_ResizeColumnNoName:"Resize column",UpsertSettingsLabel:"Upsert settings",UpsertSettingsDesc:'A grouped parameterization for "upsertSettings" property, includes "useTempDB", "interimSchemaName", and "keys".',DataFlowDebugSessionClusterInProgress:"Getting the cluster ready. This could take a few minutes",DataFlowDebugSessionClusterStarting:"Getting the cluster ready.",DataFlowDebugSessionClusterSucceeded:"Cluster is ready. Session ID : {0}",DataFlowDebugSessionClusterFailed:"Failed to setup debug session. Session ID: {0}",DataFlowDebugSessionRequestFailed:"Failed to setup debug session. Error: {0}",DataFlowDebugSessionStopFailed:"Failed to stop debug session.",DataflowDebugSessionPreviewFailed:"Failed to fetch data preview.",DataflowDebugSessionPreviewFailed_Timeout:"Failed to fetch data preview due to operation timeout.",DataflowDebugSession_StillActive:"Previous session still active. Session ID: {0}",DataflowDebugSession_StillStarting:"Previous session still starting. Session ID: {0}",DataFlowComputeOptimizedWarning:"The IR used in this data flow is configured to use Compute Optimized 4 + 4 option. We recommend selecting a different option since it will be deprecated.",DataFlowComputeOptimizedNotRecommended:"Compute Optimized is not recommended for production workloads. Please consider using a minimum General Purpose for production workloads.",DataFlowIROptionsEditWarning:"Changes to data flow settings will take effect upon completion of all jobs currently executing plus TTL time.",DataFlowVnetIRWarning:"The selected data flow uses linked service(s) with an integration runtime under a Managed Virtual Network. Please select an integration runtime to run on which is under the same Managed Virtual Network.",DataFlowDebugStatisticsFailed:"Could not fetch statistics. Try again.",DataFlowDebugStatisticsFailed_Timeout:"Could not fetch statistics due to operation timeout.",DataFlowDebugInvalidTransformationStream:"Transformation stream is invalid, fix errors to enable data preview.",DataFlowDebugFlowletDataMissing:"Input data must be configured to debug flowlet.",DataFlowDebugInvalidUpstream:"The current stream is invalid due to errors in previous transformations, please fix the errors to enable data preview.",DataFlowDebugNoColumnName:"Name the selected column to enable data preview.",DataFlowDebugDataPreviewParameterValuesMissing:"Data flow has parameters for which a value has not been provided, these values are necessary for data preview.",DataFlowDebugCallTransformationWarning:"Data preview on call transform will execute against the actual data store.",DataFlowCallTransformColumnNameError:"Column name is required or uncheck for column {0}",DataFlowCallCache:"Cache for repeat requests",DataFlowDebugNoErrors:"No errors",DataFlowDebugClickToSeeErrors:"Click to see errors",DataFlowDebugClickToSeeParameters:"Click to see parameters",DataFlowDebugClickToEditDebugSettings:"Click to edit debug settings",DataFlowDebugNullValuesTip:"There are a high number of null values or missing values which may be caused by having too few rows sampled. Try updating the debug row limit and refreshing the data.",DataFlowDebugErrorsTillTransformation:"Errors upstream of '{0}' in data flow '{1}'",DataFlowDebugInvalidSourceImportSchema:"This transformation is invalid, please fix any errors to import schema.",DataFlowDebugInvalidSinkWithParameters:"A dataset with dynamic content/parameters is used in this sink. This is not supported in data flow debug, either remove the dynamic content/parameters or use pipeline debug to test this data flow sink.",DataFlowDebugInvalidSourceWithParameters:"A dataset with dynamic content/parameters is used in source(s) {0}. This is not supported in data flow debug, either remove the dynamic content/parameters or use pipeline debug to test this data flow.",DataFlowDebugSettings:"Debug Settings",FlowletDataflowDebug:"Add to data flow",DataFlowDebugActiveIr:"Data flow debug IR:",DataFlowDebugSampleFile:"Sample file",DataFlowDebugSampleInlineNotSupported:"Sample files are not supported for inline datasets",DataFlowDebugSampleTable:"Sample table",DataFlowDebugSampleLabel:"Row limit",DataFlowDebugPreviewRequestLabel:"Preview request",DataFlowDebugPreviewRequestColumnLabel:"Column Name",DataFlowDebugPreviewRequestColumnName:"RequestPayload",DataFlowDebugPreviewRequestDescription:"Specify column name for request payload",DataFlowDebugPreviewRequestCheckDescription:"Select to show request payload for transform rows in preview",DataflowDebugBillingBanner:"Turning on debug mode will incur Azure Databricks billing costs",DataflowDebugStorageLinkedServiceDescription:"Storage linked service used in staging for SQL DW source",DataflowDebugStagingFolderDescription:"Staging folder for an SQL DW source",DataflowDebugRowLimitDescription:"Specify the number of rows used in the debug dataset",DataflowRowLimitConstraint:"Rows limit must be a number greater than 0.",DataFlowDebugImportNoParametersFound:"No parameters were found to import",UploadFile:"Upload file",UnableToUploadFile:"Unable to upload file: {0}",DisabledUploadFileMessage:"Select a container to upload a file",DataFlowDebugTurnOn:"Turn on debug",DataFlowDebugTurnOnWarning_Title:"Turn on data flow debug",DataFlowDebugSelectedIrDetails:"{0} - {1} worker cores (+ {2} driver cores)",DataFlowDebugTurnOnWarning_Content:"This may take several minutes as we start up a cluster for you, and will initiate your data flow debug session billing.",DataFlowDebugTurnOnWarning_SessionRequired:"Data flow debug session required",DataFlowDebugTurnOnWarning_SandboxContent:"You must start a data flow debug session in order to debug a data flow activity.",DataFlowDebugRunWithSession:"Use data flow debug session",DataFlowDebugRunWithoutSession:"Use activity runtime",DataFlowDebugParallelTitle:"Use debug cluster?",DataFlowDebugParallelWarning:'When executing data flow activities in parallel in your pipeline, it is not recommended to use the debug cluster. Instead, please select the "Use Integration Runtime" option.',DataFlowDebugParallelWarningUseRuntime:"Use Integration Runtime",DataFlowDebugParallelWarningContinue:"Continue with debug cluster",DataFlowDebugVnetIrTitle:"Use dataflow debug session?",DataFlowDebugVnetIrWarning:"Please select VNet based Integration runtime as your Data Flow is using VNet integrated source/target",DataFlowDebugVnetIrWarningContinue:"Continue with debug session",DataFlowDebugRunWithSessionTooltip:"Use a data flow debug session to run your pipeline. When a session is active, startup times will be greatly reduced but you may see run failures for parallel workloads.",DataFlowDebugRunWithSessionNotReadyTooltip:"The data flow debug session is not ready to run your pipeline. Please wait until your session starts successfully.",DataFlowDebugRunWithoutSessionTooltip:"Run each data flow on a separate instance using the activity runtime settings. Each data flow activity will have the standard cluster startup time.",DataFlowDebugTurnOnWaitMoreTitle:"Cluster startup is taking longer",DataFlowDebugTurnOnWaitMore:"The startup of the cluster is taking longer than expected, we recommend to wait more while the cluster finishes to startup",DataFlowDebugTurnOffWarning_Title:"Turn off Data Flow Debug",DataFlowDebugTurnOffWarning_Content:"This will end your data flow debug session.",DataFlowDebugSupportedIrs:"Data Flow debug only supports managed Azure integration runtimes and they must not be in the following regions: {1}.",DataFlowDebugSupportedTtl:"This is the amount of time that the IR will wait after your last data preview before automatically shutting down your debug cluster. To avoid billing for the entire TTL, you can shut down the debug session when you are finished working.",DataFlowDebugManagementTurnOffWarning_Content:"This will end the debug session. Please make sure it is not being used by any collaborators before proceeding.",DataFlowDebugManagementStopButtonDisabled_Stopping:"Debug session is stopping",DataFlowDebugRestart_Title:"Restart Data Flow Debug",DataFlowDebugRestart_Content:"Your debug session has timed out. Restarting the session will incur billing but will enable data flow data previews and pipeline debug runs.",DataFlowDebugProceed:"Do you wish to proceed?",DataFlowDebugKeepWaiting:"Do you want to wait more for the cluster startup?",DataFlowDebugSessionTimeoutWarning:"The current debug session will timeout in less than 10 minutes. Perform any debug action to keep your session active.",DataflowDebugError_Generic:"An error occured, please view notification for more details.",DataflowDebugError_InvalidSession:"Your debug session has expired, please restart debug mode.",DataflowDebugError_InvalidPreviewDataFormat:"Unable to parse preview data. If this issue persists, please provide feedback using the Feedback button.",DataflowDebugError_ClusterTimeout:"The request to start a debug session has timed out, please try again.",DataflowDebugError_SessionTimeout:"Your debug session has timed out due to inactivity, please restart debug mode.",DataflowDebugError_SessionUnRecoverable:"Your debug session got into a non recoverable state, please restart debug mode.",DataflowDebugError_SessionTimeout_Title:"Debug Session Timeout",DataflowDebugError_SessionTimeout_Warning:"Your debug session has timed out due to inactivity.",DataflowDebugError_SessionTimeout_Content:"Please restart debug mode to resume data flow debug capabilities.",DataflowDebugError_DatabricksTimeout:"ServiceUnavailable: The request to Databricks took too long to respond. Try decreasing the load on your cluster, checking the availibility of your cluster, or wait and try again.",DataflowDebugError_ObsoleteQuery:"Failed to fetch data for an old request.",DataflowDebugError_UnableToParseNaN:"Unable to parse NaN in preview data. Please try removing any number input that could produce NaN result.",DataflowDebugStatistics_Percentile25:"Percentile 25",DataflowDebugStatistics_StandardDeviation:"Standard Deviation",DataflowDebugStatistics_Percentile50:"Percentile 50",DataflowDebugStatistics_Percentile75:"Percentile 75",DataflowDebugStatistics_Variance:"Variance",DataflowDebugStatistics_MaxLength:"Maximum Length",DataflowDebugStatistics_MinLength:"Minimum Length",DataflowDebugStatistics_Label:"Label: {0}",DataflowDefaultContainerName:"{source container}",Count:"Count",Null:"Null",NULL:"NULL",NotNull:"Not Null",PercentSign:"%",Data:"Data",Develop:"Develop",RemainingValues:"Remaining values",CreateResourceButtonText:"+",AddPipelineText:"New pipeline",CreateDataflow:"New data flow",CreateFlowlet:"New flowlet",DataflowCreationHint:"A dataset will be automatically created from the selected file or folder to be used as a source in the data flow.",DataflowCreationHintSynapse:"An integration dataset will be automatically created from the selected file or folder to be used as a source in the data flow",AddPowerQueryText:"New power query",BillingOperationsHeader:"Number of data factory operations in current session.",BillingOperationsFooter:"See more information on ADF Data pipelines pricing",OneThousandBillionPlus:"1,000B+",Period:".",EmptyNotificationState:"No notifications to show",EmptyNotificationDescription:"New notifications from this session will appear here.",Consumption:"Consumption",PipelineRunConsumption:"Pipeline run consumption",DebugRunConsumption:"Debug run consumption",ConsumptionTooltip:"View estimated consumption units by activities while debugging your pipeline and post execution runs",Quantity:"Quantity",Unit:"Unit",PipelineOrchestration:"Pipeline orchestration",PipelineExecution:"Pipeline execution",AzureIntegrationRuntime:"Azure integration runtime",SelfHostedIntegrationRuntime:"Self-hosted integration runtime",VNetIntegrationRuntime:"VNet integration runtime",DataMovementActivities:"Data movement activities",PipelineActivities:"Pipeline activities",ExternalActivities:"External activities",ExecutionHours:"Execution hours",DIUHour:"DIU-hour",ViewDebugRunConsumption:"View debug run consumption",ViewRunDetail:"View run detail",PricingCalculator:"Pricing calculator",NotIncluded:"Not included",ValidationActivity:"Validation activity",VCoreHour:"vCore-hour",WarmPoolMessage:"You may see additional charges related to time-to-live",InteractiveClusterMessage:"There will be additional charges for the total time of your debug session",HDInsightMessage:"HDInsight activities may see additional charges related to time-to-live",ExecutePipelineMessage:"You will see additional charges for pipelines triggered by an execute pipeline activity",ValidationActivityMessage:"You will see additional charges for validation activity related to pipeline duration",ConsumptionReportLoadingWarning:"This pipeline has a large amount of activities. Loading will finish soon",CopySingleLine:"Copy as single line",CopySingleLineDescription:"Copy script as a single line for PowerShell or SDK use",CopySingleLineSuccess:"Copied!",ExportScriptDescription:"Export script via iframe communication",ManagementHub_IR_Description:"The integration runtime (IR) is the compute infrastructure to provide the following data integration capabilities across different network environment.",ManagementHub_LinkedService_Description:"Linked services are much like connection strings, which define the connection information needed for Azure Synapse Analytics to connect to external resources.",ManagementHub_LinkedService_Description_ADF:"Linked service defines the connection information to a data store or compute.",ManagementHub_Trigger_Description:"To execute a pipeline set the trigger. Triggers represent a unit of processing that determines when a pipeline execution needs to be kicked off.",ManagementHub_Git_Description:"Git repository information associated with your data factory.",ManagementHub_Global_Description:"Global parameters are constants across a Data Factory that can be consumed by a pipeline in any expression.",ManagementHub_Confirm_Title:"Manage hub",ManagementHub_Confirm_Message:"{0} are now located in Azure Data Factory's new manage hub",ManagementHub_Confirm_Button:"Open manage hub",GitRepoSettings:"Git repo settings",GitConfiguration:"Git configuration",GitRepository:"Git repository",CheckboxColumn:"Checkbox column",TableHeaderCheckbox:"Table header checkbox",TableCheckbox:"Table checkbox",PreviewDisabledByParameter:"Preview was disabled by parameters",Accessibility_ResizeColumn:"Resize column {0}",DoNotHavePermissionToSQLDataPlaneAccess:"Do not have SQL Data Plane Access",DoNotHavePermissionToLinkConnectionDataPlaneAccess:"Do not have link connection Data Plane access",DoNotHavePermissionToKQLDataPlaneAccess:"Do not have Kusto Data Plane Access",DoNotHavePermissionToGetDataPlaneAccess:"Do not have permission to get DataPlane access, please request access for operation Microsoft.DataFactory/factories/getDataPlaneAccess/action. Reason: {0}",GetDataPlaneAccessUnknownError:"Failed to get DataPlane access, reason: {0}",QueryApiErrorMessageForStatusCode0Template:'Failed to resolve the server name "{0}", please refer to troubleshooting doc and check network configuration.',QueryApiErrorMessageForStatusCode0Timeout:"Failed to get response from server. It is possible that the request takes too long. Please retry and contact Microsoft support if the issue persists.",ParameterSizeLimit:"Your ARM template exceeds the limit of {0} parameters. Please use custom parameters in your ARM template if you need to do CI/CD integration.",MoreInfo:"More info",NotImplementedYet:"Sandbox UI doesn't support this operation.",ModifiedTimeFilterHeader:"Filter by last modified",ModifiedTimeFilterHeaderAEP:"Filter by completed time",ModifiedTimeFilterDescription:"The files with last modified time in the range [Start time, End time) will be filtered for further processing. The time will be applied to UTC time zone in the format of 'yyyy-mm-ddThh:mm:ss.fffZ'. These properties can be skipped which means no file attribute filter will be applied",copyFileListPathDescription:"Point to a text file that lists each file (relative path to the path configured in the dataset) that you want to copy.",deleteFileListPathDescription:"Point to a text file that lists each file (relative path to the path configured in the dataset) that you want to delete.",fileListPathInCompatibleWithFileName:'Specifying file name in source dataset is not allowed when using "{0}"',ADLS_ListBefore_Description:"Retrieve the folders/files whose name is before this value alphabetically (inclusive). It applies the filter to the path defined in dataset.",ADLS_ListAfter_Description:"Retrieve the folders/files whose name is after this value alphabetically (exclusive). It applies the filter to the path defined in dataset",ADLS_ListBeforeAfter_Error:"Only one entity level is supported for listAfter/listBefore property.",ActivityRunNoResultError:"Activity runs return no result within 60 seconds",PipelineRunNoResultError:"Pipeline runs return no result Within 60 seconds",CancellingRunFailed:"Failed to cancel the run, error code : {0}",CancellingRunFailedDesc:"Failed to cancel the run for pipeline {0} ({1}), error code: {2}",CancelRunFailedDesc:"Failed to cancel the {0} ({1}) run because the pipeline already ran it.",ResourceFilterPlaceholder:"Filter resources by name",TriggerNowWarning:"Trigger pipeline now using last published configuration.",RunWithNewParametersWarning:"Running pipeline with modified parameters will create a new run.",ValidatorMessageNoneUniqueNestedNames:"Inner activities within the same pipeline cannot have the same name. Multiple activities named {0} were found within {1}.",SelectDropdownContainerLabel:"Dropdown container",SelectDropdownOptionList:"Dropdown option list",MultipleUserPropertiesWithSameName:"Only unique names in user properties allows. Multiple properties named {0} were found.",MaximumUserPropertiesExceeded:"The maximum number of User Properties allowed per activity is {0}, please remove extra properties.",NestedActivityNotSupportedChildParent:"Nested {0} activity is not supported. You cannot have {0} activity inside the scope of {1}. Please consider an Execute Pipeline activity to kick off new processing pipelines.",TridentLogicAppsLimitation:"{0} activity is not allowed when the pipeline has a {1} activity",DataFlowHistoryChartTitle:"Dataflow Execution History",ChangeDataCaptureChartTitle:"Captured changes count",DataFlowHistoryChartHAxisTitle:"Dataflow Execution Timeline",DataFlowHistoryLoadingText:"Loading Dataflow Runs",DataFlowHistoryButtonTooltip:"Fetch previous Runs",DataFlowHistoryCommandOption:"Run history",CurrentlySelected:" Dropdown currently selected ",Timeout_DescriptionMessage:"Specify the wait time before the query request times out. Default is 10 minutes (00:10:00).",QueryTimeout_DescriptionMessage:'Timeout for query command execution, default is 120 minutes. If parameter is set for this property, allowed values are timespan, such as "02:00:00" (120 minutes).',HttpRequestTimeout_DescriptionMessage:"Timeout for the HTTP request to get a response. Format is in TimeSpan. This value is the timeout to get a response, not the timeout to read response data. The default value is 00:05:00.",Utc:"UTC",UpgradeButtonText:"Upgrade now",UpgradeBannerTitle:"Upgrade Data Factory to G.A. version",UpgradeBannerText:"Upgrade your data factory to the latest version now! Any data factories that are not upgraded by September 30th, 2019 will be forced to upgrade.",UpgradeCheckboxText:"I understand the risks and implications",UpgradePopupSDKImpact:"The upgrade only impacts SDK and Powershell CLI users. If you are using our UX experience, you should see no impact at all",UpgradePopupPermissions:"Only users with {0} Owner or Data Factory Contributor permissions {1} can upgrade the factory",UpgradePopupImpact:"The upgrade will impact all users of this factory",UpgradePopupDownloadSDK:"After upgrade, please download the {0} updated GA SDK\u2019s and CLI\u2019s {1}",UpgradePopupListOfChanges:"To see changes between the preview API and GA API, please  {0} refer to this list {1}",UpgradePopupNoRedeployment:"No redeployment needed for this upgrade",UpgradePopupDataFlow:"In order to use data flows, you must perform this upgrade",UpgradeError:"Error: Failed to upgrade your Data Factory",UpgradeSucceeded:"Your Data Factory has been upgraded to G.A. version",UpgradePermissionWarning:"You are using the preview version of Azure Data Factory API\u2019s. Azure Data Factory is now {0} Generally Available (G.A.) {1}. Upgrade your data factory to use the GA API version and to enable data flows. Any data factories that are not upgraded by September 30th, 2019 will be forced to upgrade.",vCoreAreaAllocation:"vCore allocation",CrossPublishWarningBanner:"You have Git enabled in your data factory. Publishing in 'Data Factory' mode, while you have Git enabled, will be disabled after July 31st 2019. Please reach out to us using the feedback button above if you have any concerns.",CrossPublishInfoBanner:"You have Git enabled in your data factory. Publishing in 'Data Factory' mode is disabled. Please switch back to Git mode to make further changes.",CrossPublishInfoBannerA365:"You have Git enabled in your workspace. Publishing in 'Workspace' mode is disabled. Please switch back to Git mode to make further changes.",CrossPublishError:"You are not allowed to make changes or publish from 'Data Factory' mode as your data factory has Git enabled.",CrossPublishErrorA365:"You are not allowed to make changes or publish from 'Workspace' mode as your workspace has Git enabled.",CrossPublish_FeatureBlocked:"This feature is disabled in 'Data Factory' mode as your data factory has Git enabled. Please switch to 'Git' mode to use this.",CrossPublish_FeatureBlockedA365:"This feature is disabled in 'Workspace' mode as your workspace has Git enabled. Please switch to 'Git' mode to use this.",ReadAboutBestPractices:"Read about our Best Practices.",Error_InvalidLinkedserviceReferenced:"{0} cannot reference to an invalid or deleted {1}.",Error_InvalidIntegrationRuntimeReferenced:"The integration runtime referenced by {0} is offline or invalid.",Error_InvalidHDIAuthStorageService:"HDInsight linked service does not support storage linked services configured with {0}",Error_InvalidResourceReferenceTemplate:"Failed to load the {0}. Please make sure it exists and you have the permissions to access it.",BaseRequestId_Description:"The ID of request for delta loading. Once it is set, only data with requestId larger than the value of this property will be retrieved.",ActivityNotAdded:"Activity was not added",MaxNumberOfActivitiesWarning:"Pipeline '{0}' already has the max number of 40 activities allowed per pipeline.",MaxNumberOfActivitiesWarningTitle:"Failed to add an activity",MaxNumberOfActivitiesWarningDesc:"The pipeline {0} ({1}) was unable to add the requested activity because the pipeline already has the maximum number of 40 activities. Use multiple pipelines to balance your activities.",MaxNumberOfActivitiesError:"Pipeline '{0}' has more than the max number of 40 activities allowed per pipeline.",ConnectorMetadata_LogMessageTemplate:"Dynamic connector - {0}",ConnectorMetadata_ConfigRequiredTemplate:"Invalid configuration: {0} is required in configuration {1}.",ConnectorMetadata_InvalidConfigTemplate:"Invalid configuration: {0} is invalid in configuration {1}.",ConnectorMetadata_CannotFindPropertyConfigTemplate:"Did you remember to put it into {0}?",TabularTableName_DescriptionMessage:"Table name is required for Copy activity sink, and optional for Copy activity source or Lookup activity where query will be applied.",AzureDataExplorer_Query_DescriptionMessage:"Specify the query to retrieve data.",AzureDataExplorerCluster_SelectMethodDescription:"You can select a cluster from the list of available clusters in your Azure subscriptions, in which case you don\u2019t need to enter endpoint in free form text fields.",AzureDataExplorerDatabase_SelectMethodDescription:"Specify which account is used to list Azure Data Explorer databases. Manually input the name if you don\u2019t have permission for this operation.",MappingDataflowDescription:"Code free data transformation at scale",AzureDataExplorerEndpoint_Invalid_ValueMessage:"Please input a valid endpoint.",DisableDataFlowPreviewBanner:"The private preview version of Mapping dataflows is disabled now. You will no longer be able to execute or debug Mapping dataflows using this factory. To continue using Mapping dataflows, please migrate to the V2 version of Azure Data Factory.",WranglingDataflowDescription:"Code free data preparation at scale",WranglingDataflowConfigurationTitle:"Adding {0}",WranglignDataflowSettingsMovedTitle:"Power Query settings update",WranglingDataflowSettingsMovedMessage:"Settings have been moved to the bottom panel.",WranglignDataflowSettingsOkText:"Go to settings",WranglingDataflowEmptySourceDatasetGridMessage:"Please add at least 1 Source dataset to proceed...",WranglingDataflowEmptySinkDatasetGridMessage:"Please add at least 1 Sink dataset.",WranglingDataflowNoNameError:"Provide the name of {0}.",WranglingDataflowNameAlreadyExists:"{0} name already exists.",WranglingDataflowNotSupportedText:"{1} is not supported in {0}",WranglingDataflowUnsupportedDataset:"Dataset({0}) is of unsupported type({1}). Supported types are {2}.",WranglingDataflowDuplicateDataset:"Dataset is already selected.",WranglingDataflowComplexExpressionError:"Error found trying to resolve expressions for Power Query dataset {0}",WranglingDataflowUnsupportedMultipleSinkDataSets:"Multiple sink datasets are not supported.",WranglingDataflowUnsupportedLinkedService:"Source dataset [{0}] is referencing a linked service [{1}], which is of unsupported type [{2}]. Supported types are [{3}].",WranglingDataflowKeyBasedLinkedServiceError:"Linked service({0}) in dataset ({1}) is not supported. Only MSI or Service Principal based authentication is supported.",WranglingDataflowSchemaNotImportedError:"Dataset without schema is not allowed in source in {1}. Please import schema for dataset: {0}",WranglingDataflowUnsupportedTextFormat:"TextFormat({0}) for dataset({1}) is not supported. Supported types are {2}.",WranglingDataflowErrorLoadingMashupEditor:"Error loading mashup editor. sessionId: {0}. Error: {1}",WranglingDataflowSourceDatasetParametersLabel:"Source dataset parameters",WranglingDataflowSinkDatasetParametersLabel:"Sink dataset parameters",WranglingDataflowInvalidSourceFolder:"{0} dataset is invalid. {1}",WranglingDataflowADFResourceFolderWarning:"Do not change queries within this folder. Changes will not be persisted.",WranglingDataFlowErrorNoSource:"The {1} is invalid. No source dataset found in the {1} '{0}'.",WranglingDataFlowErrorNoSink:"No sink dataset found.",WranglingDataFlowErrorNoTransformation:"The {1} is invalid. No transformation found in the {1} '{0}'. You may have missed to click on 'Done' button in {1} tab.",WranglingDataflowLinkedServiceIRError:"Linked service with Self-hosted Integration runtime is not supported in Power Query. Referenced dataset : {0}, Linked service: {1}",WranglingDataflowSourceStorageAuthError:"Source dataset {0} is referencing linked service ({1}) with auth: {2}. Allowed auth: account key, service principal, and managed identity.",WranglingDataflowSourceBlobStorageNameAuthError:"Source dataset {0} is referencing linked service ({1}) with auth: {2}. Account name is in Key vault. Change the linked service to use connection string.",WranglingDataflowSourceAdlsGen1AuthError:"Source dataset {0} is referencing linked service ({1}) with auth: {2}. Allowed auth: service principal.",WranglingDataflowSourceSQLAuthError:"Source dataset {0} is referencing linked service ({1}) with auth: {2}. Allowed auth: Sql authentication, service principal, and managed identity.",WranglingDataflowSourceSQLNameAuthError:"Source dataset {0} is referencing linked service ({1}) with auth: {2}. Connection string is in Key vault. Change the linked service to use connection string.",WranglingDataflowMissingUserQueryError:'The {0} is invalid. Could not find "UserQuery" in the mashup. "UserQuery" is mandatory in {0}.',WranglingDataflowGenericError:"The {0} is invalid. ",WranglingDataflowStagingLinkedServiceNotNeeded:"This data flow does not require a staging linked service. Please remove it.",WranglingDataflowSourceDatasetDescription:"Supported dataset types: CSV formatted ADLS Gen1/Gen2, BLOB, SQL DB/DW.",WranglingDataflowSinkDatasetDescription:"Supported dataset types: CSV formatted ADLS Gen1/Gen2, BLOB, SQL DB/DW (with account key based linked service).",WranglingDataflowInvalidNamed:"Unable to load the {1}. Please correct the JSON. Error: {0}",WranglingDataflowSinkPropertiesLabel:"Sink properties",WranglingDataflowSourcePropertiesLabel:"Source properties",WranglingDataFlowViewSupportedTransformations:"Supported transformations",WranglingDataflowLinkedServiceAuthTypeError:"{0} cannot be used as sink dataset for {2}. It is referencing MSI or Service principal based linked service ({1})",WranglingDataflowParamValueNotProvided:'Provide value for parameter: "{0}" in dataset: "{1}"',WranglingDataflowTabularDoesNotHaveTable:"{0} is a tabular type dataset and the schema cannot be empty. Please import the schema.",WranglingDataflowParquetNotSupportedForGen1:"Parquet {0} dataset is not supported for ADLS Gen1",WranglingDataflowDatasetInUse:"Dataset {0} cannot be deleted. It may be referenced by other queries in the dataflow.",PowerQueryRegion:"Authoring region",PowerQueryRegion_Description:"Choose the power query authoring region.",PowerQuery_ConvertDataflow:"Open mapping data flow",PowerQuery_ConvertingText:"Opening...",PowerQuery_ConversionFailed:"Failed to convert power query {0} to mapping data flow. Session Id: {1}",PowerQuery_ConversionTooltip:"Unable to convert due to unsupported M functions.",PowerQuery_ValidationMessage:"{0} : {1} Please remove steps with unsupported functions to continue as a new mapping data flow.",PowerQuery_ValidationNoSourceDatasets:"Power Query must have at least 1 source dataset",Parameterization_SecureStringWarningInGit:"Parameters of type SecureString cannot have a default value when using Git mode, they must be fetched at runtime through Azure Key Vault or set when triggering the pipeline manually.",DataFlowActivityIRLabel:"Run on (Azure IR)",CircularDependenciesDetected:"Circular dependencies were detected for pipeline {0}, pipeline cannot have circular dependencies",CrossPublishWarningText:"Since you have enabled Git in your data factory, we strongly advise you to publish changes from Git mode instead of Data Factory mode as it could lead to inconsistencies in your data factory. Do you still want to continue?",PublishWarningTitle:"Publish warning",WebPageOutdated:"Webpage outdated",WebPageOutdatedSubtitle:"New changes have been deployed in this Azure service, please reload your webpage.",Reload:"Reload",LakehouseTablNameValidationError:"Table name may only contain letters, numbers, and underscores. The name must also be no more than 256 characters long.",CmdApiExportTemplateDescription:"Export the ARM template using the resources of a given folder",CmdApiExportTemplateValidationError:"Validation errors found, {0} export cancelled. Please fix the validation errors and try again.",CmdApiValidationError:"Data Factory validation failed. Found {0} validation errors.",CmdApiValidationErrorA365:"Synapse workspace validation failed. Found {0} validation errors.",CmdApiValidateDescription:"Validate all the resources of a given folder.",CmdApiRootFolderDescription:"Folder where the Data Factory resources are located.",CmdApiResourceIdDescription:"Data factory resoruce id in the format: /subscriptions/<subId>/resourceGroups/<rgName>/providers/Microsoft.DataFactory/factories/<dfName>",CmdApiOutputFolder:"Relative path to save the generated ARM template",CmdApiPreview:"Flag to use the preview deployment tools",CmdApiExportResourceNotFoundError:"No resource found in specified input path: {0}. Please set correct path and try again.",AlertError:"Evaluation frequency should be less than or equal to the search period.",AlertPeriod:"Defines the interval over which datapoints are grouped using the aggregation type function.",AlertFrequency:"Select the frequency on how often the alert rule should be run. Selecting frequency smaller than granularity of datapoints grouping will result in sliding window evaluation.",AlertOperator:"Select the operator used to compare the metric value against the threshold.",AlertAggregation:"Defines the aggregation function to apply on datapoints in \u2018Aggregation granularity\u2019.",AlertThreshold:"Please enter a number",AlertsNameNotUnique:"There is already an alert rule with the same name for this {0}.",AlertsConfirmationDelete:"Are you sure you want to delete the alert rule:\n{0}?",UndoButtonLabel:"Undo",RedoButtonLabel:"Redo",ErrorKeySequenceExpressionContainsEmptyElement:"Detected empty segment in KeySequence expression.",FailedToImportSchemaTemplate:"Failed to import {0} schema.",ToggleButton:"{0} Toggle Button {1}",TriggerLowerCase:"trigger",PipelineLowerCase:"pipeline",DataflowLowerCase:"data flow",DatasetLowerCase:"dataset",LinkedServiceLowerCase:"linked service",IntegrationRuntimeLowerCase:"integration runtime",SqlScript:"SQL script",KqlScript:"KQL script",Change:"Change",Existing:"Existing",Security:"Security",SelectAnItem:"Select an item",SynapseNotebookDescription:"Reference to the Synapse notebook to run.",SynapseNotebookActivityExecutorSizeDescription:"Number of cores and memory to be used for executors allocated in the specified Apache Spark pool for the session. For dynamic content, valid values are Small/Medium/Large/XLarge/XXLarge.",SynapseNotebookActivityMinExecutorsDescription:"Min number of executors to be allocated in the specified Apache Spark pool for the session.",SynapseNotebookActivityMaxExecutorsDescription:"Max number of executors to be allocated in the specified Apache Spark pool for the session.",SynapseNotebookActivityExecutorsDescription:"Number of executors to be allocated in the specified Apache Spark pool for the session.",SynapseNotebookActivityDriverSizeDescription:"Number of core and memory to be used for driver allocated in the specified Apache Spark pool for the session. For dynamic content, valid values are Small/Medium/Large/XLarge/XXLarge.",SynapseSparkJobDefinitionLabel:"Spark job definition",SynapseScopeJobDefinitionLabel:"SCOPE job definition",SynapseScopeJobDefinitionPriorityLabel:"Job priority",SynapseScopeJobDefinitionMaxUnavailabilityLabel:"Max unavailability(%)",SynapseScopeJobDefinitionTagsLabel:"Custom properties",StoredProcedure:"Stored procedure",SQLPoolStoredProcedure:"SQL pool stored procedure",SQLPoolStoredProcedureBanner:"To reference SQL pool, use the SQL pool stored procedure instead.",SQLPoolStoredProcedureV3Banner:"To reference SQL pool v3, use the generic stored procedure instead.",SQLPoolDatasetV3Banner:"To reference SQL pool v3, use Azure Synapse Analytics instead.",SQLPoolListCPUUsageToolTipContext:"Last 60 minutes",SQLPoolNoPermission:"You don't have permissions to view the connected SQL server. Contact your server admin.",WorkspaceDisconnectedWarning:"The SQL Logical server resource has been deleted or moved.",SynapseSparkJobDefinitionDescription:"Reference to the Spark job definition to run.",SynapseSparkJobDefinitionMainDefinitionFileLabel:"Main definition file",SynapseSparkJobDefinitionMainDefinitionFileDescription:"The main file used for the job.",SynapseSparkJobDefinitionMainDefinitionFileValidationMessage:"The path of main definition file is not a valid abfs[s] URI.",SynapseSparkJobDefinitionMainClassNameDescription:"The fully-qualified identifier or the main class that is in the main definition file.",SynapseSparkJobDefinitionArgumentsLabel:"Command line arguments",SynapseSparkJobDefinitionArgumentsToolTip:"This will override the command line arguments defined by the Spark job definition.",SynapseSparkJobDefinitionScanFolderLabel:"References from subfolders",SynapseSparkJobDefinitionScanFolderDescription:'Scanning subfolders from the root folder of the main definition file, these files will be added as reference files. The folders named "jars", "pyFiles", "files" or "archives" will be scanned, and the folders name are case sensitive.',SynapseSparkJobDefinitionPythonCodeReferenceLabel:"Python code reference",SynapseSparkJobDefinitionPythonCodeReferenceDescription:"Additional python code files used for reference in the main definition file.",SynapseSparkJobDefinitionReferenceFilesLabel:"Reference files",SynapseSparkJobDefinitionReferenceFilesDescription:"Additional files used for reference in the main definition file.",SynapseSparkJobDefinitionReferenceFilesPlaceholder:"Each line should be a storage URI.",SynapseSparkJobDefinitionSparkPoolDescription:"The job will be submitted to the selected Apache Spark pool.",SynapseSparkJobDefinitionDynamicExecutorDescription:"This setting maps to the dynamic allocation property in Spark configuration for Spark Application executors allocation.",SynapseSparkJobDefinitionExecutorSizeLabel:"Executor size",SynapseSparkJobDefinitionExecutorSizeDescription:"Number of core and memory to be used for executors allocated in the specified Spark pool for the job. For dynamic content, valid values are Small/Medium/Large/XLarge/XXLarge.",SparkPoolExecutorSize:"{0}({1} vCores, {2}GB memory)",SynapseSparkJobDefinitionMaxExecutorsLabel:"Max executors",SynapseSparkJobDefinitionMaxExecutorsDescription:"Max number of executors to be allocated in the specified Spark pool for the job.",SynapseSparkJobDefinitionMinExecutorsLabel:"Min executors",SynapseSparkJobDefinitionMinExecutorsDescription:"Min number of executors to be allocated in the specified Spark pool for the job.",SynapseSparkJobDefinitionExecutorsLabel:"Executors",SynapseSparkJobDefinitionExecutorsDescription:"Number of executors to be allocated in the specified Spark pool for the job.",SynapseSparkJobDefinitionDriverSizeLabel:"Driver size",SynapseSparkJobDefinitionDriverSizeDescription:"Number of core and memory to be used for driver allocated in the specified Spark pool for the job. For dynamic content, valid values are Small/Medium/Large/XLarge/XXLarge.",SynapseSparkJobDefinitionDriverSizeValidationMessage:"The value of driver size should be the same as the value of executor size.",SynapseSparkJobDefinitionCustomConfig:"Use customized configuration",SynapseSparkJobDefinitionDefaultConfig:"Use default configuration",SynapseSparkJobDefinitionFilesV2MigrationWarning:'The "Python code reference" field is now separated from the "Reference files" field. If you have placed any Python code files in the "Reference files" field, please manually move them to the "Python code reference" field.',SynapseScopeJobDefinitionDescription:"Reference to the SCOPE job definition to run.",SynapseScopeJobDefinitionScriptParametersToolTip:"This will override the parameters defined by the SCOPE job definition.",SynapseScopeJobDefinitionPriorityToolTip:"This will override the job priority defined by the SCOPE job definition.",SynapseScopeJobDefinitionMaxUnavailabilityToolTip:"This will override the max unavailability(%) defined by the SCOPE job definition.",SynapseScopeJobDefinitionTagsToolTip:"This will override the custom properties defined by the SCOPE job definition.",SynapseScopeJobDefinitionNebulaArgumentsToolTip:"This will override the nebula arguments defined by the SCOPE job definition.",SynapseScopeJobDefinitionStandardCapacityAllocationLabel:"Standard capacity allocation",SynapseScopeJobDefinitionStandardCapacityAllocationToolTip:"The standard capacity allocated to the job.",SynapseScopeJobDefinitionEcoCapacityAllocationLabel:"ECO capacity allocation",SynapseScopeJobDefinitionEcoCapacityAllocationToolTip:"The ECO capacity allocated to the job.",SynapseScopeJobDefinitionCapacityAllocationDefault:"Default (AU)",SynapseScopeJobDefinitionCapacityAllocationAbsoluteNumber:"Absolute number (AU)",SynapseScopeJobDefinitionCapacityAllocationPercentage:"Percentage(%)",SynapseScopeJobDefinitionCapacityAllocationCustomValue:"Custom value",SynapseSqlPoolReferenceLabel:"Reference to the SQL pool",SynapseSparkPoolReferenceLabel:"Reference to the Spark pool",SynapseRenameTextFormat:"Choose a name for your {0}. This name cannot be updated later.",SynapseRenameAfterPublishTextFormat:"Choose a name for your {0}. This name can be updated at any time until it is published.",SynapseResourceNameTextFormat:"New {0}",SynapseResourceCloneTextFormat:"Clone {0}",SystemLinkedServiceDescription:"This is the default linked service for this Synapse workspace. It cannot be edited.",SynapseDefaultImportMessage:"Importing default resources",TridentNotebookDescription:"Reference to the notebook to run.",NoResultsToShow:"No results to show",NoResultsToShowSuggestion:"Your filter yielded no displayable results.",NoResultsToShowSuggestionV2:"If you expected to see results, try changing your filters.",NoResultsToShowSuggestionV3:'You can find all supported connectors from <a target="_blank" href="https://docs.microsoft.com/azure/data-factory/connector-overview">here</a>. For any source that is not listed, you are encouraged to share idea via <a target="_blank" href="https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c">community</a>.',NoRoleAssignmentsToShow:"No role assignments to show",NoRoleAssignmentsToShowSuggestion:"If you expected to see results, try changing your filters or create new role assignment.",NoRoleAssignmentsToShowSuggestionInPanelMode:'If you expected to see results, try changing your filters or manage role assignments in "Access control".',NewItemUnpublished:"This new item is unpublished",NewItemUnpublishedSuggestion:"Publish this {0} first before managing its role assignments.",NoDataToShow:"No data to show",NoDataToShowSuggestion:"Try adding linked data using the + button above.",NoItemsToShow:"No items to show",NoItemsToShowSynapse:"Try changing your filter, create a new item, or contact your workspace administrator for permissions.",NoItemsToShowSuggestion:"Try creating a new item using the + button above.",NoRunningRequests:"No running requests",NoRunningRequestsSuggestion:"There are no running requests at this time.",FeatureNotAvailable:"Feature not available",UnsavedChangesMessage:"Some tabs are not saved. If you continue, your changes will be lost.",PublishAllChangesDescription:"You are about to publish all pending changes to the live environment.",NoItemsToShowFormat:"No {0} to show",NoItemsToShowSuggestionFormat:"If you expected to see results, try changing your filters or create a new {0}.",NoReferencedEntities:"This {0} is not referenced by other resources in this {1}",NoItemsForPreview:"No items selected for preview",Namespace:"Namespace",NamespaceQualifier:"Namespace qualifier",EndpointType:"Endpoint type",PublicEndpoint:"Public endpoint",BlobEndpoint:"Blob endpoint",TableEndpoint:"Table endpoint",FileEndpoint:"File endpoint",enablePartitionedDns:"Partioned DNS enabled",partitionedDnsTooltip:"This allows you to connect to your storage account with Azure DNS Zone endpoint type, which has a up to 5000 storage accounts limit per region per subscription.",ManagedVirtualNetworks:"Managed Virtual Networks",ManagedPrivateEndpoints:"Managed private endpoints",ManagedPrivateEndpoint:"Managed private endpoint",ManagedPrivateEndpointsDisabled:"Managed private endpoints are available after an integration runtime is enabled with a Managed Virtual Network.",CreateResourceFormat:"Create {0}",NewManagedPrivateEndpoint:"New managed private endpoint",ManagedPrivateEndpointsDescription:"Managed private endpoint uses a private IP address from within Managed Virtual Network to connect to an Azure resource or your own private link service. Connections using managed private endpoints listed below provide access to Azure resources or private link services.",ManagedPrivateEndpointDeleteMessage:"Are you sure you want to delete the managed private endpoint:\n{0}?",ManagedPrivateEndpointDeleteNow:"Managed private endpoint will be deleted immediately. This may impact any existing linked service possibly relying on this private endpoint.",ApprovalState:"Approval state",LinkedResourceId:"Linked resource ID",PublishManagedPrivateEndpointUnknownError:"Met unknown error when saving the managed private endpoint.",StorageAccountSelection:"You can select a storage account from the list of available accounts in your Azure subscriptions",StorageAccountNoSubscriptionsError:"There are no available subscriptions for the selected tenant",AzureDataExplorerSelection:"You can select a azure data explorer from the list of available accounts in your Azure subscriptions",PrivateEndpointResourceIdPlaceholder:'Enter the resource ID here. For example, "{0}"',PrivateEndpointTargetResourceIdDescription:"Resource ID of the data source.",PrivateEndpointPrivateIPDescription:"Private IP of the private endpoint.",PrivateEndpointResourceIdDescription:"Resource ID of the private endpoint.",InvalidResourceId:'Invalid resource ID format. Use the following format: "{0}"',PurviewPrivateEndpointCreateAllMessage:"This operation will create private endpoint to the Microsoft Purview account and ingestion private endpoint to the Microsoft Purview managed storage account and event hub as listed below.",PurviewPrivateEndpointCreateIngestionMessage:"This operation will create ingestion private endpoints to the Microsoft Purview managed storage account and event hub as listed below.",PurviewCannotGetMangedResourceMessage:"Cannot retrieve the information of Purview managed resources as you don\u2019t have permission. Please create the corresponding ingestion private endpoints (Storage blob, Storage queue, Event Hubs namespace) separately afterwards.",PurviewPrivateEndpointDescription:"The private endpoint to the Microsoft Purview account and ingestion private endpoints to the Microsoft Purview managed storage account and event hub. ",PrivateEndpointApprovalMessage:"After creation, {0} will be generated that must get approved by an owner of the data source.",APrivateEndpoint:"a private endpoint request",PrivateEndpoints:"private endpoint requests",PrivateEndpointsNotEnabledMessage:"To use this feature, you must create a {0} with the Managed Virtual Network option enabled.",PossibleLinkedService:"Possible Linked Services",SystemPrivateEndpointDescription:"This managed private endpoint is for system-only use, part of a proper functioning {0}.",PrivateEndpointHyperLinkText:"Manage approvals in Azure portal",ErrorMessageForDeletePrivateEndpoint:"Not able to delete managed private endpoint. Error: {0}",ManagePrivateEndpointInstantPublish:"Managed private endpoints are published immediately to the {0}.",IntegrationRuntimeInstantPublish:"Integration runtimes are published immediately to the {0}",AutoResolveIRRegionValidation:'You can only create AutoResolveIntegrationRuntime in "Auto Resolve" region.',ManagedVNetIRRegionValidation:'You can only create Virtual Network enabled integration runtime in "Auto Resolve" region or {0} region.',ManagedVNetSubstatusTooltip:"Data movement, interactive query, pipeline and external activities: {0}\nDataflow: {1}\nIRs can take 15 minutes to be ready for dataflow from the time they are created.",DataflowStateIRStateErrorMessage:"Integration runtime: {0} is not ready for data flow use. Please try again later.",DataflowStateIRStateFailedMessage:"Integration runtime: {0} failed to provision resources needed for data flow use. Please use a different integration runtime or reach out to Microsoft support.",ManagedVNetNotReadyForDataMovement:"Integration runtime: {0} is not ready for data movement, interactive query, pipeline and external activities. Please try again later.",ApprovalStateStaleMessage:"The approval state may be stale by a minute.",PrivateEndpointDuplicateDataSourceFormat:"Managed private endpoint ('{0}') has already been created for the selected resource.",PrivateLink:"Private link service",Fqdns:"Fully qualified domain names",BatchPECreationTextFormat:"Choose a name for your managed private endpoint. This will be used to generate names for the ingestion private endpoints and cannot be changed.",BatchIngestionPECreatiosnTextFormat:"Your account private endpoint name will be used to generate names for Ingestion private endpoints. ",InvalidFqdnPattern:"Fully qualified domain name is invalid.",PEUpdateButtonTooltip:"Updates can only be made when the private endpoint is in terminal state",shellStringResources,Dataflow_GoogleSheets_spreadsheetId:"SpreadSheet ID",Dataflow_GoogleSheets_startCell:"Start cell",Dataflow_GoogleSheets_endCell:"End cell",DataFlow_GoogleSheets_spreadsheetId_Description:"The Spreadsheet ID can be extracted from the Google Sheets URL.<br/>https://docs.google.com/spreadsheets/d/&lt;spreedsheet id&gt;/view#gid=&lt;gid&gt;",Monitoring_SparkJobSummaryPanel_ApplicationId:"Application ID",Monitoring_SparkJobSummaryPanel_CompletedJobsTotal:"Completed jobs (total)",Monitoring_SparkJobSummaryPanel_CompletedTasksTotal:"Completed tasks (total)",Monitoring_SparkJobSummaryPanel_Executors:"Number of executors",Monitoring_SparkJobSummaryPanel_Default_Executors:"Default number of executors",Monitoring_SparkJobSummaryPanel_Executors_Number:"{0} executors",Monitoring_SparkJobSummaryPanel_Executors_Range:"{0} to {1} executors",Monitoring_SparkJobSummaryPanel_Autopause:"Autopause timeout",Monitoring_SparkJobSummaryPanel_Autopause_Enabled:"Enabled ({0} minutes)",Monitoring_SparkJobSummaryPanel_Autoscale:"Autoscale",Monitoring_SparkJobSummaryPanel_Nodes_Number:"{0} nodes",Monitoring_SparkJobSummaryPanel_Nodes_Range:"{0} to {1} nodes",Monitoring_SparkJobSummaryPanel_DynamnicAllocation:"Dynamically allocate executors",Monitoring_Succeeded:"SUCCEEDED",Monitoring_Bar_LogQuery:"Log query",Monitoring_Bar_CompletedTasks:"Completed tasks",Monitoring_Bar_Attemps:"Attempts {0} of {0}",Monitoring_Bar_ErrorString:"This application failed due to the total number of errors: {0}.",Monitoring_Tab_Logs:"Logs",Monitoring_Tab_Logs_ToggleButtonLabel:"Spark job log toggle button",Monitoring_Scope_Tab_Logs_ToggleButtonLabel:"SCOPE job log toggle button",Monitoring_Tab_RelatedObjects:"Related",Monitoring_Tab_Diagnostics:"Diagnostics",Monitoring_Tab_Errors_Count:"Errors ({0})",Monitoring_SparkJob_CancelDialog_Title:"Cancel application?",Monitoring_SparkJob_CancelDialog_Content:"Are you sure you want to cancel this running application?",Mointoring_SparkAndScopeJob_NoGraphToShow:"Cannot show graph",Monitoring_SparkAndScopeJob_NoDataAvailableForRenderJobGraph:"No data available to render the job graph",Monitoring_SparkJob_LogType:"Log type",Monitoring_SparkJob_Log_EmptyContent:"No logs to show",Monitoring_SparkJob_Log_EmptyContent_Description:"No logs available in this application.",Monitoring_SparkJob_Log_ViewFulllist:"View full list",Monitoring_SparkJob_Log_ViewFulllogs:"View full logs",Monitoring_SparkJob_Log_FinalStatus:"Final status",Monitoring_SparkJob_Log_Length:"Length",Monitoring_SparkJob_Log_Size_GreaterThan100KB:"Logs for this application exceed 100KB. You can download full logs.",Monitoring_SparkJob_Log_Size_GreaterThan45M:"Logs for this application exceed 45MB. You can view up 100KB of logs here or download full logs by copying the 'curl' link.",Monitoring_SparkJob_Loading_Old_Log:"loading...",Monitoring_SparkJob_Load_More_Old_Log:"Load older logs",Monitoring_SarkJob_Log_Showing_Log:"Last loaded at {0}.",Monitoring_SparkJob_All_Old_Log_Have_Been_Loaded:"Last loaded at {0}. No older logs to load.",Monitoring_SparkJob_Log_Reached_Load_Old_Log_Limit_For_Stopped_Application:"Last loaded at {0}. Download logs to view more.",Monitoring_SarkJob_Log_Filer_Error_And_Warnings:"Showing errors and warnings since {0}.",Monitoring_SparkJob_Log_Reached_Load_Old_Log_Limit_For_Stopped_Application_Bubble_Content:"The max displayable logs have been reached. You may need to download the full logs to view older log data.",Monitoring_Tab_Error_ToggleButtonLabel:"SCOPE job error toggle button",Monitoring_Tab_Diagnostic_ToggleButtonLabel:"Spark job diagnostic toggle button",Monitoring_SparkJob_Diagnostic_Log_EmptyContent:"No diagnostics to show",Monitoring_SparkJob_Diagnostic_Not_Avaliable_EmptyText:"No diagnostics are available for this application.",Monitoring_SparkJob_Diagnostic_No_Jobs_Were_Executed:"No jobs were executed in this application.",SparkJob_CancelInProgress_Title:"Cancelling in progress",SparkJob_CancelInProgress_Description:"Cancelling of {0} is in progress. This can take 1-2 minutes.",SparkJob_CancelSuccess_Title:"Cancelling succeeded",SparkJob_CancelSuccess_Description:"{0} (Apache Spark application) was cancelled successfully.",SparkJob_CancelFail_Title:"Cancelling failed",SparkJob_CancelFail_Description:"The cancelling of {0} (Apache Spark application) was failed. {1}",SparkJob_FetchingAppDetailFailed_Title:"Fetching failed",SparkJob_FetchingAppAttemptsFailed_Description:"Fetching application attempts info failed.",Monitoring_ApplicatonID_Is_NA:"This application id is N/A.",Monitoring_SqlPools_WorkspaceLite_Message:"Select your dedicated SQL pool (formerly SQL DW) to view monitoring in the Azure portal.",Monitoring_No_Failed_Job:"No failed job found.",Monitoring_No_Data_Skew:"No data skew detected.",Monitoring_No_Time_Skew:"No time skew detected.",Monitoring_Executor_Utilization_Label:"Executor utilization",Monitoring_Errors:"Errors{0}",Monitoring_Warnings:"Warnings{0}",Monitoring_Line:"Line",Monitoring_Failed_Jobs_Label:"Failed jobs{0}",Monitoring_Data_Skew_Label:"Data skew{0}",Monitoring_Time_Skew_Label:"Time skew{0}",Monitoring_Failed_Stage_Id_Label:"Stage {0}",Monitoring_Failed_Task_Id_Label:"Task {0}",Monitoring_Failed_Job_Id_Label:"Job {0}",Monitoring_Failed_Stage_Summary:"{0}/{1} succeeded",Monitoring_Skipped_Tasks:" ({0} skipped)",Monitoring_Failed_Task_Summary:"{0}/{1} succeeded{2}",Monitoring_Success_Scope_Job:"No errors to display.",Monitoring_No_Warning_Scope_Job:"No warnings to display",Monitoring_Success_Job_Summary:"No failed jobs to display.",Monitoring_Executor_Utilization_Summary:"{0}% utilization efficiency",Monitoring_Not_Applicable:"Not applicable",Monitoring_Task_Error_Summary:"Task errors ({0})",Monitoring_Single_Error:" (1 error)",Monitoring_Plural_Error:" ({0} errors)",Monitoring_Single_Warning:" (1 warning)",Monitoring_Plural_Warning:" ({0} warnings)",Monitoring_Single_Failed_Job_Summary:" (1 job)",Monitoring_Plural_Failed_Job_Summary:" ({0} jobs)",Monitoring_Single_Skew_Stage_Summary:" (1 stage)",Monitoring_Plural_Skew_Stage_Summary:" ({0} stages)",Monitoring_Failed_Job_Panel_Title:"Job {0} errors",Stages:"Stages",Tasks:"Tasks",Monitoring_Stage_ID:"Stage ID",Monitoring_Skew:"Skew",Monitoring_Skewed_Tasks:"Skewed Tasks",StoredProcedureNotFound:"Successfully connected to the SQL Pool ('{0}'), but the stored procedure ('{1}') was not found.",DeepLinkActivityNotFound:"The activity ('{0}') in pipeline ('{1}') no longer exists.",DeepLinkSqlPoolNotFound:"The activity ('{0}') does not have a SQL pool set.",DeepLinkFailedToParseInput:"Failed to parse the run input of activity ('{0}') in order to extract the stored procedure name.",DeepLinkFailedStoredProcedure:"Failed to retrieve the stored procedure ('{0}') from the SQL pool ('{1}').",DeepLinkFailedToParseSparkOutput:"Failed to parse the run output of activity ('{0}') in order to extract the Spark application name.",DeepLinkFailedToNavSparkOutput:"Failed to navigate to the Spark application using its name ('{0}').",DeepLinkFailedToFindSparkName:"Failed to find the Spark application name from the run output of the activity ('{0}').",DeepLinkFailureTitle:"Failed to navigate from monitor",SqlPoolWorkspaceLite:"Dedicated (formerly SQL DW)",Dedicated:"Dedicated",Extraction_Mode:"Extraction mode",TokenEndpoint:"Token endpoint",GetLoginUserAccount_Error_Message:"If you want to get user account, please login firstly!",GetSecurityToken_Error_Message:"Please get token firstly!",AccessControl_Summary_NoScope:"Showing {0} - {1} of {2} role assignments ({3} user(s), {4} group(s), {5} service principal(s))",AccessControl_Summary_AllScope:"Showing {0} - {1} of {2} role assignments at all scopes in the workspace ({3} user(s), {4} group(s), {5} service principal(s))",AccessControl_Summary_WorkspaceScope:"Showing {0} - {1} of {2} role assignments at the workspace scope ({3} user(s), {4} group(s), {5} service principal(s))",AccessControl_Summary_WorkspaceItemScope:"Showing {0} - {1} of {2} role assignments at the selected scope and higher ({3} user(s), {4} group(s), {5} service principal(s))",AccessControl_RemoveAccess:"Remove access",AccessControl_RemoveDialogContent:"Are you sure you want to delete the selected role assignment(s)?",AccessControl_Permissions:"Permissions",AccessControl_WorkspaceAdmin:"Workspace admin",AccessControl_Admin:"admin",AccessControl_Admins:"admins",AccessControl_ApacheSparkSuperUser:"Apache Spark admin",AccessControl_ApacheSparkSuperUser_Deprecated:"Synapse Apache Spark Administrator",AccessControl_SqlAnalyticsSuperUser:"SQL admin",AccessControl_SqlAnalyticsSuperUser_Deprecated:"Synapse SQL Administrator",AccessControl_ScopeSuperUser:"Scope admin",AccessControl_ScopeSuperUser_Deprecated:"Synapse SCOPE Administrator",AccessControl_SynapseAdministrator:"Synapse Administrator",AccessControl_SynapsePrivateEndpointManager:"Synapse Linked Data Manager",AccessControl_SynapseContributor:"Synapse Contributor",AccessControl_SynapseArtifactAuthor:"Synapse Artifact Publisher",AccessControl_SynapseArtifactReader:"Synapse Artifact User",AccessControl_SynapseComputeUser:"Synapse Compute Operator",AccessControl_SynapseCredentialUser:"Synapse Credential User",AccessControl_SynapseWorkspaceReader:"Synapse User",AccessControl_SynapseMonitoringOperator:"Synapse Monitoring Operator",AccessControl_SynapseAdministratorTooltip_Deprecated:"Grants access to serverless SQL pools, Apache Spark pools, and Integration runtimes. Includes publish access to all code objects, compute operator, credential user, and linked data manager access. Includes granting access.",AccessControl_SynapseAdministratorTooltip:"Grants access to SQL pools, Apache Spark pools, and Integration runtimes. Includes publish access to all code objects, compute operator, credential user, and linked data manager access. Includes granting access.",AccessControl_SynapseApacheSparkAdministratorTooltip:"Grants access to Apache Spark pools, Apache Spark job definitions, notebooks, libraries, linked services, and credentials. Includes read access to all code objects. Excludes managed private endpoints, use of credentials, and granting access.",AccessControl_SynapseSQLAdministratorTooltip_Deprecated:"Grants access to serverless SQL pools and SQL scripts, credentials, and linked services. Includes read access to all code objects. Excludes managed private endpoints, use of credentials, and granting access.",AccessControl_SynapseSQLAdministratorTooltip:"Grants access to SQL pools and SQL scripts, credentials, and linked services. Includes read access to all code objects. Excludes managed private endpoints, use of credentials, and granting access.",AccessControl_SynapseContributorTooltip:"Grants access to Apache Spark pools, integration runtimes, and publish access to code objects including credentials and linked services. Excludes managed private endpoints, use of credentials, and granting access.",AccessControl_SynapseArtifactPublisherTooltip:"Grants access to read and publish code objects including credentials. Includes creation of new code objects but cannot run code without additional permissions. Excludes granting access.",AccessControl_SynapseArtifactUserTooltip:"Grants access to read published code objects. Includes creation of new code objects but cannot run or publish changes.",AccessControl_SynapseComputeOperatorTooltip:"Submission of Spark jobs, notebooks, and pipelines. Includes canceling Spark jobs and pipeline runs submitted by any user and viewing logs.\u200b",AccessControl_SynapseCredentialUserTooltip:"Runtime and configuration-time use of secrets within credentials, and linked services in activities like pipeline runs.\u200b",AccessControl_SynapseLinkedDataManagerTooltip:"Creation and management of managed private endpoints, linked services, and credentials.",AccessControl_SynapseUserTooltip:"Read access to published Synapse resources, excluding code artifacts. Can create new artifacts but cannot run, publish, or save without additional permissions.",AccessControl_ScopeSuperUserTooltip:"Grants access to SCOPE Pools. Create, read, update, and delete access to published SCOPE job definitions, credentials, and linked services. Includes read access to all other published code artifacts. Does not include permission to use credentials and run pipelines. Does not include granting access.\u200b",AccessControl_SynapseMonitoringOperatorTooltip:"Grants read access to published code, including logs and outputs for notebooks and pipelines. Includes ability to list and view details of serverless SQL pools, Apache Spark pools, Data Explorer pools, and Integration runtimes",AccessControl_ManagementBannerMessage:'To manage these {0}, users need sufficient <a target="_blank" href="{1}">Azure RBAC permission</a> on this workspace, such as the Owner or Contributor role.',AccessControl_SelectARole:"Select a role",AccessControl_FailedToAddRole:"Failed to add admin",AccessControl_FailedToAddRole_Content:"Failed to add admin to workspace {0}, please try again.",AccessControl_FailedToRemoveRole:"Failed to remove admin",AccessControl_FailedToRemoveRole_Content:"Failed to remove admin from workspace {0}, please try again.",AccessControl_MoreThenACLsWarningTitle:"ACL entry limit reached",AccessControl_MoreThenACLsWarningContent:"The maximum number of ACL entry {0} has been reached. Remove an ACL and try again.\n\nConsider using security groups instead of individual accounts to add a large number of users.",AccessControl_MoreThenRolesWarningTitle:"Role assignment limit reached",AccessControl_MoreThenRolesWarningContent:"The maximum number of role assignments {0} has been reached for this role: {1}. Remove a role assignment and try again.\n\nConsider using security groups instead of individual accounts to add a large number of users.",AccessControl_FailedToListResources:"Failed to list resources",AccessControl_InsufficientPermission:"You do not have permissions to perform this operation. Contact your workspace administrator to obtain access.",AccessControl_ViewRoleAssignment_AllWorkspaceUsersCanViewThisResource:"All {0} can view this {1}. ",AccessControl_ViewRoleAssignment_ManageRoleAssignments:"Manage role assignments (Access control)",AccessControl_PublishPermissionPrefix:"You do not have access to publish artifacts of this type. To be granted access, contact your workspace administrator. Missing permission: ",AccessControl_WatermarkPermissionPrefix:"You do not have required Synapse RBAC permission to perform this action.<br/>Contact a Synapse Administrator for this workspace.<br/>Required permission:<br/>Actions: {0}<br/>Scope: {1}<br/>",AccessControl_PermissionPrefix:"You do not have required Synapse RBAC permission to perform this action.\nContact a Synapse Administrator for this workspace.\nRequired permission:\nActions: {0}\nScope: {1}",AccessControl_PermissionDebugPrefix:"You do not have required Synapse RBAC permission to perform this action. To debug a pipeline, you must be able to execute the pipeline. Contact a Synapse Administrator for this workspace.\nRequired permission:\nActions: {0}\nScope: {1}",AccessControl_PermissionTriggerPrefix:"You do not have required Synapse RBAC permission to perform this action. To add a trigger to a pipeline, you must be able to execute the pipeline. Contact a Synapse Administrator for this workspace.\nRequired permission:\nActions: {0}\nScope: {1}",AccessControl_PermissionArmPrefix:"You do not have permissions to perform this operation. Contact your subscription administrator to obtain access.",AccessControl_PermissionOEData:"Currently unavailable. Contact the worksapce admin to obtain right role or permission.",AccessControl_PermissionSomeItems:"Some items are not shown.",AccessControl_RoleAssignmentLS:"Synapse role assignments on this linked service allow users to use the secret embedded in this linked service. ",AccessControl_RoleAssignmentIR:"Synapse role assignments on this integration runtime allow pipeline credentials to use this resource. ",AccessControl_RoleAssignmentCredential:"Synapse role assignments on this credential allow users to use the secret embedded in this credential. ",EditSchema:"Edit schema",DatasetReferencedTooltip:"Referenced datasets can not be deleted",AccessControl_AddRole_InProgress:"Adding the role assignment",AccessControl_AddRole_InProgress_Multiple:"Adding the role assignments",AccessControl_AddRole_InProgress_Content:"Adding {target} to the {role} role for {scope}.",AccessControl_AddRole_InProgress_Content_Multiple:"Adding {count} role assignments to the {role} role for {scope}.",AccessControl_AddRole_Succeed:"Successfully added the role assignment",AccessControl_AddRole_Succeed_Multiple:"Successfully added the role assignments",AccessControl_AddRole_Succeed_Content:"Successfully added {target} for the {role} role for {scope}. This change may take 2-5 minutes to take effect.",AccessControl_AddRole_Succeed_Content_Multiple:"Successfully added {count} role assignments for the {role} role for {scope}. This change may take 2-5 minutes to take effect.",AccessControl_AddRole_Fail:"Failed to add the role assignment",AccessControl_AddRole_Fail_Multiple:"Failed to add the role assignments",AccessControl_AddRole_Fail_Content:"Failed to add {target} for the {role} role for {scope}.",AccessControl_AddRole_Fail_Content_Multiple:"Failed to add {count} of {total} role assignments for the {role} role for {scope}.",AccessControl_DelRole_InProgress:"Deleting the role assignment",AccessControl_DelRole_InProgress_Multiple:"Deleting the role assignments",AccessControl_DelRole_InProgress_Content:"Deleting the role assignment for {target} from the {role} role for {scope}.",AccessControl_DelRole_InProgress_Content_Multiple:"Deleting {count} role assignments.",AccessControl_DelRole_Succeed:"Successfully deleted the role assignment",AccessControl_DelRole_Succeed_Multiple:"Successfully deleted the role assignments",AccessControl_DelRole_Succeed_Content:"Successfully deleted the role assignment for {target} from the {role} role for {scope}.",AccessControl_DelRole_Succeed_Content_Multiple:"Successfully deleted {count} role assignments.",AccessControl_DelRole_Fail:"Failed to delete the role assignment",AccessControl_DelRole_Fail_Multiple:"Failed to delete the role assignments",AccessControl_DelRole_Fail_Content:"Failed to delete the role assignment for {target} from the {role} role for {scope}. ",AccessControl_DelRole_Fail_Content_Multiple:"Failed to delete {count} of {total} role assignments. ",AccessControl_ArmBannerMessage:"To manage this setting, you need sufficient Azure RBAC permissions on this workspace, such as the Owner or Contributor role.",AccessControl_notAuthorizedUser:"You do not have permission to review Synapse RBAC role assignments. Contact a Synapse Administrator for more information.",AccessControl_LimitdAccess:"You do not have permission to review Synapse RBAC role assignments due to limited graph permission. Contact a Synapse Administrator for more information.",AccessControl_LimitdAccessForSearchUser:"You can only add role assignment by object ID.",AccessControl_FindObjectId:"How to find object ID?",AnalyticsPools_ApacheSparkPoolsDescription:"Apache Spark pools can be finely tuned to run different kinds of Apache Spark workloads using specific configuration libraries, permissions, etc. ",AnalyticsPools_SqlPoolsDescription:"SQL on-demand is immediately available for your workspace. SQL pools can be configured to adapt to team or organizational requirements and constraints. ",IncrementalLoad:"Incremental load",FullLoad:"Full load",FullAndIncrementalLoad:"Full and incremental load",FeedbackBannerMessage:"Need help fixing an issue?",FeedbackExperienceMessage:"Tell us about your experience. This will not engage a support professional.",FeedbackExperienceHeader:"I have feedback",FeedbackFeatureSuggestionHeader:"I have a feature suggestion",FeedbackFeatureSuggestionMessage:"You will be redirected to the Azure Feedback Forum",FeedbackOptionalEmail:"Enter your email address (optional)",FeedbackCommentsRequired:"Comments",FeedbackForum:"visit Q&A",CognitiveServices:"Cognitive Services",CognitiveService:"Cognitive Service",NoCognitiveService:"No Cognitive Services",AzureMonitorPrivateLinkScope:"Azure Monitor Private Link Scope",AzureMonitorPrivateLinkScopes:"Azure Monitor Private Link Scopes",AzureMonitorPrivateLinkScope_SelectMethodDescription:"The Azure Monitor Private Link Scope has a number of limits you should consider when planning your Private Link setup.",AzureMonitorPrivateLinkScope_SelectSubscriptionDescription:"Select your Azure subscription",CredentialWarning:"The user auth credential will be available to all {0} contributors or owners (or a custom role that lets you access the 'Credentials') ",CredentialWarningEmailActivity:"Credentials are shared with all factory users. Please consider using a dedicated account for credential related operations.",CredentialListDescriptionUami:"Credentials contain user-assigned, system-assigned managed identities and service principals that you can use in the linked services that support Azure Active Directory (AAD) authentication. It helps you consolidate and manage all your AAD credentials.",CredentialListDescription:"Credentials hold authentication details",CredentialLowercased:"credential",CredentialNameRequired:"Credential name is required",CredentialTypeRequired:"Credential type is required",ServicePrincipalIDRequired:"Service principal ID is required",ServicePrincipalKeyRequired:"Service principal key is required",AzureKeyVault:"Azure key vault",UserIdentityPrincipalId_DisplayText:"User principal identity:",UserIdentityDisplayName_DisplayText:"User display name:",ContactSupport:"Contact support",FactorySettings:"Factory settings",FactorySettings_GitWarning:"In git mode, changes to these settings are automatically saved.",DataFlowVersion_Label:"Data flow version",DataFlowVersion_DefaultValue:"Not set (defaults to Live version)",DataFlowVersion_Candidate:"Candidate",DataFlowVersion_CandidateTooltip:"Build updated on the 1st of every month and becomes the next 'Stable' build on the 15th of the following month",DataFlowVersion_Stable:"Stable",DataFlowVersion_StableTooltip:"Build updated on the 15th of every month and is the previous 'Candidate' build",DataFlowVersion_Live:"Live",DataFlowVersion_LiveTooltip:"Build with all of the latest live features of ADF Data Flow",FactoryEnvironment_Label:"Factory environment",FactoryEnvironment_Tooltip:'Deployment stage in a continuous integration and delivery (CI\\CD) setup. <a target="_blank" href="https://learn.microsoft.com/azure/data-factory/continuous-integration-delivery">Learn more</a>',FactoryEnvironment_Prod:"Production",FactoryEnvironment_Prod_Tooltip:"Content has been tested and verified, and is ready for distribution",FactoryEnvironment_Test:"Test",FactoryEnvironment_Test_Tooltip:"Content is ready for testing, previewing, and verifying",FactoryEnvironment_Dev:"Development",FactoryEnvironment_Dev_Tooltip:"Content is being developed and revised",Notebook_RunCurrentCellAndSelectBelow:"Run current cell and select below",Notebook_RunCurrentCellAndInsertBelow:"Run current cell and insert below",Notebook_RunCurrentCell:"Run current cell",Notebook_GoToEditMode:"Go to edit mode",Notebook_SelectPreviousCell:"Select previous cell",Notebook_SelectNextCell:"Select next cell",Notebook_InsertCellAbove:"Insert cell above",Notebook_InsertCellBelow:"Insert cell below",Notebook_DeleteCurrentCell:"Delete current cell",Notebook_GoToCommandMode:"Go to command mode",Notebook_CommandMode:"Command mode",Notebook_EditMode:"Edit mode",SQLEditor:"SQL editor",SQLEditor_CreateSQLScript:"Create SQL script",SQLEditor_CloseCurrentSQLScriptTab:"Close current SQL script tab",SQLEditor_RunSQLScript:"Run SQL script",SQLEditor_QueryPlanSQLScript:"Query plan SQL script",SQLEditor_CancelRunningSQLScript:"Cancel running SQL script",SQLEditor_PublishSQLScript:"Publish SQL script",SQLEditor_ToggleOutputPanel:"Toggle the output panel",SQLEditor_TogglePropertyPanel:"Toggle the property panel",SQLEditor_RefreshSQLPoolConnections:"Refresh the SQL pool connections",dataFlowFunctionDescriptions:{},errorStringResources:{},UnexpectedError_NotFound:"Unexpected error: {0} not found",ManageCookiePreferences:"Manage cookie preferences",ManageCookiePreferencesDescription:"Most Microsoft sites use cookies, small text files placed on your device which web servers in the domain that placed the cookie can retrieve later. We use cookies to store your preferences and settings, help with sign-in, and analyze site operations. For more information, see the ",CookiePrivacylinkDescription:"Cookies and similar technologies section of the Privacy Statement.",CookiePrivacylink:"https://privacy.microsoft.com/privacystatement",Essential:"Essential",EssentialInfoDetail:"We use essential cookies for authentication and to save sign in information.",PerformanceAndAnalytics:"Performance and Analytics",PerformanceAndAnalyticsDetail:"We use performance and analytics cookies to identify a user\u2019s session anonymously and to collect telemetry data.",SavePreferences:"Save preferences",Accept:"Accept",Reject:"Reject",CookieMessageBannerMessage:"We use optional cookies to provide a better experience. ",MoreOptions:"More options",UAMILabel:"User Assigned Managed Identities",UAMIOptLabel:"New User Assigned Managed Identity",uamiDescription:"Select the User Assigned Managed identity that will be used to create the credential",NoAccessLabel:"No access",NoPermissionContent:"You do not have permissions to perform this operation. Contact your Purview admin to obtain access.",CheckingPermissionLabel:"Checking permission",PermissionsLabel4Resource:"{0} permissions",UserAssignedManagedIdentity:"User assigned managed identity",DataFactoryManagedIdentity:"Data Factory managed identity",UAMI:"UAMI",SAMI:"SAMI",UAMIPermssionNewLabel:"Grant permission to UAMI",GrantPermissionLabel:"Grant permission to another Data Factory or user-assigned managed identity",ResourcePermissions:"Resource permissions",SAMIFullNameWithDataFactory:"System-assigned Managed Identity (Data Factory)",SAMIFullName:"System-assigned Managed Identity",LinkedIRTabDescription:"The list of linked integration runtimes that are created to share with this integration runtime.",aPipeline:"A pipeline",anActivity:"An activity",aDataset:"A dataset",aDataflow:"A data flow",aDataMapper:"A data mapping",aPowerQuery:"A power query",anAirflowEntity:"An airflow entity",aLinkedService:"A linked service",aConnection:"A connection",aTrigger:"A trigger",aUDFLibrary:"A data flow library",anIR:"An integration runtime",anEntity:"An entity",aAdfDataMapper:"A CDC",start:"start",Warning_Name_hasDash:"Data flow name with dashes (-) may not work with parameterized content.",Validator_NameRequired:"{0} name is required.",Validator_NameExist:"{0} with the same name {1} already exists in the {2}.",Validator_NamePattern_All_AlphanumericDash:"{0} name should only contain letters, numbers or dashes (-).",Validator_NamePattern_All_LowerAlphanumericDash:"{0} name should only contain lower letters, numbers or dashes (-). Dashes (-) in consecutive order are not permitted. First two or last one characters cannot be dashes.",Validator_NamePattern_All_AlphanumericUnderscore:"{0} name should only contain letters, numbers or underscores (_).",Validator_NamePattern_All_AlphanumericUnderscoreSpace:"{0} name should only contain letters, numbers, underscores (_), or spaces.",Validator_NamePattern_All_AlphanumericUnderscoreDashSpace:"{0} name should only contain letters, numbers, dashes (-), underscores (_), or spaces.",Validator_NameMaxLength:"{0} name has exceeded the limit of {1}.",Validator_NameMinLength:"{0} name should contain at least {1} characters.",Validator_NoConsecutiveDashes:"Dash (-) characters must be immediately preceded and followed by a letter or a number.\nDashes (-) in consecutive order are not permitted.",Validator_NoHtml:"Folder name must not contain html (<>&\"')",Validator_Email:"Please enter a valid email address",Monitoring_SparkApplication_InputOutput_ReadFormat:"Read format",Monitoring_SparkApplication_InputOutput_SizeAtExecution:"Size at execution",Monitoring_SparkApplication_InputOutput_WriteFormat:"Write format",Monitoring_SparkApplication_InputOutput_Mode:"Mode",Monitoring_SparkApplication_InputOutput_CopyInput:"Copy input",Monitoring_SparkApplication_InputOutput_ExportAsCsv:"Export as CSV",Monitoring_SparkApplication_InputOutput_CopyOutput:"Copy output",Monitoring_SparkApplication_InputOutput_SearchFile:"Filter by",Monitoring_SparkApplication_InputOutput_Previous:"< Previous",Monitoring_SparkApplication_InputOutput_Next:"Next >",Monitoring_SparkApplication_InputOutput_Page:"Page",Monitoring_SparkApplication_InputOutput_PageCount:" of {0}",Monitoring_SparkApplication_InputOutput_NoInputFileDescription:"No input data was found for this application.",Monitoring_SparkApplication_InputOutput_NoOutputFileDescription:"No output data was found for this application.",Monitoring_SparkApplication_InputOutput_StorageNotFoundErrorMessage:"Can't found storage {0} in synapse linked service.",Monitoring_SparkApplication_InputOutput_ContainerNotFoundErrorMessage:"Can't found container {0} in storage {1}",Monitoring_SparkApplication_InputOutput_FileNotFoundErrorMessage:"Can't found file {0}",Monitoring_SparkApplication_InputOutput_CantDisplayPropertiesErrorMessage:"Some information cannot be displayed due to one or more errors.",Monitoring_SparkApplication_InputOutput_InputLabel:"Input data",Monitoring_SparkApplication_FetchDriverLogFailedByNullApplicationId_ErrorMessage:"No Application ID is retrieved for this application. Thus the driver log is not available.",Monitoring_SparkApplication_FetchPrelaunchLogFailedByNullApplicationId_ErrorMessage:"No Application ID is retrieved for this application. Thus the prelaunch log is not available.",Monitoring_SparkApplication_FetchJobGraphFailedByNullApplicationId_ErrorMessage:"No Application ID is retrieved for this application. Thus the job graph is not available.",Monitoring_ScopeJob_Graph_Desc:"This functionality is not yet available",Monitoring_ScopeJob_Graph_Text:"No graph to show",Monitoring_ScopeJob_InputOutput_NoFileDescription:"No data was found.",Monitoring_ScopeJob_InputOutput_SearchFiles:"Search files",Monitoring_ScopeJob_Parallelism:"Standard utilized capacity (AU)\u200b",Monitoring_ScopeJob_EcoTokenParallelism:"ECO utilized capacity (AU)",Monitoring_SparkApplicationOrScopeJob_InputOutput_NoFileTitle:"No files to show",Monitoring_SparkApplicationOrScopeJob_InputOutput_UnableDisplayTitle:"Unable to display files",Monitoring_SparkApplicationOrScopeJob_InputOutput_UnableDisplayDescription:"The files were unable to be displayed due to an error.",DataType:"Data type",OpenPreview:"Open preview",DownloadPreview:"Download preview",CopyPath:"Copy path",FieldName:"Field name",Sign:"Sign",Low:"Low",High:"High",Inclusive:"Inclusive",Exclusive:"Exclusive",FullPath:"Full path",RelativePath:"Relative path",ShowInExplorer:"Show in explorer",AccessTime:"Access time",Modified:"Modified",LimitExceeded:"limit exceeded",PageNumberDropdown:"page number drop down",SwitchToLiveMode:"Switch to live mode",CompareApplication:"Compare applications",AppMetrics:"Application metrics",BaseApplication:"Selected application",TargetApplication:"Comparison application",SelectAnApplication:"Select an application",Disable_Selected_Application_ToolTip:"The selected application cannot be compared because it contains no available metrics.",ChangeAnApplication:"Change application",Monitoring_SparkJob_Compare_EmptyContent:"No comparison to show",Monitoring_SparkJob_Compare_EmptyText:"Select a comparison application above.",Choose_Target_Application_To_Compare:"Choose a comparison application to compare job performance metrics against the selected application.",Choose_Target_Application:"Choose comparison application",RecurringAplications:"Recurring applications",ApplicationURL:"Application URL",EnterApplicationURL:"Enter an application URL",Comparing:"Comparing...",Comparison_Method_Wording:"Enter an application URL or choose a recurring application. This will be used as the comparison application.",Comparison_Difference_Header:"Difference (selected - comparison)",No_Application_To_Show:"No applications to show",Empty_Recurrent_Job:"There are no recurring applications to show for the selected application.",Compare_In_Notebook:"compare in notebook",Invalid_Provide_URL:"The provided URL is invalid.",Selected_Application_ToolTip:"The selected application will be compared to a comparison application of your choosing.",Comparsion_Application_ToolTip:"Choose a comparison application to compare performance metrics against the selected application.",Application_URL_ToolTip:"Paste an application URL to use as the comparison application.",Recurrent_Application_ToolTip:"Recurring applications are shown below if the selected application was initiated by a scheduled pipeline or was manually ran multiple times.",No_Metrics_To_Show:"The selected application contains no available metrics. Try selecting a different application.",Monitoring_Application_LearnMoreLink:"https://go.microsoft.com/fwlink/?linkid=2152066#compare-apache-spark-applications",Log_Type_Livy:"Livy",Log_Type_Driver_Stderr:"Driver (stderr)",Log_Type_Driver_Stdout:"Driver (stdout)",Log_Type_PreLaunch_stderr:"PreLaunch (stderr)",Log_Type_PreLaunch_stdout:"PreLaunch (stdout)",SizeNumvCoresNumGb:"{0} ({1} vCores / {2} GB)",GrantAuthenticationTitle:'Click "Authenticate" to continue',GrantAuthenticationContent:"Please allow popups for {0} to complete the authentication flow. Click the Authenticate button to open popup manually.",Authenticate:"Authenticate",Reauthenticate:"Reauthenticate",Authenticated:"Authenticated",CheckingAuthenticate:"Checking Authentication",Processing:"processing",GrantAuthenticationDenyError:"Grant authentication denied. You may not able to access resources.",RecurrentInfo:"One or more recurring applications are not shown because they failed to execute or do not contain any job events for comparison.",RemainingMinutes:"{0} ({1} remaining)",Month_January:"January",Month_February:"February",Month_March:"March",Month_April:"April",Month_May:"May",Month_June:"June",Month_July:"July",Month_August:"August",Month_September:"September",Month_October:"October",Month_November:"November",Month_December:"December",Month_Jan:"Jan",Month_Feb:"Feb",Month_Mar:"Mar",Month_Apr:"Apr",Month_Jun:"Jun",Month_Jul:"Jul",Month_Aug:"Aug",Month_Sep:"Sep",Month_Oct:"Oct",Month_Nov:"Nov",Month_Dec:"Dec",Today:"Today",WeekDay_M:"M",WeekDay_T:"T",WeekDay_W:"W",WeekDay_S:"S",MappingArtifact:"Mapping artifact",MappingArtifacts:"Mapping artifacts",MappingArtifactValidationOutput:"Mapping artifact validation output",CdcValidationOutput:"CDC validation output",TargetEntityNameMustBeUnique:"Entity folder path must be unique",TargetTables:"Target tables",EditTargetDb:"Edit target database",JoinTables:"Join tables",MappingMethod:"Mapping method",CommonMappings:"Common mappings",SourceColumn:"Source column",TargetColumn:"Target column",DirectMapping:"Direct mapping",Sum:"Sum",Trim:"Trim",DataMappingDebugContent:"Data mapper requires an active data flow debug session. Try again when your data flow debug session is active.",ChangeDataCaptureDebugContent:"CDC requires an active data flow debug session. Try again when your data flow debug session is active.",DataMappingDebugSessionCompletedMessage:"Data flow debug session is now active, you can open the Data Mapping assistant now.",ChangeDataCaptureDebugSessionCompletedMessage:"Data flow debug session is now active, CDC canvas will open now.",DataMappingDebugSessionCompletedTitle:"Data flow debug session is active",MapData:"Map Data",Automap:"Automap",Automapped:"Auto Mapped",DataMapperAutoMap:"Auto map",ConditionalAutomap:"Rule-based",fuzzyMatchLevel:"fuzzy matching level",columnPatternApplyOnExistingMappingsLabel:"Apply on top of existing transformations",columnPatternApplyOnExistingMappingsDescription:"Checking this will apply the rules on top of the existing transformations in case their source column matches the rule condition. Note: Aggregate transformations cannot be applied via rules.",fuzzyMatchDescription:"Adjust the similarity threshold and click to automap columns based on column names, data types and table names.",Upper:"Upper",Lower:"Lower",Direct:"Direct",AdditionalSources:"Additional sources",PrimarySourceTable:"Primary source table",NoGeneratedResources:"No resources were generated. Please check to make sure that all of the mapping provided are valid mappings.",AlternateKeysSelected:"{0} alternate keys selected",NoLookupColumn:"No lookup column selected",NoUnpivotColumn:"No unpivot column selected",UnpivotKeyColumn:"Unpivot key column",NewLookupCondition:"New lookup condition",NewUnpivotColumn:"New unpivot column",KeyValue:"Key value",LookupSourceColumns:"Lookup source columns",LookupSource:"Lookup source",NoSourceTypeSelected:"No Source Type Selected",NoLinkedServiceSelected:"No Linked Service Selected",NoTargetTypeSelected:"No Target Type Selected",DuplicateSourceEntitySelected:"Duplicate Source Entity Selected",DuplicateTargetEntitySelected:"Duplicate Target Entity Selected",NoGeneratedResourcesHeader:"Failed to create pipeline",ErrorInLinkedServiceFetch:"Unable to fetch the Linked Service. Error: {0}",ErrorInDatamappingListFetch:"Unable to fetch the CDC list. Error: {0}",ErrorInStartStopAdfDataMapper:"Unable to {0} the CDC. Error: {1}",ErrorInAdfDataMapperRunHistoryFetch:"Unable to fetch the CDC run details. Error: {0}",ErrorStartBeforePublishAdfDataMapper:"Unable to find the CDC resource {0}. Please publish your changes and try again",ErrorInStartStopAdfDataMapperHeader:"Failed to {0} CDC",StartAdfDataMapperPreviewBannerMessage:"This CDC is not running. If you start this CDC, the last published configuration will be used. You can configure, start, or",AdfDataMapperPreviewBannerLinkMessage:"monitor this CDC",StopAdfDataMapperPreviewBannerMessage:"This CDC is currently running. While your data is being processed, you will be billed 4 v-cores of General Purpose Data Flows. You can add or remove tables from the configuration or ",AddOrRemoveTables:"Add or remove tables",ReAutomap:"Reset mapping",MatchingAccuracy:"{0} accuracy",AutomapMatchingDescription:"{0} matching threshold for fuzzy matching",TargetFolderPath:"Select target folder paths",TargetRootPath:"Target base path",TargetFolderPathLabel:"Target folder path",TargetBasePathDescription:"Auto-populates the browsed base path for all new target entities (Optional)",TargetRootPathPlaceholder:"Browse base path",ApacheSparkConfigurations:"Apache Spark configurations",CodeAndLibraries:"Configurations + libraries",ManagementHub_UDF_Description:"Data flow libraries contain custom functions composed using the expression builder. This can be especially helpful if you often find yourself combining functions regularly in your data flows.",UDFLibrary:"Data flow library",UDFLibraries:"Data flow libraries",NewUDFLibrary:"New data flow Library",UserDefinedFunction:"Data flow library function",UserDefinedFunctions:"Data flow library functions",NoFunctionsFound:"No functions in data flow library",NoLibrariesFound:"No data flow libraries found in your factory",ReturnType:"Return Type",Function:"Function",NewFunction:"New data flow function",EditFunction:"Edit data flow function",NoArguments:"This function has no arguments.",NoFunctionsInLibraryWarning:"A data flow library must contain at least one function",UserDefinedFunctionTooltipDescription:"This function is defined in the {0} data flow library as:\n\n\t{1}",UDFArgumentNotFound:"User defined function argument not found",UDFLibraryPlaceholderDesc:"Functions:\n{0}",UDFUnsupportedType:"This function currently returns an unsupported type",FunctionAlreadyExists:"This library contains function names that already exist in another data flow library. Consider renaming them before importing: {0}",ImportUDFLibrary:"Import data flow library",ImportFromCode:"Import from code",ImportFromFile:"Import from file",FailedToLoadLibraryDuplicateFunctionError:"The library {0} contains a function that already exist in another data flow library. Already present function name: {1}",UDFMapDriftedWarning:"Using a data flow library function in a matching expression or column name will cause drifted columns. Please use the 'Map Drifted' button inside of the data preview panel to consume any drifted columns in later transformations.",AutomaticSaving:"Automatic saving",AutosaveDescription:"Changes to {0} are saved automatically. If you disable this feature, all {1} saved changes will be discarded.",Unpublished:"unpublished",Uncommitted:"uncommitted",Tour_Autosave_Setting_Title:"Enable automatic saving",Tour_Autosave_Setting_Description:"Any changes made in your active session are automatically saved.",TryIt:"Try it",SwitchOrUpdate:"Switch language or update automatic saving setting",SavingPaused:"Saving paused",LastSaved:"Last saved: {0}",ToastTitle_AutosaveSettingSucceed:"Successfully applied workspace settings",ToastDescription_AutosaveSettingSucceed:"The current workspace is being saved automatically.",ToastTitle_AutosaveSettingFailed:"Failed to apply workspace settings",ToastDescription_AutosaveSettingFailed:"The current workspace is not being saved automatically. Try again later.",EnableInLowercase:"enable",EnabledInLowercase:"enabled",DisableInLowercase:"disable",DisabledInLowercase:"disabled",AutosaveInactiveSessionInfo:'Automatic saving has been disabled because the session is inactive. To make it active again, click <strong> Refresh </strong>. Any changes made while the session was inactive will be lost.\n\n<a target="_blank" href="https://docs.microsoft.com/azure/synapse-analytics">Learn more</a>',AutosaveConnectivityIssueInfo:'Automatic saving has been disabled because of a connectivity issue. Verify that you are still connected to the network and the session is active.\n\n<a target="_blank" href="https://docs.microsoft.com/azure/synapse-analytics">Learn more</a>',AutosaveNoPermissionInfo:'Automatic saving has been disabled because you do not have Read permissions to the workspace. Contact a workspace administrator.\n\n<a target="_blank" href="https://docs.microsoft.com/azure/synapse-analytics">Learn more</a>',Refresh_Tooltip_ActiveAutosave:"Refresh and discard any unsaved changes.",Refresh_Tooltip_InactiveAutosave:"Make the current session active and discard any unsaved changes.",DiscardAll_Publish_Autosave:"Discard all unpublished changes and revert to the last publish version.",DiscardAll_Git_Autosave:"Discard all uncommitted changes and revert to the last committed version.",TryDataFirstView:'Azure Data Factory Studio preview update <a target="_blank" href="https://aka.ms/adfpreviewexp">Learn more</a>',TryPreviewFeaturesSimplified:"Preview experience",EnablePreviewFeaturesConfirmDialogTitle:"Enable preview experience",DisablePreviewFeaturesConfirmDialogTitle:"Disable preview experience",EnablePreviewFeaturesDescription:"Opt-in to the Azure Data Factory studio preview experience to try new features. Some examples of what is currently in preview include: ",TurnOffPreviewFeaturesConfirmation:"Are you sure you want to turn off preview features?",PreviewDynamicSettings:"New dynamic content flyout for parameterizable pipeline settings",PreviewDataFirstView:"Dataflow data first view",PreviewSimplifiedMonitoring:"Simplified default monitoring view",NewDataFlowView:"the preview updates to Azure Data Factory Studio",billingSetting_label:"Show billing report",billingSettingOption_byPipeline:"by pipeline",billingSettingOption_byFactory:"by factory",billingSetting_description:"Select to view your billing report on Azure portal at the pipeline level or factory level",policyValidationSetting_label:"Advanced policy validation (preview)",policyValidationSetting_description:"Enable dataplane policies on your datafactory, which will allow controlling outbound endpoints ",Seconds:"seconds",SaveBeforeClose:"Saving the artifact before closing.",SaveSuccessfully:"Save the closed artifact successfully.",SaveFailed:"Save the closed artifact failed.",WelcomeToDataFactory:"Welcome to Azure Data Factory",WelcometoADF:"Welcome to ADF",CreateDataFactoryTitle:"Create a new data factory",CreateDataFactoryDesc:"Name your factory, then pick a location and subscription to deploy in. For advanced options, please use",Location:"Location",AzurePortal:"Azure Portal",CreateDataFactoryGenericError:"Something went wrong while creating your data factory. Please use the link below to create your data factory from the Azure Portal.",CreatingResourceGroup:"Creating resource group '{0}'",RegisteringDataFactoryProvider:"Registering Data Factory provider",CreatingDataFactory:"Creating Data Factory '{0}'",LoadingStep:"(Step {0} of {1})",FreeTrial:"Free trial",CreateSubscriptionDesc:"Before you can start using Azure Data Factory, you'll need to create an azure subscription. Click the button below to redeem your free trial, or for more information, please visit",SelectEntryOption:"Select an entry option",Learn:"Learn",LearningCenter:"Learning Center",GetHelp:"Get Help",MicrosoftQA:"Microsoft Q&A",RefetchSubscriptions:"I have created a subscription",NoSubscriptionsFoundError:"We could not find any subscriptions linked to this tenant. Please ensure the subscription has been successfully created and try again.",CreateDataFactoryPermissionsError:"You do not have the proper permissions to create a Data Factory in the subscription provided. Please use the below link to learn more about the permissions and roles required.",CreateDataFactorySelectResourceGroupDescription:"It looks like you do not have permissions to create a new resource group. Please select an existing resource group instead.",CreateDataFactory_SubscriptionTooltip:"This is the subscription the data factory will be created in. If you are not seeing the subscription you want to use, trying using the Azure Portal instead.",Disable_Certificate_Validation_TT:"Turn off server certificate validation",Trino_DisplayText:"Trino",TrinoModelName_ClusterName:"Trino cluster",TrinoAccountSelectionMethod:"Select your account from Azure subscription or enter the host manually",Trino_FormalName:"Azure HDInsight Trino",DataFirstOnDescription:"Switch data flow editor styles between classic (default) and data-first (preview). Current style is data-first.",DataFirstOffDescription:"Switch data flow editor styles between classic (default) and data-first (preview). Current style is classic (default).",CurrentOfTotal:"{0} of {1}",EmailPreviewWarning:"This activity is currently in preview and is not recommended for production use cases at this time.",Authentication_Validation4SHIR:"{0} is not supported for Self-hosted integration runtime (SHIR)",DocsSearchPlaceholder:"Search documentation",SearchHistory:"Search history",RowAdded:"Row added successfully",HeaderTextBox:"Header edits textbox",TypeMisMatch:"Type MisMatch"};globalThis.stringResources=stringResources;